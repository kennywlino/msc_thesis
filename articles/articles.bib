
@article{holland_preliminary_1999,
	title = {Preliminary {Tests} of {Language} {Learning} in a {Speech}-{Interactive} {Graphics} {Microworld}},
	volume = {16},
	abstract = {A speech-interactive graphics microworld is described in which learners speak problem-solving directions to an animated agent and in which new scenarios can be authored. A proof-of-concept application is illustrated for sustaining basic speaking skill in Modern Standard Arabic. Preliminary tests of the application are summarized involving learners from both university and military settings. Problems are discussed in predicting and measuring learning gains based on brief exposure to new technologies.},
	number = {3},
	journal = {CALICO Journal},
	author = {Holland, V Melissa and Kaplan, Jonathan D and Sabol, Mark A},
	year = {1999},
	pages = {22},
	file = {Holland_Preliminary Tests of Language Learning in a Speech-Interactive Graphics.pdf:files/105/Holland_Preliminary Tests of Language Learning in a Speech-Interactive Graphics.pdf:application/pdf}
}

@article{munro_foreign_1999,
	title = {Foreign {Accent}, {Comprehensibility}, and {Intelligibility} in the {Speech} of {Second} {Language} {Learners}},
	volume = {49},
	issn = {00238333},
	url = {http://doi.wiley.com/10.1111/0023-8333.49.s1.8},
	doi = {10.1111/0023-8333.49.s1.8},
	language = {en},
	urldate = {2018-03-18},
	journal = {Language Learning},
	author = {Munro, Murray J. and Derwing, Tracey M.},
	year = {1999},
	pages = {285--310},
	file = {Munro_1999_Foreign Accent, Comprehensibility, and Intelligibility in the Speech of Second.pdf:files/101/Munro_1999_Foreign Accent, Comprehensibility, and Intelligibility in the Speech of Second.pdf:application/pdf}
}

@article{hardison_contextualized_2013,
	title = {Contextualized {Computer}-based {L}2 {Prosody} {Training}: {Evaluating} the {Effects} of {Discourse} {Context} and {Video} {Input}},
	volume = {22},
	issn = {2056-9017},
	shorttitle = {Contextualized {Computer}-based {L}2 {Prosody} {Training}},
	url = {https://www.equinoxpub.com/journals/index.php/CALICO/article/view/23182},
	doi = {10.1558/cj.v22i2.175-190},
	abstract = {Two types of contextualized input in prosody training were investigated for 28 advanced L2 speakers of English (L1 Chinese). Their oral presentations provided training materials. Native-speakers (NSs) of English provided global prosody ratings, and participants completed questionnaires on perceived training effectiveness. Two groups received training input using Anvil, a web-based annotation tool integrating the video of a speech event with visual displays of the pitch contour, and practiced with Real-Time Pitch (RTP) in Computerized Speech Lab including feedback from a NS. Two groups used only RTP to view their pitch contours and practiced with the same feedback. Within these pairs, one group received discourse-level input and the other individual sentences. Each group served as its own control in a time-series design. All had comparable levels of performance prior to training. Results indicated that although all groups improved as a result of training, discourse-level input produced better transfer to novel natural discourse. The presence of video was more helpful with discourselevel input than with individual sentences. Speech samples collected 1 week after training revealed sustained improvement. Questionnaire results support the use of computer-based tools and authentic speech samples. Findings strongly suggest that meaningful contextualized input is valuable in prosody training when the measurement is at the level of extended connected speech typical of natural discourse.},
	number = {2},
	urldate = {2018-03-18},
	journal = {CALICO Journal},
	author = {Hardison, Debra M.},
	month = jan,
	year = {2013},
	pages = {175--190},
	file = {Hardison_2013_Contextualized Computer-based L2 Prosody Training.pdf:files/95/Hardison_2013_Contextualized Computer-based L2 Prosody Training.pdf:application/pdf}
}

@article{thompson_decoding_2004,
	title = {Decoding speech prosody: {Do} music lessons help?},
	volume = {4},
	issn = {1931-1516, 1528-3542},
	shorttitle = {Decoding speech prosody},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1528-3542.4.1.46},
	doi = {10.1037/1528-3542.4.1.46},
	language = {en},
	number = {1},
	urldate = {2018-03-18},
	journal = {Emotion},
	author = {Thompson, William Forde and Schellenberg, E. Glenn and Husain, Gabriela},
	year = {2004},
	pages = {46--64},
	file = {Thompson_2004_Decoding speech prosody.pdf:files/96/Thompson_2004_Decoding speech prosody.pdf:application/pdf}
}

@inproceedings{eskenazi_detection_1996,
	title = {Detection of foreign speakers' pronunciation errors for second language training-preliminary results},
	volume = {3},
	isbn = {978-0-7803-3555-4},
	url = {http://ieeexplore.ieee.org/document/607892/},
	doi = {10.1109/ICSLP.1996.607892},
	urldate = {2018-03-18},
	publisher = {IEEE},
	author = {Eskenazi, M.},
	year = {1996},
	pages = {1465--1468},
	file = {Eskenazi_1996_Detection of foreign speakers' pronunciation errors for second language.pdf:files/97/Eskenazi_1996_Detection of foreign speakers' pronunciation errors for second language.pdf:application/pdf}
}

@article{eskenazi_overview_2009,
	title = {An overview of spoken language technology for education},
	volume = {51},
	issn = {01676393},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167639309000673},
	doi = {10.1016/j.specom.2009.04.005},
	abstract = {This paper reviews research in spoken language technology for education and more speciﬁcally for language learning. It traces the history of the domain and then groups main issues in the interaction with the student. It addresses the modalities of interaction and their implementation issues and algorithms. Then it discusses one user population – children – and an application for them. Finally it has a discussion of overall systems. It can be used as an introduction to the ﬁeld and a source of reference materials.},
	language = {en},
	number = {10},
	urldate = {2018-03-18},
	journal = {Speech Communication},
	author = {Eskenazi, Maxine},
	month = oct,
	year = {2009},
	keywords = {read},
	pages = {832--844},
	file = {Eskenazi_2009_An overview of spoken language technology for education.pdf:files/93/Eskenazi_2009_An overview of spoken language technology for education.pdf:application/pdf}
}

@article{hwang_evaluating_2016,
	title = {Evaluating listening and speaking skills in a mobile game-based learning environment with situational contexts},
	volume = {29},
	issn = {0958-8221, 1744-3210},
	url = {http://www.tandfonline.com/doi/full/10.1080/09588221.2015.1016438},
	doi = {10.1080/09588221.2015.1016438},
	language = {en},
	number = {4},
	urldate = {2018-03-18},
	journal = {Computer Assisted Language Learning},
	author = {Hwang, Wu-Yuin and Shih, Timothy K. and Ma, Zhao-Heng and Shadiev, Rustam and Chen, Shu-Yu},
	month = may,
	year = {2016},
	pages = {639--657},
	file = {Hwang_2016_Evaluating listening and speaking skills in a mobile game-based learning.pdf:files/98/Hwang_2016_Evaluating listening and speaking skills in a mobile game-based learning.pdf:application/pdf}
}

@article{felps_foreign_2009,
	title = {Foreign accent conversion in computer assisted pronunciation training},
	volume = {51},
	issn = {01676393},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167639308001763},
	doi = {10.1016/j.specom.2008.11.004},
	abstract = {Learners of a second language practice their pronunciation by listening to and imitating utterances from native speakers. Recent research has shown that choosing a well-matched native speaker to imitate can have a positive impact on pronunciation training. Here we propose a voicetransformation technique that can be used to generate the (arguably) ideal voice to imitate: the own voice of the learner with a native accent. Our work extends previous research, which suggests that providing learners with prosodically corrected versions of their utterances can be a suitable form of feedback in computer assisted pronunciation training. Our technique provides a conversion of both prosodic and segmental characteristics by means of a pitch-synchronous decomposition of speech into glottal excitation and spectral envelope. We apply the technique to a corpus containing parallel recordings of foreign-accented and native-accented utterances, and validate the resulting accent conversions through a series of perceptual experiments. Our results indicate that the technique can reduce foreign accentedness without significantly altering the voice quality properties of the foreign speaker. Finally, we propose a pedagogical strategy for integrating accent conversion as a form of behavioral shaping in computer assisted pronunciation training.},
	language = {en},
	number = {10},
	urldate = {2018-03-18},
	journal = {Speech Communication},
	author = {Felps, Daniel and Bortfeld, Heather and Gutierrez-Osuna, Ricardo},
	month = oct,
	year = {2009},
	pages = {920--932},
	file = {Felps_2009_Foreign accent conversion in computer assisted pronunciation training.pdf:files/100/Felps_2009_Foreign accent conversion in computer assisted pronunciation training.pdf:application/pdf}
}

@incollection{borthwick_can_2017,
	title = {Can an interactive digital game help {French} learners improve their pronunciation?},
	isbn = {978-2-490-05704-7},
	url = {https://research-publishing.net/display_article.php?doi=10.14705/rpnet.2017.eurocall2017.691},
	abstract = {This study examines the effects of the pedagogical use of an interactive mobile digital game, Prêt à Négocier (PàN), on improving learners’ pronunciation of French as a Second Language (FSL), using three holistic measures: comprehensibility, fluency, and overall pronunciation. Two groups of FSL learners engaged in different types of game-playing over one month: while the experimental group played PàN, the control group engaged in paper-based gamified information gap activities. Following a pre-test/post-test research design, our findings revealed no statistically significant differences between the two groups.},
	urldate = {2018-03-18},
	booktitle = {{CALL} in a climate of change: adapting to turbulent global conditions – short papers from {EUROCALL} 2017},
	publisher = {Research-publishing.net},
	author = {Cardoso, Walcir and Rueb, Avery and Grimshaw, Jennica},
	editor = {Borthwick, Kate and Bradley, Linda and Thouësny, Sylvie},
	month = dec,
	year = {2017},
	doi = {10.14705/rpnet.2017.eurocall2017.691},
	pages = {67--72},
	file = {Cardoso_2017_Can an interactive digital game help French learners improve their pronunciation.pdf:files/94/Cardoso_2017_Can an interactive digital game help French learners improve their pronunciation.pdf:application/pdf}
}

@article{hardison_generalization_2004,
	title = {Generalization of computer assisted prosody training: {Quantitative} and qualitative findings},
	abstract = {Two experiments investigated the effectiveness of computer-assisted prosody training, its generalization to novel sentences and segmental accuracy, and the relationship between prosodic and lexical information in long-term memory. Experiment 1, using a pretest-posttest design, provided native English-speaking learners of French with 3 weeks of training focused on prosody using a real-time computerized pitch display. Multiple exemplars produced by native speakers (NSs) of French and stored on hard disk provided training feedback. Learners' recorded pre- and posttest productions were presented to NSs for evaluation in two conditions: filtered (unintelligible segmental information) and unfiltered. Ratings using 7-point scales for the prosody and segmental accuracy of unfiltered samples revealed significant improvement in prosody with generalization to segmental production and novel sentences. Comparison of prosody ratings for filtered and unfiltered samples revealed some segmental influence on the pretest ratings of prosody. In Experiment 2, involving a memory recall task using filtered stimuli of reduced intelligibility, learners identified the exact lexical content of an average of 80\% of the training sentences based on prosodic cues consistent with exemplar-based learning models. Questionnaire responses indicated a greater awareness of the various aspects of speech and increased confidence in producing another language.},
	journal = {Language Learning},
	author = {Hardison, Debra M.},
	year = {2004},
	pages = {19},
	file = {Hardison_GENERALIZATION OF COMPUTER-ASSISTED PROSODY TRAINING.pdf:files/103/Hardison_GENERALIZATION OF COMPUTER-ASSISTED PROSODY TRAINING.pdf:application/pdf}
}

@article{hyde-simon_phonology_2008,
	title = {\textit{{Phonology} and {Second} {Language} {Acquisition}} - {Edited} by {Jette} {G}. {Hansen} {Edwards} and {Mary} {L}. {Zampini}},
	volume = {18},
	issn = {08026106, 14734192},
	url = {http://doi.wiley.com/10.1111/j.1473-4192.2008.00205.x},
	doi = {10.1111/j.1473-4192.2008.00205.x},
	language = {en},
	number = {3},
	urldate = {2018-03-18},
	journal = {International Journal of Applied Linguistics},
	author = {Hyde-Simon, Caroline},
	month = nov,
	year = {2008},
	pages = {306--309},
	file = {Hyde-Simon_2008_iPhonology and Second Language Acquisition-i - Edited by Jette G.pdf:files/92/Hyde-Simon_2008_iPhonology and Second Language Acquisition-i - Edited by Jette G.pdf:application/pdf}
}

@article{shroff_gamified_2016,
	title = {{GAMIFIED} {PEDAGOGY}: {EXAMINING} {HOW} {A} {PHONETICS} {APP} {COUPLED} {WITH} {EFFECTIVE} {PEDAGOGY} {CAN} {SUPPORT} {LEARNING}},
	abstract = {Research has demonstrated that educational game-based apps may provide an approach to instruction in education that allows for greater learning outcomes. The focal context of this paper centres around the discussion of how gamified pedagogy supports learning. The first part of this paper will delve into the components of gaming, including the application of gamification to education and the methods by which digital game-based components such as scores and rewards are used to engage and motivate learners. The second part will focus on existing research on gaming pedagogy and the gaming elements of a phonetics app developed by the Resource Centre for Ubiquitous Learning and Integrated Pedagogy (ULIP) at Hong Kong Baptist University. The gamified pedagogical element of the app is designed to offer levels of challenge that motivate the players by making learning more exciting and rewarding. The game-based elements of the app promote active student involvement in learning, as the games are specifically designed to provide challenges and goals for players. Moreover, the need to capture and maintain the players’ attention through visual experiences and audio designs is also an important element in the design of the app. When learners are engaged in a game-based app of this nature, they are not only reinforcing their cognitive skills, but they are also constantly drawing connections between images, text and sounds, thereby allowing students to learn and practise basic skills in order to master complex tasks.},
	author = {Shroff, Ronnie H and Keyes, Christopher J and Wee, Lian-Hee},
	year = {2016},
	pages = {18},
	file = {Shroff_2016_GAMIFIED PEDAGOGY.pdf:files/102/Shroff_2016_GAMIFIED PEDAGOGY.pdf:application/pdf}
}

@article{tejedor-garcia_improving_2016,
	title = {Improving {L}2 {Production} with a {Gamiﬁed} {Computer}-{Assisted} {Pronunciation} {Training} {Tool}, {TipTopTalk}!},
	abstract = {We present a foreign language (L2) pronunciation training serious game, TipTopTalk!, based on the minimal-pairs technique. We carried out a three-week test experiment where participants had to overcome several challenges including exposure, discrimination and production, while using Text-To-Speech (TTS) and Automatic Speech Recognition (ASR) systems in a mobile application. The quality of users’ production is measured in order to assess their improvement. The application implements gamiﬁcation resources with the aim of promoting continued practice. Preliminary results show that users with poorer initial performance levels make relatively more progress than the rest. However, it is desirable to include speciﬁc and individualized feedback in future versions so as to avoid the performance drop detected after the protracted use of the tool.},
	author = {Tejedor-García, Cristian},
	year = {2016},
	keywords = {read},
	pages = {9},
	file = {Tejedor-Garc_Improving L2 Production with a Gamiﬁed Computer-Assisted Pronunciation Training.pdf:files/104/Tejedor-Garc_Improving L2 Production with a Gamiﬁed Computer-Assisted Pronunciation Training.pdf:application/pdf}
}

@inproceedings{tejedor-garcia_evaluating_2017,
	title = {Evaluating the {Efficiency} of {Synthetic} {Voice} for {Providing} {Corrective} {Feedback} in a {Pronunciation} {Training} {Tool} {Based} on {Minimal} {Pairs}},
	url = {http://www.isca-speech.org/archive/SLaTE_2017/abstracts/SLaTE_2017_paper_24.html},
	doi = {10.21437/SLaTE.2017-5},
	abstract = {Feedback is an important concern in Computer-Assisted Pronunciation Training (CAPT), inasmuch as it bears on a system’s capability to correct users’ input and promote improved L2 pronunciation performance in the target language. In this paper, we test the use of synthetic voice as a corrective feedback resource. A group of students used a CAPT tool for carrying out a battery of minimal-pair discrimination-production tasks; to those who failed in production routines, the system offered the possibility of undergoing extra training by using synthetic voice as a model in a round of exposure exercises. Participants who made use of this resource signiﬁcantly outperformed those who directly repeated the previously failed exercise. Results suggest that the Text-To-Speech systems offered by current operating systems (Android in our case) must be considered a relevant feedback resource in pronunciation training, especially when combined with efﬁcient teaching methods.},
	language = {en},
	urldate = {2018-03-18},
	publisher = {ISCA},
	author = {Tejedor-García, Cristian and Escudero, David and González-Ferreras, César and Cámara-Arenas, Enrique and Cardeñoso-Payo, Valentín},
	month = aug,
	year = {2017},
	pages = {25--29},
	file = {Tejedor-García_2017_Evaluating the Efficiency of Synthetic Voice for Providing Corrective Feedback.pdf:files/99/Tejedor-García_2017_Evaluating the Efficiency of Synthetic Voice for Providing Corrective Feedback.pdf:application/pdf}
}

@article{eskenazi_fluency_1998,
	title = {The {Fluency} {Pronunciation} {Trainer}},
	abstract = {In this article we describe the basis of the Fluency project for foreign language pronunciation training using automatic speech recognition. We describe the theoretical base, the interactive duration correction module, and our work toward adaptation to the way in which the user learns best. We show results in preliminary tests of the latter, and discuss future directions of the project.},
	author = {Eskenazi, Maxine and Hansma, Scott},
	year = {1998},
	keywords = {read},
	pages = {6},
	file = {Eskenazi_The Fluency Pronunciation Trainer.pdf:files/108/Eskenazi_The Fluency Pronunciation Trainer.pdf:application/pdf}
}

@incollection{romero-trillo_prosody_2012,
	address = {Dordrecht},
	title = {Prosody and {Second} {Language} {Teaching}: {Lessons} from {L}2 {Speech} {Perception} and {Production} {Research}},
	volume = {15},
	isbn = {978-94-007-3882-9 978-94-007-3883-6},
	shorttitle = {Prosody and {Second} {Language} {Teaching}},
	url = {http://www.springerlink.com/index/10.1007/978-94-007-3883-6_3},
	urldate = {2018-03-18},
	booktitle = {Pragmatics and {Prosody} in {English} {Language} {Teaching}},
	publisher = {Springer Netherlands},
	author = {Lengeris, Angelos},
	editor = {Romero-Trillo, Jesús},
	year = {2012},
	doi = {10.1007/978-94-007-3883-6_3},
	pages = {25--40},
	file = {Lengeris_2012_Prosody and Second Language Teaching.pdf:files/106/Lengeris_2012_Prosody and Second Language Teaching.pdf:application/pdf}
}

@article{neri_pedagogy-technology_2002,
	title = {The {Pedagogy}-{Technology} {Interface} in {Computer} {Assisted} {Pronunciation} {Training}},
	volume = {15},
	issn = {0958-8221},
	url = {http://www.tandfonline.com/doi/abs/10.1076/call.15.5.441.13473},
	doi = {10.1076/call.15.5.441.13473},
	abstract = {In this paper, we examine the relationship between pedagogy and technology in Computer Assisted Pronunciation Training (CAPT) courseware. First, we will analyse available literature on second language pronunciation teaching and learning in order to derive some general guidelines for effective training. Second, we will present an appraisal of various CAPT systems with a view to establishing whether they meet pedagogical requirements. In this respect, we will show that many commercial systems tend to prefer technological novelties to the detriment of pedagogical criteria that could beneﬁt the learner more. While examining the limitations of today’s technology, we will consider possible ways to deal with these shortcomings. Finally, we will combine the information thus gathered to suggest some recommendations for future CAPT.},
	number = {5},
	urldate = {2018-03-18},
	journal = {Computer Assisted Language Learning},
	author = {Neri, Ambra and Cucchiarini, Catia and Strik, Helmer and Boves, Lou},
	month = dec,
	year = {2002},
	keywords = {read},
	pages = {441--467},
	file = {Neri_2002_The Pedagogy-Technology Interface in Computer Assisted Pronunciation Training.pdf:files/109/Neri_2002_The Pedagogy-Technology Interface in Computer Assisted Pronunciation Training.pdf:application/pdf}
}

@article{pellegrino_self-imitation_2015,
	title = {Self-imitation in prosody training: {A} study on {Japanese} learners of {Italian}},
	abstract = {The proficiency in a second language is fully attained only if students have learnt to modulate the rhythmic and prosodic parameters equivalent to those of the native speakers. This study is aimed to test the pedagogical effectiveness of the selfimitation technique for the purpose of developing a native-like prosodic competence. Seven intermediate Japanese learners of Italian (NNSs) and 2 native Italian speakers (NSs) were involved in a read speech activity. NSs and NNSs were asked to read and record two Italian sentences conveying three different pragmatic functions (granting, order, request). NNSs performed the task twice, before and after the self-imitation prosodic training. The items used for the training were obtained by transferring the suprasegmental features of the native speakers, used as donors, to the Japanese learners, considered as the receivers. During the training phase, Japanese learners mimic their utterances previously modified to match the prosody of the reference native speaker, and then recorded the new performance. Seventeen native Italian listeners rated pre- and post-training productions for pragmatic function and accentedness. The results indicate that selfimitation promoted an improvement in learners' performances in terms of communicative effectiveness. Conversely, average rate of accentedness does not change significantly before and after training.},
	author = {Pellegrino, Elisa and Vigliano, Debora},
	year = {2015},
	pages = {5},
	file = {Pellegrino_2015_Self-imitation in prosody training.pdf:files/107/Pellegrino_2015_Self-imitation in prosody training.pdf:application/pdf}
}

@article{darcy_bringing_2012,
	title = {Bringing pronunciation instruction back into the classroom: an {ESL} teachers' pronunciation "toolbox"},
	author = {Darcy, Isabelle and Ewert, Doreen and Lidster, Ryan},
	year = {2012},
	keywords = {read},
	pages = {18},
	file = {Darcy_2012_BRINGING PRONUNCIATION INSTRUCTION BACK INTO THE CLASSROOM.pdf:files/112/Darcy_2012_BRINGING PRONUNCIATION INSTRUCTION BACK INTO THE CLASSROOM.pdf:application/pdf}
}

@article{gangireddy_prosodically-enhanced_2015,
	title = {Prosodically-enhanced {Recurrent} {Neural} {Network} {Language} {Models}},
	abstract = {Recurrent neural network language models have been shown to consistently reduce the word error rates (WERs) of large vocabulary speech recognition tasks. In this work we propose to enhance the RNNLMs with prosodic features computed using the context of the current word. Since it is plausible to compute the prosody features at the word and syllable level we have trained the models on prosody features computed at both these levels. To investigate the effectiveness of proposed models we report perplexity and WER for two speech recognition tasks, Switchboard and TED. We observed substantial improvements in perplexity and small improvements in WER.},
	author = {Gangireddy, Siva Reddy and Renals, Steve and Nankaku, Yoshihiko and Lee, Akinobu},
	year = {2015},
	pages = {5},
	file = {Gangireddy et al. - Prosodically-enhanced Recurrent Neural Network Lan.pdf:files/114/Gangireddy et al. - Prosodically-enhanced Recurrent Neural Network Lan.pdf:application/pdf}
}

@article{gauthier_learning_2009,
	title = {Learning {Prosodic} {Focus} from {Continuous} {Speech} {Input}:{A} {Neural} {Network} {Exploration}},
	volume = {5},
	issn = {1547-5441, 1547-3341},
	shorttitle = {Learning {Prosodic} {Focus} from {Continuous} {Speech} {Input}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/15475440802698524},
	doi = {10.1080/15475440802698524},
	language = {en},
	number = {2},
	urldate = {2018-03-27},
	journal = {Language Learning and Development},
	author = {Gauthier, Bruno and Shi, Rushen and Xu, Yi},
	month = apr,
	year = {2009},
	pages = {94--114},
	file = {Gauthier et al. - 2009 - Learning Prosodic Focus from Continuous Speech Inp.pdf:files/116/Gauthier et al. - 2009 - Learning Prosodic Focus from Continuous Speech Inp.pdf:application/pdf}
}

@article{bernardy_modelling_2017,
	title = {Modelling prosodic structure using {Artificial} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.03952},
	abstract = {The ability to accurately perceive whether a speaker is asking a question or is making a statement is crucial for any successful interaction. However, learning and classifying tonal patterns has been a challenging task for automatic speech recognition and for models of tonal representation, as tonal contours are characterized by significant variation. This paper provides a classification model of Cypriot Greek questions and statements. We evaluate two state-of-the-art network architectures: a Long Short-Term Memory (LSTM) network and a convolutional network (ConvNet). The ConvNet outperforms the LSTM in the classification task and exhibited an excellent performance with 95\% classification accuracy.},
	urldate = {2018-03-27},
	journal = {arXiv:1706.03952 [cs]},
	author = {Bernardy, Jean-Philippe and Themistocleous, Charalambos},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03952},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:files/122/1706.html:text/html;Bernardy_2017_Modelling prosodic structure using Artificial Neural Networks.pdf:files/121/Bernardy_2017_Modelling prosodic structure using Artificial Neural Networks.pdf:application/pdf}
}

@phdthesis{vainio_artificial_2001,
	address = {Helsinki},
	title = {Artificial neural network based prosody models for {Finnish} text-to-speech synthesis},
	language = {English},
	school = {University of Helsinki},
	author = {Vainio, Martti},
	year = {2001},
	note = {OCLC: 58373280},
	file = {Vainio - 2001 - Artificial neural network based prosody models for.pdf:files/123/Vainio - 2001 - Artificial neural network based prosody models for.pdf:application/pdf}
}

@article{han_speech_2014,
	title = {Speech {Emotion} {Recognition} {Using} {Deep} {Neural} {Network} and {Extreme} {Learning} {Machine}},
	abstract = {Speech emotion recognition is a challenging problem partly because it is unclear what features are effective for the task. In this paper we propose to utilize deep neural networks (DNNs) to extract high level features from raw data and show that they are effective for speech emotion recognition. We ﬁrst produce an emotion state probability distribution for each speech segment using DNNs. We then construct utterance-level features from segment-level probability distributions. These utterancelevel features are then fed into an extreme learning machine (ELM), a special simple and efﬁcient single-hidden-layer neural network, to identify utterance-level emotions. The experimental results demonstrate that the proposed approach effectively learns emotional information from low-level features and leads to 20\% relative accuracy improvement compared to the stateof-the-art approaches.},
	author = {Han, Kun and Yu, Dong and Tashev, Ivan},
	year = {2014},
	pages = {5},
	file = {Han et al. - Speech Emotion Recognition Using Deep Neural Netwo.pdf:files/125/Han et al. - Speech Emotion Recognition Using Deep Neural Netwo.pdf:application/pdf}
}

@inproceedings{fu_integrating_2015,
	title = {Integrating prosodic information into recurrent neural network language model for speech recognition},
	isbn = {978-988-14768-0-7},
	url = {http://ieeexplore.ieee.org/document/7415462/},
	doi = {10.1109/APSIPA.2015.7415462},
	abstract = {Prosody is a kind of cues that are critical to human speech perception and comprehension, so it is plausible to integrate prosodic information into machine speech recognition. However, as a result of the supra-segmental nature, it is hard to integrate prosodic information with conventional acoustic features. Recently, RNNLMs have shown to be the state-of-theart language model in many tasks. We thus attempt to integrate prosodic information into RNNLMs for improving speech recognition performance based on rescoring strategy. Firstly, three word-level prosodic features are extracted from speech and then passed to RNNLMs separately. Therefore RNNLMs predict the next word based on prosodic features and word history. Experiments conducted on LibriSpeech Corpus show that the word error rate decreases from 8.07\% to 7.96\%. Secondly, prosodic information is combined on feature-level and modellevel for further improvements and word error rate decreases 4.71\% relatively.},
	language = {en},
	urldate = {2018-03-29},
	publisher = {IEEE},
	author = {Fu, Tong and Han, Yang and Li, Xiangang and Liu, Yi and Wu, Xihong},
	month = dec,
	year = {2015},
	pages = {1194--1197},
	file = {Fu et al. - 2015 - Integrating prosodic information into recurrent ne.pdf:files/130/Fu et al. - 2015 - Integrating prosodic information into recurrent ne.pdf:application/pdf}
}

@article{mao_learning_2014,
	title = {Learning {Salient} {Features} for {Speech} {Emotion} {Recognition} {Using} {Convolutional} {Neural} {Networks}},
	volume = {16},
	issn = {1520-9210, 1941-0077},
	url = {http://ieeexplore.ieee.org/document/6913013/},
	doi = {10.1109/TMM.2014.2360798},
	abstract = {As an essential way of human emotional behavior understanding, speech emotion recognition (SER) has attracted a great deal of attention in human-centered signal processing. Accuracy in SER heavily depends on ﬁnding good affect-related, discriminative features. In this paper, we propose to learn affect-salient features for SER using convolutional neural networks (CNN). The training of CNN involves two stages. In the ﬁrst stage, unlabeled samples are used to learn local invariant features (LIF) using a variant of sparse auto-encoder (SAE) with reconstruction penalization. In the second step, LIF is used as the input to a feature extractor, salient discriminative feature analysis (SDFA), to learn affect-salient, discriminative features using a novel objective function that encourages feature saliency, orthogonality, and discrimination for SER. Our experimental results on benchmark datasets show that our approach leads to stable and robust recognition performance in complex scenes (e.g., with speaker and language variation, and environment distortion) and outperforms several well-established SER features.},
	language = {en},
	number = {8},
	urldate = {2018-03-29},
	journal = {IEEE Transactions on Multimedia},
	author = {Mao, Qirong and Dong, Ming and Huang, Zhengwei and Zhan, Yongzhao},
	month = dec,
	year = {2014},
	pages = {2203--2213},
	file = {Mao et al. - 2014 - Learning Salient Features for Speech Emotion Recog.pdf:files/132/Mao et al. - 2014 - Learning Salient Features for Speech Emotion Recog.pdf:application/pdf}
}

@inproceedings{stuhlsatz_deep_2011,
	title = {Deep neural networks for acoustic emotion recognition: {Raising} the benchmarks},
	isbn = {978-1-4577-0538-0},
	shorttitle = {Deep neural networks for acoustic emotion recognition},
	url = {http://ieeexplore.ieee.org/document/5947651/},
	doi = {10.1109/ICASSP.2011.5947651},
	abstract = {Deep Neural Networks (DNNs) denote multilayer artiﬁcial neural networks with more than one hidden layer and millions of free parameters. We propose a Generalized Discriminant Analysis (GerDA) based on DNNs to learn discriminative features of low dimension optimized with respect to a fast classiﬁcation from a large set of acoustic features for emotion recognition. On nine frequently used emotional speech corpora, we compare the performance of GerDA features and their subsequent linear classiﬁcation with previously reported benchmarks obtained using the same set of acoustic features classiﬁed by Support Vector Machines (SVMs). Our results impressively show that low-dimensional GerDA features capture hidden information from the acoustic features leading to a signiﬁcantly raised unweighted average recall and considerably raised weighted average recall.},
	language = {en},
	urldate = {2018-03-29},
	publisher = {IEEE},
	author = {Stuhlsatz, Andre and Meyer, Christine and Eyben, Florian and Zielke, Thomas and Meier, Gunter and Schuller, Bjorn},
	month = may,
	year = {2011},
	pages = {5688--5691},
	file = {Stuhlsatz et al. - 2011 - Deep neural networks for acoustic emotion recognit.pdf:files/134/Stuhlsatz et al. - 2011 - Deep neural networks for acoustic emotion recognit.pdf:application/pdf}
}

@inproceedings{luengo_automatic_2005,
	title = {Automatic {Emotion} {Recognition} using {Prosodic} {Parameters}},
	abstract = {This paper presents the experiments made to automatically identify emotion in an emotional speech database for Basque. Three different classifiers have been built: one using spectral features and GMM, other with prosodic features and SVM and the last one with prosodic features and GMM. 86 prosodic features were calculated and then an algorithm to select the most relevant ones was applied. The first classifier gives the best result with a 98.4 \% accuracy when using 512 mixtures, but the classifier built with the best 6 prosodic features achieves an accuracy of 92.3 \% in spite of its simplicity, showing that prosodic information is very useful to identify emotions. 1.},
	booktitle = {in {Proc}. of {INTERSPEECH}},
	author = {Luengo, Iker and Navas, Eva and Hernáez, Inmaculada and Sánchez, Jon},
	year = {2005},
	pages = {493--496},
	file = {Citeseer - Snapshot:files/139/summary.html:text/html;Luengo_2005_Automatic Emotion Recognition using Prosodic Parameters.pdf:files/141/Luengo_2005_Automatic Emotion Recognition using Prosodic Parameters.pdf:application/pdf}
}

@article{skerry-ryan_towards_2018,
	title = {Towards {End}-to-{End} {Prosody} {Transfer} for {Expressive} {Speech} {Synthesis} with {Tacotron}},
	url = {http://arxiv.org/abs/1803.09047},
	abstract = {We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.},
	urldate = {2018-04-03},
	journal = {arXiv:1803.09047 [cs, eess]},
	author = {Skerry-Ryan, R. J. and Battenberg, Eric and Xiao, Ying and Wang, Yuxuan and Stanton, Daisy and Shor, Joel and Weiss, Ron J. and Clark, Rob and Saurous, Rif A.},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.09047},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:files/166/1803.html:text/html;Skerry-Ryan_2018_Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with.pdf:files/162/Skerry-Ryan_2018_Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with.pdf:application/pdf}
}

@article{wang_style_2018,
	title = {Style {Tokens}: {Unsupervised} {Style} {Modeling}, {Control} and {Transfer} in {End}-to-{End} {Speech} {Synthesis}},
	shorttitle = {Style {Tokens}},
	url = {http://arxiv.org/abs/1803.09017},
	abstract = {In this work, we propose "global style tokens" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable "labels" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.},
	urldate = {2018-04-03},
	journal = {arXiv:1803.09017 [cs, eess]},
	author = {Wang, Yuxuan and Stanton, Daisy and Zhang, Yu and Skerry-Ryan, R. J. and Battenberg, Eric and Shor, Joel and Xiao, Ying and Ren, Fei and Jia, Ye and Saurous, Rif A.},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.09017},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:files/170/1803.html:text/html;Wang_2018_Style Tokens.pdf:files/169/Wang_2018_Style Tokens.pdf:application/pdf}
}

@article{wang_uncovering_2017,
	title = {Uncovering {Latent} {Style} {Factors} for {Expressive} {Speech} {Synthesis}},
	url = {http://arxiv.org/abs/1711.00520},
	abstract = {Prosodic modeling is a core problem in speech synthesis. The key challenge is producing desirable prosody from textual input containing only phonetic information. In this preliminary study, we introduce the concept of "style tokens" in Tacotron, a recently proposed end-to-end neural speech synthesis model. Using style tokens, we aim to extract independent prosodic styles from training data. We show that without annotation data or an explicit supervision signal, our approach can automatically learn a variety of prosodic variations in a purely data-driven way. Importantly, each style token corresponds to a fixed style factor regardless of the given text sequence. As a result, we can control the prosodic style of synthetic speech in a somewhat predictable and globally consistent way.},
	urldate = {2018-04-04},
	journal = {arXiv:1711.00520 [cs]},
	author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Xiao, Ying and Stanton, Daisy and Shor, Joel and Battenberg, Eric and Clark, Rob and Saurous, Rif A.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00520},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
	file = {arXiv.org Snapshot:files/179/1711.html:text/html;Wang_2017_Uncovering Latent Style Factors for Expressive Speech Synthesis.pdf:files/178/Wang_2017_Uncovering Latent Style Factors for Expressive Speech Synthesis.pdf:application/pdf}
}

@article{wang_tacotron:_2017,
	title = {Tacotron: {Towards} {End}-to-{End} {Speech} {Synthesis}},
	shorttitle = {Tacotron},
	url = {http://arxiv.org/abs/1703.10135},
	abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
	urldate = {2018-04-04},
	journal = {arXiv:1703.10135 [cs]},
	author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.10135},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Sound},
	file = {arXiv.org Snapshot:files/191/1703.html:text/html;Wang_2017_Tacotron.pdf:files/190/Wang_2017_Tacotron.pdf:application/pdf}
}

@article{arik_neural_2018,
	title = {Neural {Voice} {Cloning} with a {Few} {Samples}},
	url = {http://arxiv.org/abs/1802.06006},
	abstract = {Voice cloning is a highly desired feature for personalized speech interfaces. Neural network based speech synthesis has been shown to generate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio samples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios and to be used with a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios. While speaker adaptation can achieve better naturalness and similarity, the cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment.},
	urldate = {2018-04-04},
	journal = {arXiv:1802.06006 [cs, eess]},
	author = {Arik, Sercan O. and Chen, Jitong and Peng, Kainan and Ping, Wei and Zhou, Yanqi},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06006},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Arik_2018_Neural Voice Cloning with a Few Samples.pdf:files/196/Arik_2018_Neural Voice Cloning with a Few Samples2.pdf:application/pdf;arXiv.org Snapshot:files/197/1802.html:text/html}
}

@article{nguyen_high_2016,
	title = {High quality voice conversion using prosodic and high-resolution spectral features},
	volume = {75},
	issn = {1380-7501, 1573-7721},
	url = {http://arxiv.org/abs/1512.01809},
	doi = {10.1007/s11042-015-3039-x},
	abstract = {Voice conversion methods have advanced rapidly over the last decade. Studies have shown that speaker characteristics are captured by spectral feature as well as various prosodic features. Most existing conversion methods focus on the spectral feature as it directly represents the timbre characteristics, while some conversion methods have focused only on the prosodic feature represented by the fundamental frequency. In this paper, a comprehensive framework using deep neural networks to convert both timbre and prosodic features is proposed. The timbre feature is represented by a high-resolution spectral feature. The prosodic features include F0, intensity and duration. It is well known that DNN is useful as a tool to model high-dimensional features. In this work, we show that DNN initialized by our proposed autoencoder pretraining yields good quality DNN conversion models. This pretraining is tailor-made for voice conversion and leverages on autoencoder to capture the generic spectral shape of source speech. Additionally, our framework uses segmental DNN models to capture the evolution of the prosodic features over time. To reconstruct the converted speech, the spectral feature produced by the DNN model is combined with the three prosodic features produced by the DNN segmental models. Our experimental results show that the application of both prosodic and high-resolution spectral features leads to quality converted speech as measured by objective evaluation and subjective listening tests.},
	number = {9},
	urldate = {2018-04-06},
	journal = {Multimedia Tools and Applications},
	author = {Nguyen, Hy Quy and Lee, Siu Wa and Tian, Xiaohai and Dong, Minghui and Chng, Eng Siong},
	month = may,
	year = {2016},
	note = {arXiv: 1512.01809},
	keywords = {Computer Science - Sound},
	pages = {5265--5285},
	file = {arXiv.org Snapshot:files/202/1512.html:text/html;Nguyen_2016_High quality voice conversion using prosodic and high-resolution spectral.pdf:files/201/Nguyen_2016_High quality voice conversion using prosodic and high-resolution spectral.pdf:application/pdf}
}

@article{chorowski_using_2017,
	title = {On {Using} {Backpropagation} for {Speech} {Texture} {Generation} and {Voice} {Conversion}},
	url = {http://arxiv.org/abs/1712.08363},
	abstract = {Inspired by recent work on neural network image generation which rely on backpropagation towards the network inputs, we present a proof-of-concept system for speech texture synthesis and voice conversion based on two mechanisms: approximate inversion of the representation learned by a speech recognition neural network, and on matching statistics of neuron activations between different source and target utterances. Similar to image texture synthesis and neural style transfer, the system works by optimizing a cost function with respect to the input waveform samples. To this end we use a differentiable mel-filterbank feature extraction pipeline and train a convolutional CTC speech recognition network. Our system is able to extract speaker characteristics from very limited amounts of target speaker data, as little as a few seconds, and can be used to generate realistic speech babble or reconstruct an utterance in a different voice.},
	urldate = {2018-04-06},
	journal = {arXiv:1712.08363 [cs, eess, stat]},
	author = {Chorowski, Jan and Weiss, Ron J. and Saurous, Rif A. and Bengio, Samy},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.08363},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:files/207/1712.html:text/html;Chorowski_2017_On Using Backpropagation for Speech Texture Generation and Voice Conversion.pdf:files/206/Chorowski_2017_On Using Backpropagation for Speech Texture Generation and Voice Conversion.pdf:application/pdf}
}

@article{mohammadi_overview_2017,
	title = {An overview of voice conversion systems},
	volume = {88},
	issn = {01676393},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167639315300698},
	doi = {10.1016/j.specom.2017.01.008},
	abstract = {Voice transformation (VT) aims to change one or more aspects of a speech signal while preserving linguistic information. A subset of VT, Voice conversion (VC) speciﬁcally aims to change a source speaker’s speech in such a way that the generated output is perceived as a sentence uttered by a target speaker. Despite many years of research, VC systems still exhibit deﬁciencies in accurately mimicking a target speaker spectrally and prosodically, and simultaneously maintaining high speech quality. In this work we provide an overview of real-world applications, extensively study existing systems proposed in the literature, and discuss remaining challenges.},
	language = {en},
	urldate = {2018-04-07},
	journal = {Speech Communication},
	author = {Mohammadi, Seyed Hamidreza and Kain, Alexander},
	month = apr,
	year = {2017},
	keywords = {read},
	pages = {65--82},
	file = {Mohammadi and Kain - 2017 - An overview of voice conversion systems.pdf:files/209/Mohammadi and Kain - 2017 - An overview of voice conversion systems.pdf:application/pdf}
}

@inproceedings{luo_emotional_2016,
	title = {Emotional voice conversion using deep neural networks with {MCC} and {F}0 features},
	doi = {10.1109/ICIS.2016.7550889},
	abstract = {An artificial neural network is one of the most important models for training features in a voice conversion task. Typically, Neural Networks (NNs) are not effective in processing low-dimensional F0 features, thus this causes that the performance of those methods based on neural networks for training Mel Cepstral Coefficients (MCC) are not outstanding. However, F0 can robustly represent various prosody signals (e.g., emotional prosody). In this study, we propose an effective method based on the NNs to train the normalized-segment-F0 features (NSF0) for emotional prosody conversion. Meanwhile, the proposed method adopts deep belief networks (DBNs) to train spectrum features for voice conversion. By using these approaches, the proposed method can change the spectrum and the prosody for the emotional voice at the same time. Moreover, the experimental results show that the proposed method outperforms other state-of-the-art methods for voice emotional conversion.},
	booktitle = {2016 {IEEE}/{ACIS} 15th {International} {Conference} on {Computer} and {Information} {Science} ({ICIS})},
	author = {Luo, Z. and Takiguchi, T. and Ariki, Y.},
	month = jun,
	year = {2016},
	keywords = {artificial neural network, Artificial neural networks, belief networks, cepstral analysis, DBN, deep belief network, deep neural network, emotional voice conversion, F0 feature, feature extraction, Feature extraction, MCC, mel cepstral coefficient, Mel frequency cepstral coefficient, neural nets, NN, prosody signal, spectrum feature training, Speech, speech processing, Training, Transforms},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:files/212/7550889.html:text/html}
}

@inproceedings{aryal_can_2014,
	title = {Can voice conversion be used to reduce non-native accents?},
	isbn = {978-1-4799-2893-4},
	url = {http://ieeexplore.ieee.org/document/6855134/},
	doi = {10.1109/ICASSP.2014.6855134},
	abstract = {Voice-conversion (VC) techniques aim to transform utterances from a source speaker to sound as if they had been produced by a target speaker. This includes not only organic properties (i.e., voice quality) but also linguistic cues (i.e., regional accents) of the target speaker. For this reason, VC is generally ill-suited for accent-conversion (AC) purposes, where the goal is to capture the voice quality of the target speaker but the regional accent of the source speaker. In this paper, we propose a modification of the conventional training process for VC that allows it to perform as an AC transform. The approach consists of pairing source and target vectors based not on their ordering within a parallel corpus, as is commonly done in VC, but based on their linguistic similarity. We validate the AC approach on a corpus containing native-accented and Spanish-accented utterances, and compare it against conventional VC through a series of perceptual listening tests. We also analyze the extent to which phonological differences between the two languages (Spanish and American English) help predict the relative performance of the two methods.},
	language = {en},
	urldate = {2018-04-07},
	publisher = {IEEE},
	author = {Aryal, Sandesh and Gutierrez-Osuna, Ricardo},
	month = may,
	year = {2014},
	pages = {7879--7883},
	file = {Aryal and Gutierrez-Osuna - 2014 - Can voice conversion be used to reduce non-native .pdf:files/213/Aryal and Gutierrez-Osuna - 2014 - Can voice conversion be used to reduce non-native .pdf:application/pdf}
}

@article{felps_developing_2010,
	title = {Developing objective measures of foreign-accent conversion},
	abstract = {Abstract—Various methods have recently appeared to transform foreign-accented speech into its native-accented counterpart. Eval-uation of these accent conversion methods requires extensive lis-tening tests across a number of perceptual dimensions. This article presents three objective measures that may be used to assess the acoustic quality, degree of foreign accent, and speaker identity of accent-converted utterances. Accent conversion generates novel ut-terances: those of a foreign speaker with a native accent. There-fore, the acoustic quality in accent conversion cannot be evaluated with conventional measures of spectral distortion, which assume that a clean recording of the speech signal is available for compar-ison. Here we evaluate a single-ended measure of speech quality, ITU-T recommendation P.563 for narrow-band telephony. We also propose a measure of foreign accent that exploits a weakness of automatic speech recognizers: their sensitivity to foreign accents. Namely, we use phoneme-level match scores given by the HTK rec-ognizer trained on a large number of English American speakers to obtain a measure of native accent. Finally, we propose a measure of speaker identity that projects acoustic vectors (e.g., Mel cep-stral, F0) onto the linear discriminant that maximizes separability for a given pair of source and target speakers. The three measures are evaluated on a corpus of accent-converted utterances that had been previously rated through perceptual tests. Our results show that the three measures have a high degree of correlation with their corresponding subjective ratings, suggesting that they may be used to accelerate the development of foreign-accent conversion tools. Applications of these measures in the context of computer assisted pronunciation training and voice conversion are also discussed. Index Terms—Accent conversion, foreign accent recognition, speaker recognition, voice conversion. I.},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Felps, Daniel and Member, Student and Gutierrez-osuna, Ricardo and Member, Senior},
	year = {2010},
	pages = {1030--1040},
	file = {Citeseer - Snapshot:files/217/summary.html:text/html;Felps_2010_Developing objective measures of foreign-accent conversion.pdf:files/219/Felps_2010_Developing objective measures of foreign-accent conversion.pdf:application/pdf}
}

@article{aryal_foreign_2010,
	title = {Foreign {Accent} {Conversion} {Through} {Voice} {Morphing}},
	abstract = {We present a voice morphing strategy that can be used to generate a continuum of accent transformations between a foreign speaker and a native speaker. The approach performs a cepstral decomposition of speech into spectral slope and spectral detail. Accent conversions are then generated by combining the spectral slope of the foreign speaker with a morph of the spectral detail of the native speaker. Spectral morphing is achieved by representing the spectral detail through pulse density modulation and averaging pulses in a pair-wise fashion. The technique is validated on parallel recordings from two ARCTIC speakers using both objective and subjective measures of acoustic quality, speaker identity and foreign accent.},
	language = {en},
	author = {Aryal, Sandesh and Felps, Daniel and Gutierrez-Osuna, Ricardo},
	year = {2010},
	pages = {5},
	file = {Aryal et al. - Foreign Accent Conversion Through Voice Morphing.pdf:files/220/Aryal et al. - Foreign Accent Conversion Through Voice Morphing.pdf:application/pdf}
}

@inproceedings{aryal_accent_2014,
	title = {Accent conversion through cross-speaker articulatory synthesis},
	isbn = {978-1-4799-2893-4},
	url = {http://ieeexplore.ieee.org/document/6855097/},
	doi = {10.1109/ICASSP.2014.6855097},
	abstract = {Accent conversion (AC) seeks to transform second-language (L2) utterances to appear as if produced with a native (L1) accent. In the acoustic domain, AC is difficult due to the complex interaction between linguistic content and voice quality. Alternatively, AC can be performed in the articulatory domain by building a mapping from L2 articulators to L2 acoustics, and then driving the model with L1 articulators. However, collecting articulatory data for each L2 learner is impractical. Here we propose an approach that avoids this expensive step. Our method builds a cross-speaker forward mapping (CSFM) to generate L2 acoustic observations directly from L1 articulatory trajectories. We evaluated the CSFM against a baseline articulatory synthesizer trained with L2 articulators. Subjective listening tests show that both methods perform comparably in terms of accent reduction and ability to preserve the voice quality of the L2 speaker, with only a small impact in acoustic quality.},
	language = {en},
	urldate = {2018-04-11},
	publisher = {IEEE},
	author = {Aryal, Sandesh and Gutierrez-Osuna, Ricardo},
	month = may,
	year = {2014},
	pages = {7694--7698},
	file = {Aryal and Gutierrez-Osuna - 2014 - Accent conversion through cross-speaker articulato.pdf:files/222/Aryal and Gutierrez-Osuna - 2014 - Accent conversion through cross-speaker articulato.pdf:application/pdf}
}

@inproceedings{toda_voice_2016,
	title = {The {Voice} {Conversion} {Challenge} 2016},
	url = {http://www.isca-speech.org/archive/Interspeech_2016/abstracts/1066.html},
	doi = {10.21437/Interspeech.2016-1066},
	abstract = {This paper describes the Voice Conversion Challenge 2016 devised by the authors to better understand different voice conversion (VC) techniques by comparing their performance on a common dataset. The task of the challenge was speaker conversion, i.e., to transform the voice identity of a source speaker into that of a target speaker while preserving the linguistic content. Using a common dataset consisting of 162 utterances for training and 54 utterances for evaluation from each of 5 source and 5 target speakers, 17 groups working in VC around the world developed their own VC systems for every combination of the source and target speakers, i.e., 25 systems in total, and generated voice samples converted by the developed systems. These samples were evaluated in terms of target speaker similarity and naturalness by 200 listeners in a controlled environment. This paper summarizes the design of the challenge, its result, and a future plan to share views about unsolved problems and challenges faced by the current VC techniques.},
	language = {en},
	urldate = {2018-04-11},
	author = {Toda, Tomoki and Chen, Ling-Hui and Saito, Daisuke and Villavicencio, Fernando and Wester, Mirjam and Wu, Zhizheng and Yamagishi, Junichi},
	month = sep,
	year = {2016},
	pages = {1632--1636},
	file = {Toda et al. - 2016 - The Voice Conversion Challenge 2016.pdf:files/226/Toda et al. - 2016 - The Voice Conversion Challenge 2016.pdf:application/pdf}
}

@article{luo_emotional_2017,
	title = {Emotional voice conversion using neural networks with arbitrary scales {F}0 based on wavelet transform},
	volume = {2017},
	issn = {1687-4722},
	url = {https://link.springer.com/article/10.1186/s13636-017-0116-2},
	doi = {10.1186/s13636-017-0116-2},
	abstract = {An artificial neural network is an important model for training features of voice conversion (VC) tasks. Typically, neural networks (NNs) are very effective in processing nonlinear features, such as Mel Cepstral Coefficients (MCC), which represent the spectrum features. However, a simple representation of fundamental frequency (F0) is not enough for NNs to deal with emotional voice VC. This is because the time sequence of F0 for an emotional voice changes drastically. Therefore, in our previous method, we used the continuous wavelet transform (CWT) to decompose F0 into 30 discrete scales, each separated by one third of an octave, which can be trained by NNs for prosody modeling in emotional VC. In this study, we propose the arbitrary scales CWT (AS-CWT) method to systematically capture F0 features of different temporal scales, which can represent different prosodic levels ranging from micro-prosody to sentence levels. Meanwhile, the proposed method uses deep belief networks (DBNs) to pre-train the NNs that then convert spectral features. By utilizing these approaches, the proposed method can change the spectrum and the F0 for an emotional voice simultaneously as well as outperform other state-of-the-art methods in terms of emotional VC.},
	language = {en},
	number = {1},
	urldate = {2018-04-13},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Luo, Zhaojie and Chen, Jinhui and Takiguchi, Tetsuya and Ariki, Yasuo},
	month = dec,
	year = {2017},
	pages = {18},
	file = {Luo_2017_Emotional voice conversion using neural networks with arbitrary scales F0 based.pdf:files/231/Luo_2017_Emotional voice conversion using neural networks with arbitrary scales F0 based.pdf:application/pdf;Snapshot:files/232/s13636-017-0116-2.html:text/html}
}

@article{lorenzo-trueba_voice_2018,
	title = {The {Voice} {Conversion} {Challenge} 2018: {Promoting} {Development} of {Parallel} and {Nonparallel} {Methods}},
	shorttitle = {The {Voice} {Conversion} {Challenge} 2018},
	url = {http://arxiv.org/abs/1804.04262},
	abstract = {We present the Voice Conversion Challenge 2018, designed as a follow up to the 2016 edition with the aim of providing a common framework for evaluating and comparing different state-of-the-art voice conversion (VC) systems. The objective of the challenge was to perform speaker conversion (i.e. transform the vocal identity) of a source speaker to a target speaker while maintaining linguistic information. As an update to the previous challenge, we considered both parallel and non-parallel data to form the Hub and Spoke tasks, respectively. A total of 23 teams from around the world submitted their systems, 11 of them additionally participated in the optional Spoke task. A large-scale crowdsourced perceptual evaluation was then carried out to rate the submitted converted speech in terms of naturalness and similarity to the target speaker identity. In this paper, we present a brief summary of the state-of-the-art techniques for VC, followed by a detailed explanation of the challenge tasks and the results that were obtained.},
	urldate = {2018-04-13},
	journal = {arXiv:1804.04262 [cs, eess, stat]},
	author = {Lorenzo-Trueba, Jaime and Yamagishi, Junichi and Toda, Tomoki and Saito, Daisuke and Villavicencio, Fernando and Kinnunen, Tomi and Ling, Zhenhua},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.04262},
	keywords = {Computer Science - Computation and Language, read, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:files/237/1804.html:text/html;Lorenzo-Trueba_2018_The Voice Conversion Challenge 2018.pdf:files/236/Lorenzo-Trueba_2018_The Voice Conversion Challenge 2018.pdf:application/pdf}
}

@article{azarov_real-time_2013,
	title = {Real-{Time} {Voice} {Conversion} {Using} {Artificial} {Neural} {Networks} with {Rectified} {Linear} {Units}},
	abstract = {This paper presents an approach to parametric voice conversion that can be used in real-time entertainment applications. The approach is based on spectral mapping using an artificial neural network (ANN) with rectified linear units (ReLU). To overcome the oversmoothing problem a special network configuration is proposed that utilizes temporal states of the speaker. The speech is represented using the harmonic plus noise model. The parameters of the model are estimated using instantaneous harmonic parameters. Using objective and subjective measures the proposed voice conversion technique is compared to the main alternative approaches.},
	language = {en},
	author = {Azarov, Elias and Vashkevich, Maxim and Likhachov, Denis and Petrovsky, Alexander},
	year = {2013},
	pages = {5},
	file = {Azarov et al. - Real-Time Voice Conversion Using Artificial Neural.pdf:files/238/Azarov et al. - Real-Time Voice Conversion Using Artificial Neural.pdf:application/pdf}
}

@inproceedings{sun_voice_2015,
	title = {Voice conversion using deep {Bidirectional} {Long} {Short}-{Term} {Memory} based {Recurrent} {Neural} {Networks}},
	doi = {10.1109/ICASSP.2015.7178896},
	abstract = {This paper investigates the use of Deep Bidirectional Long Short-Term Memory based Recurrent Neural Networks (DBLSTM-RNNs) for voice conversion. Temporal correlations across speech frames are not directly modeled in frame-based methods using conventional Deep Neural Networks (DNNs), which results in a limited quality of the converted speech. To improve the naturalness and continuity of the speech output in voice conversion, we propose a sequence-based conversion method using DBLSTM-RNNs to model not only the frame-wised relationship between the source and the target voice, but also the long-range context-dependencies in the acoustic trajectory. Experiments show that DBLSTM-RNNs outperform DNNs where Mean Opinion Scores are 3.2 and 2.3 respectively. Also, DBLSTM-RNNs without dynamic features have better performance than DNNs with dynamic features.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Sun, L. and Kang, S. and Li, K. and Meng, H.},
	month = apr,
	year = {2015},
	keywords = {Speech, speech processing, Training, acoustic trajectory, Acoustics, bidirectional long short-term memory, Context, DBLSTM-RNNs, deep bidirectional long short-term memory based recurrent neural networks, DNNs, dynamic features, frame-based methods, Logic gates, long-range context-dependency, mean opinion scores, recurrent neural nets, recurrent neural networks, Recurrent neural networks, sequence-based conversion method, speech frames, temporal correlations, voice conversion},
	pages = {4869--4873},
	file = {IEEE Xplore Abstract Record:files/243/7178896.html:text/html;Sun_2015_Voice conversion using deep Bidirectional Long Short-Term Memory based.pdf:files/242/Sun_2015_Voice conversion using deep Bidirectional Long Short-Term Memory based.pdf:application/pdf}
}

@inproceedings{desai_voice_2009,
	title = {Voice conversion using {Artificial} {Neural} {Networks}},
	doi = {10.1109/ICASSP.2009.4960478},
	abstract = {In this paper, we propose to use artificial neural networks (ANN) for voice conversion. We have exploited the mapping abilities of ANN to perform mapping of spectral features of a source speaker to that of a target speaker. A comparative study of voice conversion using ANN and the state-of-the-art Gaussian mixture model (GMM) is conducted. The results of voice conversion evaluated using subjective and objective measures confirm that ANNs perform better transformation than GMMs and the quality of the transformed speech is intelligible and has the characteristics of the target speaker.},
	booktitle = {2009 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Desai, S. and Raghavendra, E. V. and Yegnanarayana, B. and Black, A. W. and Prahallad, K.},
	month = apr,
	year = {2009},
	keywords = {Artificial neural networks, neural nets, speech processing, voice conversion, ANN, artificial neural networks, Artificial Neural Networks, Books, Data mining, Databases, Filters, Frequency estimation, Gaussian mixture model, Gaussian Mixture Model, Gaussian processes, Loudspeakers, source speaker, spectral analysis, spectral feature mapping, speech intelligibility, Speech synthesis, target speaker, Training data, Vectors, Voice conversion},
	pages = {3893--3896},
	file = {Desai_2009_Voice conversion using Artificial Neural Networks.pdf:files/246/Desai_2009_Voice conversion using Artificial Neural Networks.pdf:application/pdf;IEEE Xplore Abstract Record:files/247/4960478.html:text/html}
}

@article{chen_voice_2014,
	title = {Voice {Conversion} {Using} {Deep} {Neural} {Networks} with {Layer}-wise {Generative} {Training}},
	volume = {22},
	issn = {2329-9290},
	url = {http://dx.doi.org/10.1109/TASLP.2014.2353991},
	doi = {10.1109/TASLP.2014.2353991},
	abstract = {This paper presents a new spectral envelope conversion method using deep neural networks (DNNs). The conventional joint density Gaussian mixture model (JDGMM) based spectral conversion methods perform stably and effectively. However, the speech generated by these methods suffer severe quality degradation due to the following two factors: 1) inadequacy of JDGMM in modeling the distribution of spectral features as well as the non-linear mapping relationship between the source and target speakers, 2) spectral detail loss caused by the use of high-level spectral features such as mel-cepstra. Previously, we have proposed to use the mixture of restricted Boltzmann machines (MoRBM) and the mixture of Gaussian bidirectional associative memories (MoGBAM) to cope with these problems. In this paper, we propose to use a DNN to construct a global non-linear mapping relationship between the spectral envelopes of two speakers. The proposed DNN is generatively trained by cascading two RBMs, which model the distributions of spectral envelopes of source and target speakers respectively, using a Bernoulli BAM (BBAM). Therefore, the proposed training method takes the advantage of the strong modeling ability of RBMs in modeling the distribution of spectral envelopes and the superiority of BAMs in deriving the conditional distributions for conversion. Careful comparisons and analysis among the proposed method and some conventional methods are presented in this paper. The subjective results show that the proposed method can significantly improve the performance in terms of both similarity and naturalness compared to conventional methods.},
	number = {12},
	urldate = {2018-04-13},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Chen, Ling-Hui and Ling, Zhen-Hua and Liu, Li-Juan and Dai, Li-Rong},
	month = dec,
	year = {2014},
	keywords = {deep neural network, voice conversion, Gaussian mixture model, bidirectional associative memory, restricted Boltzmann machine, spectral envelope conversion},
	pages = {1859--1872},
	file = {Chen_2014_Voice Conversion Using Deep Neural Networks with Layer-wise Generative Training.pdf:files/250/Chen_2014_Voice Conversion Using Deep Neural Networks with Layer-wise Generative Training.pdf:application/pdf}
}

@inproceedings{mohammadi_voice_2014,
	title = {Voice conversion using deep neural networks with speaker-independent pre-training},
	doi = {10.1109/SLT.2014.7078543},
	abstract = {In this study, we trained a deep autoencoder to build compact representations of short-term spectra of multiple speakers. Using this compact representation as mapping features, we then trained an artificial neural network to predict target voice features from source voice features. Finally, we constructed a deep neural network from the trained deep autoencoder and artificial neural network weights, which were then fine-tuned using back-propagation. We compared the proposed method to existing methods using Gaussian mixture models and frame-selection. We evaluated the methods objectively, and also conducted perceptual experiments to measure both the conversion accuracy and speech quality of selected systems. The results showed that, for 70 training sentences, frame-selection performed best, regarding both accuracy and quality. When using only two training sentences, the pre-trained deep neural network performed best, regarding both accuracy and quality.},
	booktitle = {2014 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Mohammadi, S. H. and Kain, A.},
	month = dec,
	year = {2014},
	keywords = {Artificial neural networks, deep neural network, neural nets, Speech, Training, voice conversion, Gaussian processes, Accuracy, artificial neural network training, autoencoder, back-propagation, backpropagation, Conferences, conversion accuracy, deep autoencoder, frame-selection, Gaussian mixture models, mapping features, mixture models, pre-training, signal representation, source voice features, speaker recognition, speaker short-term spectra compact representation, speaker-independent pretraining, Speech processing, speech quality, target voice feature prediction},
	pages = {19--23},
	file = {IEEE Xplore Abstract Record:files/254/7078543.html:text/html;Mohammadi_2014_Voice conversion using deep neural networks with speaker-independent.pdf:files/253/Mohammadi_2014_Voice conversion using deep neural networks with speaker-independent.pdf:application/pdf}
}

@incollection{bongaerts_can_1995,
	title = {Can {Late} {Starters} {Attain} a {Native} {Accent} in a {Foreign} {Language}? {A} {Test} of the {Critical} {Period} {Hypothesis}},
	author = {Bongaerts, Theo and Planken, Brigitte and Schils, Erik},
	year = {1995}
}

@article{wu_text-independent_2010,
	title = {Text-{Independent} {F}0 {Transformation} with {Non}-{Parallel} {Data} for {Voice} {Conversion}},
	abstract = {In voice conversion, frame-level mean and variance normalization is typically used for fundamental frequency (F0) transformation, which is text-independent and requires no parallel training data. Some advanced methods transform pitch contours instead, but require either parallel training data or syllabic annotations. We propose a method which retains the simplicity and text-independence of the frame-level conversion while yielding high-quality conversion. We achieve these goals by (1) introducing a text-independent tri-frame alignment method, (2) including delta features of F0 into Gaussian mixture model (GMM) conversion and (3) reducing the well-known GMM oversmoothing effect by F0 histogram equalization. Our objective and subjective experiments on the CMU Arctic corpus indicate improvements over both the mean/variance normalization and the baseline GMM conversion.},
	language = {en},
	author = {Wu, Zhi-Zheng and Kinnunen, Tomi and Chng, Eng Siong and Li, Haizhou},
	year = {2010},
	pages = {4},
	file = {Wu et al. - Text-Independent F0 Transformation with Non-Parall.pdf:files/272/Wu et al. - Text-Independent F0 Transformation with Non-Parall.pdf:application/pdf}
}

@inproceedings{wu_use_2016,
	title = {On the use of {I}-vectors and average voice model for voice conversion without parallel data},
	doi = {10.1109/APSIPA.2016.7820901},
	abstract = {Recently, deep and/or recurrent neural networks (DNNs/RNNs) have been employed for voice conversion, and have significantly improved the performance of converted speech. However, DNNs/RNNs generally require a large amount of parallel training data (e.g., hundreds of utterances) from source and target speakers. It is expensive to collect such a large amount of data, and impossible in some applications, such as cross-lingual conversion. To solve this problem, we propose to use average voice model and i-vectors for long short-term memory (LSTM) based voice conversion, which does not require parallel data from source and target speakers. The average voice model is trained using other speakers' data, and the i-vectors, a compact vector representing the identities of source and target speakers, are extracted independently. Subjective evaluation has confirmed the effectiveness of the proposed approach.},
	booktitle = {2016 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA})},
	author = {Wu, J. and Wu, Z. and Xie, L.},
	month = dec,
	year = {2016},
	keywords = {Feature extraction, Speech, speech processing, Training, DNNs, recurrent neural nets, recurrent neural networks, voice conversion, Data mining, Speech processing, Adaptation models, average voice model, compact vector, Data models, deep neural networks, i-vector, i-vectors, long short-term memory, long short-term memory based voice conversion, LSTM based voice conversion, nonparallel training, RNNs, speech conversion, vectors},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:files/307/7820901.html:text/html;IEEE Xplore Full Text PDF:files/306/Wu et al. - 2016 - On the use of I-vectors and average voice model fo.pdf:application/pdf}
}

@article{kobayashi_statistical_2014,
	title = {Statistical {Singing} {Voice} {Conversion} with {Direct} {Waveform} {Modification} based on the {Spectrum} {Differential}},
	abstract = {This paper presents a novel statistical singing voice conversion (SVC) technique with direct waveform modiﬁcation based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms. SVC makes it possible to convert singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer. However, speech quality of the converted singing voice is signiﬁcantly degraded compared to that of a natural singing voice due to various factors, such as analysis and modeling errors in the vocoderbased framework. To alleviate this degradation, we propose a statistical conversion process that directly modiﬁes the signal in the waveform domain by estimating the difference in the spectra of the source and target singers’ singing voices. The differential spectral feature is directly estimated using a differential Gaussian mixture model (GMM) that is analytically derived from the traditional GMM used as a conversion model in the conventional SVC. The experimental results demonstrate that the proposed method makes it possible to signiﬁcantly improve speech quality in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional SVC.},
	language = {en},
	author = {Kobayashi, Kazuhiro and Toda, Tomoki and Neubig, Graham and Sakti, Sakriani and Nakamura, Satoshi},
	year = {2014},
	pages = {5},
	file = {Kobayashi et al. - Statistical Singing Voice Conversion with Direct W.pdf:files/308/Kobayashi et al. - Statistical Singing Voice Conversion with Direct W.pdf:application/pdf}
}

@article{toda_voice_2007,
	title = {Voice {Conversion} {Based} on {Maximum}-{Likelihood} {Estimation} of {Spectral} {Parameter} {Trajectory}},
	volume = {15},
	issn = {1558-7916},
	doi = {10.1109/TASL.2007.907344},
	abstract = {In this paper, we describe a novel spectral conversion method for voice conversion (VC). A Gaussian mixture model (GMM) of the joint probability density of source and target features is employed for performing spectral conversion between speakers. The conventional method converts spectral parameters frame by frame based on the minimum mean square error. Although it is reasonably effective, the deterioration of speech quality is caused by some problems: 1) appropriate spectral movements are not always caused by the frame-based conversion process, and 2) the converted spectra are excessively smoothed by statistical modeling. In order to address those problems, we propose a conversion method based on the maximum-likelihood estimation of a spectral parameter trajectory. Not only static but also dynamic feature statistics are used for realizing the appropriate converted spectrum sequence. Moreover, the oversmoothing effect is alleviated by considering a global variance feature of the converted spectra. Experimental results indicate that the performance of VC can be dramatically improved by the proposed method in view of both speech quality and conversion accuracy for speaker individuality.},
	number = {8},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Toda, T. and Black, A. W. and Tokuda, K.},
	month = nov,
	year = {2007},
	keywords = {speech processing, voice conversion, Gaussian mixture model, Gaussian processes, Loudspeakers, spectral analysis, Training data, Speech processing, speech quality, Dynamic feature, Gaussian mixture model (GMM), global variance, joint probability density, least mean squares methods, maximum likelihood estimation, Maximum likelihood estimation, maximum-likelihood estimation, maximum-likelihood estimation (MLE), Mean square error methods, minimum mean square error, Natural languages, Parameter estimation, probability, spectral conversion method, spectral parameter trajectory, Speech enhancement, speech synthesis, statistical modeling, Statistics, Virtual colonoscopy, voice conversion (VC)},
	pages = {2222--2235},
	file = {IEEE Xplore Abstract Record:files/312/4317579.html:text/html;IEEE Xplore Full Text PDF:files/311/Toda et al. - 2007 - Voice Conversion Based on Maximum-Likelihood Estim.pdf:application/pdf}
}

@inproceedings{kinnunen_non-parallel_2017,
	title = {Non-parallel voice conversion using i-vector {PLDA}: towards unifying speaker verification and transformation},
	isbn = {978-1-5090-4117-6},
	shorttitle = {Non-parallel voice conversion using i-vector {PLDA}},
	url = {http://ieeexplore.ieee.org/document/7953215/},
	doi = {10.1109/ICASSP.2017.7953215},
	abstract = {Text-independent speaker veriﬁcation (recognizing speakers regardless of content) and non-parallel voice conversion (transforming voice identities without requiring content-matched training utterances) are related problems. We adopt i-vector method to voice conversion. An i-vector is a ﬁxed-dimensional representation of a speech utterance that enables treating voice conversion in utterance domain, as opposed to frame domain. The high dimensionality (800) and small number of training utterances (24) necessitates using prior information of speakers. We adopt probabilistic linear discriminant analysis (PLDA) for voice conversion. The proposed approach requires neither parallel utterances, transcriptions nor time alignment procedures at any stage.},
	language = {en},
	urldate = {2018-05-16},
	publisher = {IEEE},
	author = {Kinnunen, Tomi and Juvela, Lauri and Alku, Paavo and Yamagishi, Junichi},
	month = mar,
	year = {2017},
	pages = {5535--5539},
	file = {Kinnunen et al. - 2017 - Non-parallel voice conversion using i-vector PLDA.pdf:files/336/Kinnunen et al. - 2017 - Non-parallel voice conversion using i-vector PLDA.pdf:application/pdf}
}

@article{aryal_articulatory-based_2015,
	title = {Articulatory-{Based} {Conversion} of {Foreign} {Accents} with {Deep} {Neural} {Networks}},
	abstract = {We present an articulatory-based method for real-time accent conversion using deep neural networks (DNN). The approach consists of two steps. First, we train a DNN articulatory synthesizer for the non-native speaker that estimates acoustics from contextualized articulatory gestures. Then we drive the DNN with articulatory gestures from a reference native speaker –mapped to the nonnative articulatory space via a Procrustes transform. We evaluate the accent-conversion performance of the DNN through a series of listening tests of intelligibility, voice identity and nonnative accentedness. Compared to a baseline method based on Gaussian mixture models, the DNN accent conversions were found to be 31\% more intelligible, and were perceived more native-like in 68\% of the cases. The DNN also succeeded in preserving the voice identity of the nonnative speaker.},
	language = {en},
	author = {Aryal, Sandesh and Gutierrez-Osuna, Ricardo},
	year = {2015},
	pages = {5},
	file = {Aryal and Gutierrez-Osuna - Articulatory-Based Conversion of Foreign Accents w.pdf:files/338/Aryal and Gutierrez-Osuna - Articulatory-Based Conversion of Foreign Accents w.pdf:application/pdf}
}

@article{fang_high-quality_2018,
	title = {High-quality nonparallel voice conversion based on cycle-consistent adversarial network},
	url = {http://arxiv.org/abs/1804.00425},
	abstract = {Although voice conversion (VC) algorithms have achieved remarkable success along with the development of machine learning, superior performance is still difficult to achieve when using nonparallel data. In this paper, we propose using a cycle-consistent adversarial network (CycleGAN) for nonparallel data-based VC training. A CycleGAN is a generative adversarial network (GAN) originally developed for unpaired image-to-image translation. A subjective evaluation of inter-gender conversion demonstrated that the proposed method significantly outperformed a method based on the Merlin open source neural network speech synthesis system (a parallel VC system adapted for our setup) and a GAN-based parallel VC system. This is the first research to show that the performance of a nonparallel VC method can exceed that of state-of-the-art parallel VC methods.},
	urldate = {2018-05-24},
	journal = {arXiv:1804.00425 [cs, eess, stat]},
	author = {Fang, Fuming and Yamagishi, Junichi and Echizen, Isao and Lorenzo-Trueba, Jaime},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.00425},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:files/345/1804.html:text/html;Fang_2018_High-quality nonparallel voice conversion based on cycle-consistent adversarial.pdf:files/344/Fang_2018_High-quality nonparallel voice conversion based on cycle-consistent adversarial.pdf:application/pdf}
}

@article{demarco_native_nodate,
	title = {Native {Accent} {Classification} via {I}-{Vectors} and {Speaker} {Compensation} {Fusion}},
	abstract = {We present a comprehensive analysis of the use of I-vector based classiﬁers for the classiﬁcation of unlabelled acoustic data as native British accents. We demonstrate the different behaviours of various popular dimensionality reduction techniques that have been previously used in problems such as speaker and language classiﬁcation. Our results show that a fusion of I-vector based systems gives state-of-the-art performance for unlabelled classiﬁcation of British accent speech data, reaching ∼81\% accuracy.},
	language = {en},
	author = {DeMarco, Andrea and Cox, Stephen J},
	pages = {5},
	file = {DeMarco and Cox - Native Accent Classification via I-Vectors and Spe.pdf:files/347/DeMarco and Cox - Native Accent Classification via I-Vectors and Spe.pdf:application/pdf}
}

@article{darcy_accents_2004,
	title = {The {Accents} of the {British} {Isles} ({ABI}), corpus},
	language = {en},
	author = {D’Arcy, Shona M and Russell, Martin J and Browning, Sue R and Tomlinson, Mike J},
	year = {2004},
	pages = {6},
	file = {D’Arcy et al. - 2004 - The Accents of the British Isles (ABI), corpus.pdf:files/351/D’Arcy et al. - 2004 - The Accents of the British Isles (ABI), corpus.pdf:application/pdf}
}