{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through the voice conversion of speakers from the ARCTIC and ARCTIC2 corpus using the SIDEKIT toolkit for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SIDEKIT=theano=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kennylino/.local/share/virtualenvs/thesis-W2ZHkp3l/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "WARNING:root:WARNNG: libsvm is not installed, please refer to the documentation if you intend to use SVM classifiers\n"
     ]
    }
   ],
   "source": [
    "%set_env SIDEKIT=theano=false\n",
    "import sidekit\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, expanduser\n",
    "from shutil import copy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kennylino/Documents/em_lct/UoM/thesis/experiments\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kennylino/Documents/em_lct/UoM/thesis\n"
     ]
    }
   ],
   "source": [
    "# if cwd not './thesis' (e.g. './thesis/experiments/'), moves one up\n",
    "os.chdir('..')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename files and create train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are renamed and reorganized to better follow the structure desired by the SIDEKIT toolkit.\n",
    "Some easier file movement may have been conducted using drag in drop in Finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = \"./data/arctic2/\" # corpus directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: run only once or filenames will keep changing\n",
    "for dirName, subdirList, fileList in os.walk(corpus_dir):\n",
    "    if len(fileList) > 1:\n",
    "        for fname in fileList:\n",
    "            split_dir_name = dirName.split('/') # splits the directory path into each folder; e.g. ['.', 'data', 'arctic2', 'YDCK', 'wav']\n",
    "            os.rename(os.path.join(dirName, fname), os.path.join(dirName, split_dir_name[-2] + '_' + fname)) # renames files to name of speaker + audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine number of audio files per folder because some folders missing audio\n",
    "for dirName, subdirList, fileList in os.walk(corpus_dir):\n",
    "    if len(fileList) > 1:\n",
    "        print (dirName)\n",
    "        print('size= ' +  str(len(fileList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBVS_dir = \"./data/arctic2/EBVS/wav\" # corpus directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select EBVS folder to decide train/test because it has the least audio files\n",
    "EBVS_files = os.listdir(EBVS_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects 150 audios at random using the seed and splits them into 100 train, 50 test\n",
    "data_split = train_test_split(EBVS_files, train_size=100, test_size=50, random_state=42)\n",
    "train_data = data_split[0]\n",
    "test_data = data_split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a train and test folder for each speaker\n",
    "for dirName, subdirList, fileList in os.walk(corpus_dir):\n",
    "    for subdir in subdirList:\n",
    "        subdir_path = os.path.join(dirName, subdir)\n",
    "        os.mkdir((os.path.join(subdir_path, 'train')))\n",
    "        os.mkdir((os.path.join(subdir_path, 'test')))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moves files from 'wav' folder to 'train' folder for each speaker\n",
    "for file in train_data:\n",
    "    split_fname = file.split('_') #splits filename into something like ['EBVS', 'arctic', 'b0072.wav']\n",
    "    for dirName, subdirList, fileList in os.walk(corpus_dir):\n",
    "        if len(fileList) > 101: # cheap hack to avoid error of copying within same folder; 101 for size of train data\n",
    "            for f in fileList:\n",
    "                if f.split('_')[2] == split_fname[2]:\n",
    "                    spk_folder = os.path.split(dirName)[0]\n",
    "                    copy(os.path.join(dirName, f), os.path.join(spk_folder, 'train'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # moves files from 'wav' folder to 'test' folder for each speaker\n",
    "for file in test_data:\n",
    "    split_fname = file.split('_') #splits filename into something like ['EBVS', 'arctic', 'b0072.wav']\n",
    "    for dirName, subdirList, fileList in os.walk(corpus_dir):\n",
    "        if len(fileList) > 101: # cheap hack to avoid error of copying within same folder; 101 for size of training ta\n",
    "            for f in fileList:\n",
    "                if f.split('_')[2] == split_fname[2]:\n",
    "                    spk_folder = os.path.split(dirName)[0]\n",
    "                    copy(os.path.join(dirName, f), os.path.join(spk_folder, 'test'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dir = \"./data/arctic2/train/\" # training data directory\n",
    "training_data_list = os.listdir(training_data_dir)\n",
    "training_data_list = ['train/' + f.split('.')[0] for f in training_data_list] # remove extension; add folder name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www-lium.univ-lemans.fr/sidekit/tutorial/hdf5.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines how to extract features (MFCCs, bottleneck, etc.) of audio files in a given path\n",
    "extractor = sidekit.FeaturesExtractor(audio_filename_structure='./data/arctic2/{}.wav',\n",
    "                                      feature_filename_structure='./data/arctic2/feat/{}.h5',\n",
    "                                      sampling_frequency=16000,\n",
    "                                      lower_frequency=200,\n",
    "                                      higher_frequency=3800,\n",
    "                                      filter_bank=\"log\",\n",
    "                                      filter_bank_size=24,\n",
    "                                      window_size=0.025,\n",
    "                                      shift=0.01,\n",
    "                                      ceps_number=24,\n",
    "                                      vad=\"snr\",\n",
    "                                      snr=40,\n",
    "                                      pre_emphasis=0.97,\n",
    "                                      save_param=[\"vad\", \"energy\", \"cep\"],\n",
    "                                      keep_all_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like number of threads can change whether ALL files get processed or not;\n",
    "# keep at 12 threads!\n",
    "%%time\n",
    "extractor.save_list(show_list=training_data_list,\n",
    "                    channel_list=[0]*len(training_data_list),\n",
    "                    num_thread=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UBM-GMM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading features without processing features and saving first does not seem to work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dir = \"./data/arctic2/feat/train\" # training data directory\n",
    "feat_list = os.listdir(feat_dir)\n",
    "feat_list = ['train/' + f.split('.')[0] for f in feat_list] # remove extension; add folder name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www-lium.univ-lemans.fr/sidekit/tutorial/ubmTraining.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/BDL_arctic_b0247\n"
     ]
    }
   ],
   "source": [
    "# be careful here; SIDEKIT expects the internal folder of the h5 \n",
    "# and the given filename_structure (the thing inside {}) to match\n",
    "print(feat_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a FeaturesServer that describes *how* to load the data\n",
    "fs = sidekit.FeaturesServer(feature_filename_structure='./data/arctic2/feat/{}.h5',\n",
    "                             sources=None,\n",
    "                             dataset_list=[\"vad\", \"energy\", \"cep\"],\n",
    "                             mask=None,\n",
    "                             feat_norm=\"cmvn\",\n",
    "                             global_cmvn=None,\n",
    "                             dct_pca=False,\n",
    "                             dct_pca_config=None,\n",
    "                             sdc=False,\n",
    "                             sdc_config=None,\n",
    "                             delta=True,\n",
    "                             double_delta=True,\n",
    "                             delta_filter=None,\n",
    "                             context=None,\n",
    "                             traps_dct_nb=None,\n",
    "                             rasta=False,\n",
    "                             keep_all_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubm = sidekit.Mixture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# started at 1:20am\n",
    "%%time\n",
    "model = ubm.EM_split(features_server=fs,\n",
    "                      feature_list=feat_list,\n",
    "                      distrib_nb=512,\n",
    "                      num_thread=12,\n",
    "                      save_partial=False\n",
    "                      )\n",
    "ubm.write('ubm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubm.write(corpus_dir + 'ubm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
