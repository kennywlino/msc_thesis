{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice conversion prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with GMMs; Arctic corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcmu_us_awb_arctic\u001b[m\u001b[m \u001b[34mcmu_us_clb_arctic\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# open data\n",
    "from os.path import join, expanduser\n",
    "DATA_ROOT = join(expanduser(\"~\"), \"Documents/em_lct/UoM/thesis/data/\", \"arctic\")\n",
    "!ls $DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kennylino/.local/share/virtualenvs/thesis-W2ZHkp3l/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# nnmnkwii imports taken from example notebook on VC\n",
    "%pylab inline\n",
    "rcParams[\"figure.figsize\"] = (16,5)\n",
    "\n",
    "from nnmnkwii.datasets import PaddedFileSourceDataset\n",
    "from nnmnkwii.datasets.cmu_arctic import CMUArcticWavFileDataSource\n",
    "from nnmnkwii.preprocessing.alignment import DTWAligner\n",
    "from nnmnkwii.preprocessing import trim_zeros_frames, remove_zeros_frames, delta_features\n",
    "from nnmnkwii.util import apply_each2d_trim\n",
    "from nnmnkwii.metrics import melcd\n",
    "from nnmnkwii.baseline.gmm import MLPG\n",
    "\n",
    "from os import listdir\n",
    "from os.path import basename, splitext, join, splitext, isdir\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import pyworld\n",
    "import pysptk\n",
    "from pysptk.synthesis import MLSADF, Synthesizer\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16000\n",
    "fftlen = pyworld.get_cheaptrick_fft_size(fs)\n",
    "alpha = pysptk.util.mcepalpha(fs)\n",
    "order = 24\n",
    "frame_period = 5\n",
    "hop_length = int(fs * (frame_period * 0.001))\n",
    "max_files = 150 # number of utterances to be used.\n",
    "test_size = 0.33\n",
    "use_delta = True\n",
    "\n",
    "if use_delta:\n",
    "    windows = [\n",
    "        (0, 0, np.array([1.0])),\n",
    "        (1, 1, np.array([-0.5, 0.0, 0.5])),\n",
    "        (1, 1, np.array([1.0, -2.0, 1.0])),\n",
    "    ]\n",
    "else:\n",
    "    windows = [\n",
    "        (0, 0, np.array([1.0])),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFileDataSource(CMUArcticWavFileDataSource):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyFileDataSource, self).__init__(*args, **kwargs)\n",
    "        self.test_paths = None\n",
    "\n",
    "    def collect_files(self):\n",
    "        paths = super(\n",
    "            MyFileDataSource, self).collect_files()\n",
    "        paths_train, paths_test = train_test_split(\n",
    "            paths, test_size=test_size, random_state=42)\n",
    "\n",
    "        # keep paths for later testing\n",
    "        self.test_paths = paths_test\n",
    "\n",
    "        return paths_train\n",
    "\n",
    "    def collect_features(self, path):\n",
    "        x, frame_sample = librosa.load(path, sr=fs, dtype=float64)\n",
    "        f0, timeaxis = pyworld.dio(x, frame_sample, frame_period=frame_period)\n",
    "        f0 = pyworld.stonemask(x, f0, timeaxis, frame_sample)\n",
    "        spectrogram = pyworld.cheaptrick(x, f0, timeaxis, frame_sample)\n",
    "        spectrogram = trim_zeros_frames(spectrogram)\n",
    "        mc = pysptk.sp2mc(spectrogram, order=order, alpha=alpha)\n",
    "        return mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown speaker 'tni'. It should be one of ['awb', 'bdl', 'clb', 'jmk', 'ksp', 'rms', 'slt']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-471435c4fd52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc_speaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyFileDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeakers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is actually speaker TNI from Arctic L2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtgt_speaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyFileDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeakers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tni\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d61bf9b4bac5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMyFileDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCMUArcticWavFileDataSource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyFileDataSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/thesis-W2ZHkp3l/lib/python3.6/site-packages/nnmnkwii/datasets/cmu_arctic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_root, speakers, labelmap, max_files)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 raise ValueError(\n\u001b[1;32m     44\u001b[0m                     \"Unknown speaker '{}'. It should be one of {}\".format(\n\u001b[0;32m---> 45\u001b[0;31m                         speaker, available_speakers))\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown speaker 'tni'. It should be one of ['awb', 'bdl', 'clb', 'jmk', 'ksp', 'rms', 'slt']"
     ]
    }
   ],
   "source": [
    "src_speaker = MyFileDataSource(data_root=DATA_ROOT, speakers=[\"clb\"], max_files=max_files) # This is actually speaker TNI from Arctic L2\n",
    "tgt_speaker = MyFileDataSource(data_root=DATA_ROOT, speakers=[\"awb\"], max_files=max_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training sets into arrays\n",
    "X = PaddedFileSourceDataset(src_speaker, 1200).asarray()\n",
    "Y = PaddedFileSourceDataset(tgt_speaker, 1200).asarray()\n",
    "print(X.shape)\n",
    "print(Y.shape) \n",
    "# shape = (13 training audios, 2800 frames, 25 features per frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_speaker.test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_speaker.test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting util\n",
    "def plot_parallel(x,y):\n",
    "    figure(figsize=(16,7))\n",
    "    subplot(2,1,1)\n",
    "    librosa.display.specshow(trim_zeros_frames(x).T, sr=fs, hop_length=hop_length, x_axis=\"time\")\n",
    "    colorbar()\n",
    "    subplot(2,1,2)\n",
    "    librosa.display.specshow(trim_zeros_frames(y).T, sr=fs, hop_length=hop_length, x_axis=\"time\")\n",
    "    colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6 # any \n",
    "plot_parallel(X[idx],Y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment\n",
    "X_aligned, Y_aligned = DTWAligner(verbose=0, dist=melcd).transform((X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel(X_aligned[idx],Y_aligned[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 1st (power) dimension\n",
    "X_aligned, Y_aligned = X_aligned[:, :, 1:], Y_aligned[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_dim = X_aligned.shape[-1]\n",
    "if use_delta:\n",
    "    X_aligned = apply_each2d_trim(delta_features, X_aligned, windows)\n",
    "    Y_aligned = apply_each2d_trim(delta_features, Y_aligned, windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel(X_aligned[idx],Y_aligned[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans clustering of MFCC feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering of source speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_X = KMeans(n_clusters=512, init='k-means++', max_iter=100, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aligned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates all of the frames in each sentence since k-means needs one large vector\n",
    "all_frames_X = numpy.concatenate(X_aligned, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should equal number of frames * number of utterances\n",
    "len(all_frames_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 frames are removed since they are all exactly the same\n",
    "all_frames_X = remove_zeros_frames(all_frames_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_frames_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "km_X.fit(all_frames_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help taken from: https://stackoverflow.com/questions/36195457/python-sklearn-kmeans-how-to-get-the-values-in-the-cluster\n",
    "clustered_X = pd.DataFrame()\n",
    "clustered_X['frame'] = all_frames_X.tolist()\n",
    "clustered_X['cluster #'] = km_X.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dist_X = clustered_X.groupby('cluster #').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use print(cluster_dist_X.to_string()) to see whole dist\n",
    "print (cluster_dist_X.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dist_X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering of the target speaker data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_Y = KMeans(n_clusters=512, init='k-means++', max_iter=100, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates all of the frames in each sentence since k-means needs one large vector\n",
    "all_frames_Y = numpy.concatenate(Y_aligned, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should equal number of frames * number of utterances\n",
    "len(all_frames_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = all_frames_Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 frames are removed since they are all exactly the same\n",
    "all_frames_Y = remove_zeros_frames(all_frames_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "km_Y.fit(all_frames_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help taken from: https://stackoverflow.com/questions/36195457/python-sklearn-kmeans-how-to-get-the-values-in-the-cluster\n",
    "clustered_Y = pd.DataFrame()\n",
    "clustered_Y['frame'] = all_frames_Y.tolist()\n",
    "clustered_Y['cluster #'] = km_Y.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dist_Y = clustered_Y.groupby('cluster #').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use print(cluster_dist_Y.to_string()) to see whole dist\n",
    "print (cluster_dist_Y.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dist_Y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the look-up table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# shape = (13 training audios, 2800 frames, 25 features per frame)\n",
    "\n",
    "# all_frames_x contains each frame from each audio inside of X (aka X_aligned)\n",
    "\n",
    "# the psuedocode for finding closest frames \n",
    "# for frame in all_frames_X:\n",
    "#    predict the cluster of the frame in clustered_Y\n",
    "#    find the closest data point in clustered_Y to frame\n",
    "#    store X_frame and most similar frame, Y_frame in paired_frames_X\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "# bytes version of X: array from Y\n",
    "paired_frames_X = {}\n",
    "\n",
    "for frame in all_frames_X:\n",
    "    # needs to be reshaped to shape of km training data \n",
    "    frame_pred = frame.reshape(1, -1)\n",
    "    predicted_cluster = km_Y.predict(frame_pred)[0] # returns cluster num\n",
    "    # grabs the possible frames and converts them to the proper array format for KDTree\n",
    "    possible_frames = (clustered_Y[clustered_Y['cluster #'] == predicted_cluster]['frame'].values).tolist()\n",
    "    tree = spatial.KDTree(possible_frames)\n",
    "    # tree.query() returns (the distance between the closest frame and index in series)\n",
    "    most_similar = tree.query(frame)\n",
    "    most_similar_vec = np.array(possible_frames[most_similar[1]]) # index in possible_frames\n",
    "    frame_key = frame.tobytes() # converted to bytes since arrays cannot be keys\n",
    "    paired_frames_X[frame_key] = most_similar_vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the 'most similar to X' array\n",
    "Y = np.array([paired_frames_X[frame.tobytes()] for frame in all_frames_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = np.concatenate((all_frames_X, Y), axis=-1)\n",
    "print(XY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains the GMM between the L1 and L2 speaker\n",
    "gmm = GaussianMixture(\n",
    "    n_components=64, covariance_type=\"full\", max_iter=100, verbose=1)\n",
    "\n",
    "%time gmm.fit(XY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    plot(gmm.means_[k], linewidth=1.5, label=\"Mean of mixture {}\".format(k+1))\n",
    "legend(prop={\"size\": 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(gmm.covariances_[0], origin=\"bottom left\")\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    plot(np.diag(gmm.covariances_[k]), linewidth=1.5, \n",
    "         label=\"Diagonal part of covariance matrix, mixture {}\".format(k))\n",
    "legend(prop={\"size\": 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_utt(src_path, tgt_path, disable_mlpg=False, diffvc=True):\n",
    "    # GMM-based parameter generation is provided by the library in `baseline` module\n",
    "    if disable_mlpg:\n",
    "        # Force disable MLPG\n",
    "        paramgen = MLPG(gmm, windows=[(0,0, np.array([1.0]))], diff=diffvc)\n",
    "    else:\n",
    "        paramgen = MLPG(gmm, windows=windows, diff=diffvc)\n",
    "\n",
    "    x, frame_sample = librosa.load(src_path, sr=fs, dtype=float64)\n",
    "    f0, timeaxis = pyworld.dio(x, frame_sample, frame_period=frame_period)\n",
    "    f0 = pyworld.stonemask(x, f0, timeaxis, frame_sample)\n",
    "    spectrogram = pyworld.cheaptrick(x, f0, timeaxis, frame_sample)\n",
    "    aperiodicity = pyworld.d4c(x, f0, timeaxis, frame_sample)\n",
    "\n",
    "    mc = pysptk.sp2mc(spectrogram, order=order, alpha=alpha)\n",
    "    c0, mc = mc[:, 0], mc[:, 1:]\n",
    "    if use_delta:\n",
    "        mc = delta_features(mc, windows)\n",
    "    mc = paramgen.transform(mc)\n",
    "    if disable_mlpg and mc.shape[-1] != static_dim:\n",
    "        mc = mc[:,:static_dim]\n",
    "    assert mc.shape[-1] == static_dim\n",
    "    mc = np.hstack((c0[:, None], mc))\n",
    "    if diffvc:\n",
    "        mc[:, 0] = 0 # remove power coefficients\n",
    "        engine = Synthesizer(MLSADF(order=order, alpha=alpha), hopsize=hop_length)\n",
    "        b = pysptk.mc2b(mc.astype(np.float64), alpha=alpha)\n",
    "        waveform = engine.synthesis(x, b)\n",
    "    else:\n",
    "        spectrogram = pysptk.mc2sp(\n",
    "            mc.astype(np.float64), alpha=alpha, fftlen=fftlen)\n",
    "        waveform = pyworld.synthesize(\n",
    "            f0, spectrogram, aperiodicity, frame_sample, frame_period)\n",
    "        \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results with [this](http://people.tamu.edu/~guanlong.zhao/icassp18_demo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (src_path, tgt_path) in enumerate(zip(src_speaker.test_paths, tgt_speaker.test_paths)):\n",
    "    print(\"{}-th sample\".format(i+1))\n",
    "    wo_MLPG = test_one_utt(src_path, tgt_path, disable_mlpg=True)\n",
    "    w_MLPG = test_one_utt(src_path, tgt_path, disable_mlpg=False)\n",
    "    src, _ = librosa.load(src_path, sr=fs, dtype=float64)\n",
    "    tgt, _ = librosa.load(tgt_path, sr=fs, dtype=float64)\n",
    "    \n",
    "    print(\"Source:\", basename(src_path))\n",
    "    IPython.display.display(Audio(src, rate=fs))\n",
    "    print(\"Target:\", basename(tgt_path))\n",
    "    IPython.display.display(Audio(tgt, rate=fs))\n",
    "    print(\"w/o MLPG\")\n",
    "    IPython.display.display(Audio(wo_MLPG, rate=fs))\n",
    "    print(\"w/ MLPG\")\n",
    "    IPython.display.display(Audio(w_MLPG, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
