\chapter{Literature Review}
This section provides a brief overview of second language acquisition and education in order to frame the challenge of pronunciation and to motivate the potential usage of technology in language learning. I then examine some previous research in spoken language technology used in the domain of language education, including discussion on computer assisted pronunciation (CAPT) systems in order to shed light on where voice conversion and accent conversion could be applied, and then detail some important pivotal work done in the two areas.


# Theoretical and educational motivations
Linguists have long debated over the possibility of whether second language (L2) learners (e.g. adult learners) could ever acquire a language to the extent of a native speaker. Some still cite ideas like the Critical Period (CP) Hypothesis and neuroplasticity which claims that learners cannot acquire language (at least as well as a native speaker) after a certain point in time due to the loss of plasticity in the brain \parencite{lenneberg1967a,scovel1988a}. This theory has been particularly cited in reference to pronunciation, perhaps due to the obvious difficultly in overcoming the L1 negative transfer (e.g. effect of our native language) that many, if not all, language learners experience in speaking a new language. 

Since the emergence of the CP hypothesis, many linguists have investigated the relationship between a number of variables such as age, motivation, and language use, that interact with the level of language acquisition. \textcite{piske2001} and \textcite{lengeris2012} present an excellent review of different literature that investigates the interactions between these various variables and their effects on foreign accent. They discuss that although many L2 foreign accent studies do support the idea that the earlier a language learner learns a language, the better their accent would be, there isn't strong enough indication to support the notion of a 'critical' period. They do concede that many studies do indicate that there is a linear correlation between age and foreign accent, but this only indicates a 'sensitive' period, not a 'critical' period, a distinction that some fail to acknowledge. That is to say, following advocates of the CP, the critical period should end roughly around 12 years old \parencite{scovel1988a}, or no later than 15 years old \parencite{patkowski1990}, and beyond this point, there would be "a sharp drop-off in a learner's abilities" \parencite{lengeris2012}, indicating that a learner could not acquire a native-like accent beyond this period.

Other researchers such as \textcite{long1990} suggested that an L2 learner could speak accent-free if they learned the language before 6 years old but not after 12 years old. Although this notion also has been supported through a number of studies, there has also been counter-evidence found in other studies that found that there were learners younger than 6 who had detectable traces of a foreign accent. In other studies that examined learners of English who started beyond 12 years old, they also found evidence of learners with no detectable foreign accent. For example, in \textcite{flege1995}, it was found that 6% of 120 native speakers of Italian who started learning English after the age of 12 years old had native-like pronunciation, and in \textcite{bongaerts1995}, it was found that 5 out of 11 speakers were rated comparable to the native English control subjects. Thus \textcite{piske2001} conclude that while there is evidence that earlier learners can learn an L2 with less chance or degree of a foreign accent, this does not necessarily support the CP hypothesis or the idea that the loss of plasticity in the brain leads to an inability to acquire language.

Following their review of the correlation between other potential variables and the degree of foreign accent, \textcite{piske2001} take a further look at \textcite{bongaerts1995}. Although it was found that 5 out of 11 speakers were perceived as \textit{indistinguishable} from native English speakers, \textcite{piske2001} also point to a number of other factors in play that could have potentially allowed these speakers to be perceived as such. Perhaps the most controversial factor is the fact that these speakers were native Dutch speakers. Because Dutch and English have a similar phonetic inventory, it has been argued that this allows for easier acquisition of a native-like accent. In examination of their motivation, these learners were considered highly motivated as they felt the need to speak English without a Dutch accent due to their positions as university-level English teachers. On top of this, these learners also received intensive training in the perception and production of English sounds. Examining the interaction between these variables, it may be appropriate to conclude that the reason why 5 out of the 11 speakers were evaluated as indistinguishable from native speakers is due to the fact that they are a very niche population.   

Regardless, this points to the fact that there is the possibility of language learners acquiring native-like pronunciation given the right configuration, even if the said configuration may be difficult to achieve. This is contrary to what is suggested by the CP hypothesis, which suggests that _all_ learners are limited by a critical period. This is not to say that learners are not still deterred by a 'sensitive period', but this does highlight the potential that learners could be taught pronunciation should they have the right tools.

Aside from the issue of whether or not language learners could ever achieve native-like performance, another question that arises is whether or not there is even a \textit{need} for learners to aim so high. In \textcite{munro1999}, they discuss the interaction between foreign accent, comprehensibility and intelligibility and point out that the goal for many L2 learners is to communicate and not necessarily sound like a native speaker. Thus while there are unique groups of learners like those from \textcite{bongaerts1995}, \textcite{munro1999} point out that most learners strive for effective communication. In order to observe the interaction between foreign accent, comprehensibility and intelligibility, a they conduct a perceptual study on the performance of native Mandarin speakers. Following this study,  they found that despite the fact that some speakers may have what some consider a 'heavy accent', this does not automatically mean that they are unintelligible. However, they do cite that some accents may cause longer processing times than others. When observing the interaction of variables such as phonemic errors and intonation with intelligibility and comprehensibility, they found that intonation was the most influential factor in comprehensibility, while phonemic errors affected intelligibility. This substantiates the concepts of comprehensibility and intelligibility themselves, as intelligibility is the degree a speaker is understood without involving interpretation (e.g. "What did they say?"), while comprehensibility is the degree a speaker is understood in terms of meaning (e.g. "What do they mean?"). Thus, they suggest that successful communication requires attention to both sounds and prosody for better comprehensibility and intelligibility.   

While linguists make these discoveries and observations of L2 learning, it seems that it takes a lot of effort for them to trickle down to the foreign language classroom. In \textcite{darcy2012}, they find through a small survey of 14 teachers that although teachers tend to find pronunciation to be 'very important', the majority do not teach it at all. When asked why they do not teach it, they cited reasons such as `time, a lack of training and the need for more guidance and institutional support'. Even though the number of teachers surveyed may be significantly small, this gives us a glimpse through the lens of what language teachers themselves experience in relation to pronunciation. We see that even though teachers would like to address it, this would require a restructuring in their curriculum and training-- something that would undoubtedly take even more time before students get more pronunciation attention. Compounded with the issue of time and the fact that not all learners need or want equal amount of pronunciation training, it may be unlikely to see such change in second language curriculum so soon.

This points to the potential solution of employing a technology-based system to improve pronunciation as learners could individually address their needs \textit{outside} of the classroom.

# Spoken language technology for education
Over the decades, as speech technology has slowly evolved and started to show its potential, many researchers have tried to test its limits by innovating a number of systems to address the challenge of pronunciation. Included in these systems are systems such as computer-assisted pronunciation training (CAPT) systems which attempt to tutor pronunciation through explicit teaching as well as more modern gamified techniques, which attempt to coerce language learners in to practicing pronunciation by making the process more engaging. 

Among the two, CAPT systems have had more history due to the extra development and testing gamified techniques require. In fact, gamified systems can be considered a subclass of CAPT systems, as both require a fundamental setup in order to assist the language learner. In general, these systems utilize some form of automatic speech recognition (ASR) to record a speaker and compares their recordings (usually) with a native speaker gold standard. They also usually include a feedback mechanism with a combination of pitch contours, spectrograms or audio recordings to help the user adjust their pronunciation, with gamified systems including at least a point mechanism to motivate the user.

In order to understand the connection between language education and spoken language technology, we take a look at \textcite{neri2002} where we a presented with a through overview between the two areas. Here, we see that aside from the classroom, there seems to be an issue in relating the findings of linguistics/language pedagogy with technology. Part of the reason, they suggest, stems from the fact that there are not 'clear guidelines' on how to adapt second language acquisition research and thus many CAPT systems 'fail to meet sound pedagogical requirements'. They emphasize the need for the learners to have appropriate input, output, and feedback and exhibit how the systems available at the time were lacking.
For example, they criticize some CAPT systems that were prevalent at the time including systems like \textit{Pro-nunciation} and the \textit{Tell Me More} series for utilizing feedback systems that give the users feedback in waveforms and spectrograms, which cannot be easily interpreted without training. Further, they argue that although visual feedback has its merits, this kind of feedback suggests to the user that their utterance must look close to what is shown on the screen, which is not the case. An utterance can be pronounced perfectly fine, but look completely different from a spectrogram, and \textit{especially} a waveform due to the number of features represented in each visualization, such as the intensity, which will indefinitely vary from user to user and the given examplar. 
They conclude their article by making it a point to discuss recommendations for CAPT systems, by stating that they should integrate what has been found in research from second language acquisition, and to train pronunciation in a communicative manner to give context to the learners. They also point to the problematic area of feedback and advise that systems provide more easily interpretable feedback with both audio and visual information, and propose that systems give exercises that are `realistic, varied, and engaging'. Although this article was first published in 2002, it outlines a good fundamental structure that CAPT systems require-- something that many systems still seem to be lacking.  

In \textcite{eskenazi2009}, we are given a more technical review of CAPT systems with attention to the different CAPT system types and their limitations as well as some discussion on prosody detection.

The article explains that CAPT systems can be generally split into two main types: individual error detection and pronunciation assessment. As indicated, individual error detection systems are more focused on one particular aspect of the user's speech, such as the phones or pitch, while pronunciation assessment systems are more designed to represent how a human would judge a non-native utterance. 

Early individual error detection systems, including one of the author's very own \textcite{eskenazi1998}, started by using a variety of speech recognition techniques such as forced alignment or unconstrained speech recognition. They also worked with a variety of measures to detect the differences between the individual errors and gold standard. Some of these measures include hidden Markov model (HMM) based recognition scoring, a confidence score based system known as Goodness of Pronunciation (GOP), and Linear Discriminant Analysis (LDA). Each of these measures were found to somehow detect the users' errors; however they suffer from issues like low precision or the need for a very homogeneous sample (e.g. Japanese speakers). 

Although some of these early systems showed some signs of promise, they tended to over-simplify the issue of pronunciation training. \textcite{eskenazi2009} makes a point of this by emphasizing the fact that improving non-native pronunciation is not simply a binary question of native vs. non-native. Instead the L1 of the system's users must be considered as this in itself can greatly affect the evaluation. \textcite{eskenazi2009} also points out that the level of language learning of the speakers can also impact the metrics and success of the system as well, and thus an appropriate population must be selected carefully when building a CAPT system, especially when considering individual errors.

In examining previous CAPT systems, \textcite{eskenazi2009} briefly discusses prosody correction, an often overlooked area as previously mentioned. \textcite{eskenazi2009} points to some pivotal works that have used a variety of methods to address the issue, including systems that use Pitch Synchronous Overlap and Add (PSOLA) to resynthesize the prosody of users to help them hear what an appropriate utterance would sound like. This has been suggested to be a potentially effective feedback mechanism to employ in future systems, as it has been said that imitating one's own voice is the most effective \parencite{felps2009}. Among prosody correction systems, \textcite{eskenazi2009} mentions two main types-- those that use appropriate L2 phonological models and break prosody down into two levels--- syllable-word and utterance-phrase, and systems that detect the 'liveliness' of a speaker. However, these systems require tuning of a variety features including F0, power, duration or phonetic transcription, which makes it difficult to automatically create the necessary adjustments not just cross-linguistically, but across speakers as well. \textcite{eskenazi2009} concludes by stating that although such limitations exist in these systems, the usage of ASR and other speech technologies has grown from such a sparse beginning, and that because the market appeal for such systems is large, they shall soon serve central roles in language education. 

Aside from general discussion on CAPT systems, \textcite{chun2008} present a review of other technologies used in pronunciation training, with emphasis on feedback mechanisms used to train prosody. They discuss four main tools used in teaching prosody: 'visualization of pitch contours', 'multimodal tools', 'spectrographic displays' and `vowel analysis programs'. Citing previous work, it appears that they suggest that the visualization of pitch contours is the most robust method of feedback for learners as it is the most intuitive and non-language specific. Aside from this however, they also discuss the potential of a multimedia approach used by \textcite{hardison2005} that integrate both audio and video in a system called \textit{Anvil}. Following this research, users of this system were able to generalize their training beyond a sentence level and were able to perform better at a discourse-level. This methodology encapsulates a good feedback mechanism as described by \textcite{neri2002} as it provides adequate feedback by being easily interpretable, stimulates both audio and visual channels and puts the language in context.

\textcite{chun2008} also discuss the two main methods of such prosody systems: one which utilizes isolated scripted sentences and the other utilizing imitation. While both types of systems are common, possibly due to their easier implementation, they conclude that neither method is useful for generalizing to novel speech production. Aside from the fact that the language may not be put into context, one problem with both types of systems is their limited number of sentences and limited source speakers in terms of imitation. This prevents learners from gaining an understanding of the variability of acceptable (and hence unacceptable) prosody across speakers and contexts, and may limit learners to speaking as similarly as they can to the given exemplars. Thus, while these systems might provide language learners with a basis to improve their prosody, further work needs to be put in to help them contextualize prosody as a general concept and to give learners more autonomy in developing their own speaking style.

Similar to \textcite{neri2002}, \textcite{chun2008} also gives insight on potential ways to improve future CAPT systems. One particularly compelling suggestion they give is to expand systems to include gestures and movement. While most research on language education and in turn, linguistics, is focused on the spoken aspect, \textcite{chun2008} points out that spoken language is complementary with our body language. For example, when we raise our pitch, we also tend to raise our eyebrows and chin, and when we emphasize a specific, we might also stress it by opening our eyes slightly wider, bobbing our head or pointing our finger. In fact, one of the few studies that examine the effect of including body language in teaching L2 listening comprehension found that learners performed much better with these visual cues as opposed to only having audio cues. Thus they conclude that in order to create better pronunciation training systems, learners must be provided with better feedback and language must be placed into context not only in terms of real-life situations, but also in terms of communication as a whole by including gestures and movement.

With that said, building pronunciation systems that take all of the previous suggestions into consideration requires adept planning and expertise, and can be demanding for most research groups. Instead, some of have tried to adapt already existing technology and build a small architecture around it. For example, \parencite{tejedor-garcia2017} experiment with utilizing synthetic voices for corrective feedback in a pronunciation training tool. In their study, they use Google's offline Android text-to-speech (TTS) system as feedback for B1 and B2 Spanish learners of English, and have them focus on the six most difficult pairs of vowels \colorbox{magenta}{[insert IPA here?]}. In order to train the users, the researchers first had them watch videos that describe the articulatory/perceptive features of the vowels, and had them listen to a number of minimal pairs produced by the TTS system in succession. Afterwards, they were asked to discriminate minimal pairs in a listening task and then asked to pronounce them. From this study, they conclude that making use of commercial TTS systems are beneficial for users and instructors alike as indicated by both the improvement in performance by the users and the feedback given by those involved in the experiment. However, because the study was limited to individual words and only six pairs of vowels, further experimentation needs to be conducted in order to understand whether these learners can generalize their training.

Through examining these various works, it is evident that there is a large potential for appropriately adapting technology to guide and help language learners and teachers alike. Yet, in order to provide long-standing worthwhile results, further consideration needs to be given to the suggestions and evidence of previous research and should be integrated in the design and implementation of future systems. This implies that the appropriate time and resources may need to be dedicated in order to push the boundaries of CAPT systems.

# Voice conversion
There have been a number of efforts to design voice conversion systems using various methodologies. Much like the rest of the speech technology field, earlier voice conversion systems began with utilizing MFCCs and GMMs for conversion and slowly evolved towards utilizing more advanced features and adaptation techniques.  

In particular, a variation of GMM voice conversion set forth by \textcite{toda2007} has become what appears to be the standard set-up. Following their reasoning, they argue that although regular GMMs perform fairly well in voice conversion, they also lead to the deterioration of speech quality. Instead, they propose that by using a maximum-likelihood estimation of the spectral parameter trajectories, issues that cause the loss of quality such as oversmoothing of the spectral features can be avoided. They provide detailed theoretical evidence to support their method which can be further observed by taking a look at their paper.  

GMMs have long been used for voice conversion alongside other speech tasks, but more recently another method-- or more accurately another feature in place of MFCCs, known as *i-vectors* have taken off. To put concisely, i-vectors are akin to word embeddings in text-based natural language processing tasks in the sense that i-vectors encapsulate any type of desired speech information in a vectorized fashion. This may be confusable with MFCCs, which also vectorize speech information; however MFCCs specifically vectorize individual speech sounds from frames, while i-vectors tend to vectorize more large-scale, dynamic speech information.

The usage of i-vectors have proven to be successful in a number of tasks, such as speaker verification, language identification, and native accent identification. They have become especially popular due to the fact that they work well with unlabeled acoustic data. Referring back to the overview of voice conversion in the previous section, it is mentioned that labeled acoustic data often leads to better results in the conversion, but is also often unavailable. Thus i-vectors are able to fill this gap in the lack of available labeled data and the loss of conversion quality.  

In the instance of voice conversion, i-vectors are made of speaker super-vectors trained on GMMs and low dimensional features that represent an individual speaker's features \parencite{wu2016}. This is extracted per utterance and then averaged to form an i-vector that represents an individual speaker. In this way, a source speaker's i-vector can be approximated towards a target speaker's i-vector by a mapping function using neural networks, gaussian mixture models, or other appropriate algorithms. 

The usage of i-vectors in voice conversion has been seen in works such as \textcite{wu2016} and \textcite{kinnunen2017}. Following \textcite{kinnunen2017}, the usage of i-vectors in voice conversion aligns perfectly with the task as it is highly similar to speaker verification; however instead of being a classification task (e.g. is this said speaker or not), voice conversion is a regression task. 
In \textcite{wu2016}, they test and compare the performance of using plain mel-cepstral coefficients (MCCs) against i-vectors by training a variety of systems.
Among their systems, they utilize a strategy known as the _average voice model_, which models what an average speaker would sound like by utilizing a large amount of parallel utterances, which also allows for conversion between two speakers _without_ having parallel utterances. In order to compare MCCs vs. i-vectors, they train systems using MCCs as features with a deep bi-directional long-short term memory neural (DBLSTM) network architecture,  a DBLSTM combined with an average voice model (DBLSTM + AVM), and a DBLSTM combined with an average voice model retrained on some paralleled data from the testing source-target speakers (DBLSTM + RM). They then train another system with i-vectors using the DBLSTM and average voice model (DBLSTM + AVM + i-vectors). In order to evaluate these models, they provide both an objective evaluation using a measure known as mel-cepstral distortion (MCD) and a subjective evaluation rated on quality and similarity, which was decided by the votes of 20 listeners. 

Following the results of the objective evaluation, they find that the system with the lowest mel-cepstral distortion (e.g. the best system) is the DBLSTM + RM model, followed by the DBLSTM + AVM model, with the regular DBLSTM system and DBLSTM + AVM + i-vector system performing roughly the same. They note that the DBLSTM + RM system likely performed the best because of the inclusion of parallel data from the test dataset, while the DBLSTM + AVM outperformed the regular DBLSTM likely due to the size of the training data. However, they do not give much indication as to why the DBLSTM + AVM and DBLSTM + AVM + i-vectors perform similarly. Based off of the MCD alone, it would seem that i-vectors do not provide much benefit; however they emphasize that the DBLSTM + RM system does include parallel data while the DBLSTM + AVM + i-vectors system does not.

In the subjective evaluation, they compare the four systems by using an ABX preference test to compare: DBLSTM + RM vs. DBLSTM, DBLSTM + AVM + i-vectors vs. DBLSTM + RM and DBLSTM + AVM + i-vectors vs. DBLSTM + AVM. With each pair, they have the listeners evaluate 10 sentences for a total of 200 votes for each system. Following the results, they find that the DBLSTM + AVM + i-vectors system outperforms the DBLSTM + AVM system in both the speech quality and speaker similarity categories with statistical significance, which shows that the average voice model _without_ i-vectors (e.g. MCCs only cannot capture speaker specific information. They also find that the DBLSTM + RM system outperforms the plain DBLSTM system with statistical significance, indicating that the average voice model is not only useful, but also helps reduce the amount of parallel training data required to improve the performance. Finally, they find that the DBLSTM + AVM + i-vectors system was rated slightly higher in quality, but opposite in similarity. However this was without statistical significance, indicating that they perform roughly the same. From this study, \textcite{wu2016} concludes that the DBLSTM + AVM + i-vectors method has potential as it allows for great flexibility to generate the target speaker spectrum without using parallel data. 

As oppose to \textcite{wu2016} which utilizes the average voice model in order to create a strong voice conversion system, \textcite{kinnunen2017} takes a different approach by \colorbox{magenta}{[hmmm...]}

\textcite{demarco2013}, present a through analysis of the usage of i-vectors in classifying native British accents. \colorbox{magenta}{[continue here or move to accent conversion?]}


Even though systematic objective and subjective evaluation against older methods do indicate that recent methods have improved upon the older ones, comparing the performance of these systems against a true human voice, or perhaps more fairly, against other recent systems in other areas of speech technology, these systems still seem to leave a lot left to be desired. For example, in listening to the audio of \textcite{wu2016}\footnote{Visit http://www.nwpu-aslp.org/vc/apsipa-jiewu-demo.pptx to hear samples.} \colorbox{magenta}{[Maybe change footnotes to say listen to audio on the accompanying disk?]} it is apparent that regardless of the low quality of the original source and target audios, the quality of the converted audio sounds muffled. This can be attributed to the various nuanced steps and features required to have high quality voice conversion.  

For example, in a shared task dedicated to voice conversion, appropriately called *The Voice Conversion Challenge* where many leading research groups involved in speech technology around the world have submitted systems in attempts to tackle the issue. In the second iteration of the challenge \textcite{lorenzo-trueba2018}, the organizers proposed both a parallel and non-parallel version of the task, both of which were evaluated on natural and similarity using crowdsourcing.

The type of systems submitted to the 2018 edition of the task displays the current state of voice conversion and perhaps machine learning research in general as this year saw a huge increase in the number of systems using neural networks. However, it does not go without saying that there were indeed systems that used more traditional statistical methods, such as Gaussian Mixture Models (GMM) and one of its variations, differential GMM (DIFFGMM).

In order to evaluate the systems, a group of roughly 300 listeners were gathered to carry out a perceptual evaluation. The systems were evaluated on two main measures: naturalness, which was evaluated on a scale of 1 (completely unnatural) to 5 (completely natural); and similarity, which was evaluated using a same/different paradigm. Following the results, only one system, referred to as N10, was able to outperform the baseline in terms of naturalness (alongside the original source and target audios). When observing the performance of other systems in terms of similarity, we see about 5 our of 23 submitted systems outperforming the baseline. From this, we can conclude that it easier to create a system with high similarity than high naturalness, which is consistent with other common systems.  

In discussing the results of the N10 system, the authors credit the success of the system to the *hundreds of hours* of external speech data that was utilized to train a model to recognize content-related features, as well manual fine-tuning. The creators of this system also made use of WaveNet, a novel high-fidelity vocoder and dozens of hours of clean English speech, which could also explain the success of their results. Thus, as previously discussed, we can conclude that creating a high-fidelity voice conversion requires not only appropriate fine-tuning of the data, but also a large amount of external data to support the system.

Thus, even though many systems were neural network based, only one neural network based system was able to outperform the sprocket GMM-based baseline, which could suggest that NN-based methods require proper fine-tuning of the hyperparameters.

Although we see limitations in the systems presented in The 2018 Voice Conversion Challenge, there have bene other efforts to present high quality voice conversion systems in works such as and \textcite{nguyen2016} and \textcite{fang2018}.

\textcite{fang2018} leverages a cycle-consistent adversarial network (CycleGAN) architecture, a variation of the recently trending generative adversarial network (GAN) architecture, which was originally used for unpaired image-to-image translation. For example, GANs have been shown to be able to convert images of zebras into horses, as well as winter into summer.
 
While not necessarily directly related to the standard idea of voice conversion, there have also been some incredible breakthroughs in systems set forth by research teams at Google Brain. One such system involves the Tacotron end-to-end system, which has been proposed to replace the current set-up of text-to-speech systems by reducing the amount of components (decoder, vocoder, etc.) into one piece. The researchers working on this system have recently revealed a impressive system that also takes advantage of deep neural networks to encode speaker characteristics into embeddings, which are then utilized to transfer style \parencite{wang2018}. They show how their system is capable of transferring a variety of emotions and accents, making the synthesized audio sound more human-like. Samples of these audios can be found at the following link\footnote{Visit \url{https://google.github.io/tacotron/publications/global_style_tokens/} to hear samples.}.

\colorbox{magenta}{Also add to disc?}

Even though the these systems created by Google Brain are highly impressive,  it is evident that the reason for the success of their systems is due to very fine-grained parameter tuning and the availability of large-scale, high quality data that many research institutions likely do not have access to or have funding for. For example, if we juxtaposed the audio from the Google Brain systems to the best performing system of the Voice Conversion Challenge 2018, we can still observe some disfluencies in the audios of the best system of the VCC 2018.  Thus, it may be a long while before the general public has the ability to completely replicate such systems and before this work trickles in to the domain of accent conversion. 

# Accent conversion 
Due to the specialized nature of accent conversion as compared to voice conversion, there are fewer articles and systems available for reference. In fact, most of the recent articles that are easily accessible on accent conversion were all published by the same group of researchers at Texas A&M University.

However, before the work of these researchers, works such as \textcite{yan2004} and \textcite{huckvale2007}, explored manipulating various features in order to observe their relationship with a perceived accent. In \textcite{yan2004}, they manipulate spectral features, intonation patterns and duration in order to observe their correlation across British, Australian and American accents. Through an ABX perceptual test, they found that 75% of the synthesized utterances were evaluated as having the native accent, highlighting the potential for segmental accent conversion.

In \textcite{huckvale2007}, they examine the relationship between intelligibility and the of morphing various segmental and suprasegmental features such as pitch, rhythm and segments of an English TTS system designed to speak 'accented' Japanese. This TTS system was designed by creating a custom dictionary and mapping the Japanese sounds to their closest English counterpart. They found through native speaker evaluation that morphing pitch and rhythm individually had no effect, and similarly modifying segments alone only gave a small improvement. However, they discovered that combining the morphing of all of these features created a large increase in intelligibility, with intelligibility going up from 57% as seen in their lowest-performing system to 84%. The results emphasize the need to consider the interaction between segmental and suprasegmental in the conversion task.

In one of the earliest works from the Texas A&M research group, and perhaps a key influential paper to this work, \textcite{felps2009} examines the potential of using a method known as Pitch-Synchronous Overlap and Add (PSOLA) for accent with the motivation of applying it in the context of language learning. Specifically, they utilize a specialized PSOLA method known as Fourier-domain PSOLA (FD-PSOLA), as it performs best in preventing spectral distortion when modifying the pitch. \colorbox{magenta}{[explain PSOLA]} In order to conduct the conversion process, they separate the converting of the segments and the converting of the prosody into two separate parts, with both parts evaluated individually and combined. In evaluating their method, they measured the accentedness, acoustic quality and identity of each converted audio using auditory tests given to a number of speakers. Similar to \textcite{huckvale2007}, they observe that the combination of prosodic and segemental transformation lead to a large improvement in reducing foreign accent. However, in terms of quality, they found that all transformations led to lower ratings, which likely indicates the loss of some spectral information. The identity ratings proved to be the most interesting as \textcite{felps2009} find that the listeners indicate a 'third' speaker. In other words, the converted audio sounds neither like the source or target speaker. Thus \textcite{felps2009} concludes that while accentedness is reduced by their system, their proposed system also loses the necessary information needed to retain the speaker's identity.

\colorbox{magenta}{\textcite{felps2010}} ? [paper on evaluation]

\colorbox{magenta}{\textcite{aryal2010}}

\colorbox{magenta}{\textcite{felps2012}}


With that said, \textcite{aryal2014} and other works done by the group of researchers have made efforts to address the challenge. Throughout their research, they test a variety of methodologies, including accent conversion through voice morphing and articulatory synthesis. In the same work of \textcite{aryal2014}, they propose a variation to standard forced alignment techniques used in voice conversion to pair frames based on acoustic similarity. 

To achieve this, they first use dynamic time warping (DTW) to align parallel utterances from the L1 and L2 speakers in order to apply vocal tract length normalization to dampen the differences in pitch. They then extract sequences of 24 MFCCs per utterance, and cluster the MFCC vectors into 512 clusters using the \textit{k}-means algorithm to easily find the most acoustically similar sound for each frame.  The most acoustically similar frames are then calculated by finding the closest L2 cluster, and then selecting the most similar frame within the cluster. After the closest vectors are paired, they map the conversion using a GMM. 

In order to evaluate their system, they had a group of 13 participants rate 12 utterances from the test set for their perceived accent (Which utterance was less accented?) and perceived speaker identity (Does utterance X sound more similar to A or B?). This system was compared to a standard voice conversion system that uses standard forced alignment and trained using GMMs. They found that comparing the AC system to the original L2 audio resulted in participants rating the converted audio as sounding less accented 86% of the time, while the VC system compared to the original L2 audio was rated at 91% of the time. However, when the converted audios from both systems were compared, participants rated the AC system to be less accented compared to the VC system 59% of the time. It was also concluded that the AC system was more successful in retaining speaker identity, as the participants found the converted audio more similar to the L2 speaker 78% of the time. More interestingly, they found that the AC system was especially effective in converted utterances that are harder for the L2 speaker to pronounce. This was measured by examining the relationship between the number of phonemes that do not exist in the L2 language (in this case Spanish), and the number of listeners who judged the converted speech as sounding less accented.They found that there was a 0.86 correlation, indicating the robustness of the AC system. Thus, it appears that adjusting the alignment method to align acoustically similar sounds is a good start for accent conversion systems. 

\colorbox{magenta}{\textcite{aryal2014a}}

In \textcite{aryal2015}, we see a more novel method that looks beyond acoustic features to perform accent conversion. Citing the results of their previous work, they motivate the usage of articulatory gesture information in accent conversion reasoning that acoustic-based systems often struggle in the challenge of separating accent from speaker identity, which causes the accent converted audio to sound like a combination of the L1 speaker and L2 speaker.  To do this, they propose a system that combines both the more standard acoustic information like aperiodicity, pitch and energy from the L1 speaker with articulatory information recording using an electromagnetic articulograph (EMA). Like many recent works, they test a DNN-based mapping function between the L1 and L2 data, which they compare to the previously popular GMM-based system. 

In the evaluation of their system, they again use crowdsourced efforts to rate their system based on intelligibility, accentedness, and speaker identity. According to their sample size of 15 participants,  they find that the DNN-based system was rated to have a 4.3 out of 7 in terms of intelligibility as compared to 3.84 out of 7 for the GMM-based system, proving that including articulatory gesture information and DNNs are more robust in this instance. The participants also rated the DNN-based system to be more native-like in 67% of cases as compared to the GMM-based system. With that said, the test set was only 15 sentences, which indicates that 10 out of 15 sentences were better with the DNN system; thus the test set used may be too small to draw hard conclusions. The most important conclusions drawn from their experiments was that of the voice identity assessment. In asking the participants to rate whether an MFCC compression and AC audio from the DNN and GMM-based systems, they found that the participants were fairly confident that the two audios were from the same person with both systems, with the DNN-based system outperforming the GMM-based system as before at a score of 4.3 out of 7 on average, and the GMM-based system at a score of 4.0. However, this is difficult to compare to more common acoustic-only accent conversion systems, as this is not including in their evaluation. With that said, it may be possible to conclude that this would outperform acoustic-based systems, as they proposed this system to tackle flaws in their previous work.

Evidentally, although including articulatory gesture information seems to improve the performance of accent conversion systems, as discussed in the closing remarks of their paper, recording articulatory gesture information can cost a great deal of money and time \parencite{aryal2015}. Most publically (and privatized) speech corpora also do not include this type of information, meaning that experimenting with it in accent conversion at a broader scale is unfeasibile. Thus, it is ambitious to accept adding articulatory information to accent conversion systems and further work needs to be done in order to scale standard audio-based speech corpora.

Departing from utilizing articulatory gesture information, \textcite{zhao2018a} returns to a more simpler method similar to \textcite{aryal2014}. However, instead of matching frames based on their _acoustic_ similarity, they test matching frames based on their _phonetic_ similarity. They do this by mapping the frames of each source and target speaker into something referred to as a _phonetic posteriorgram_. Following \textcite{hazen2009}, a phonetic posteriorgram is 'a time vs. class matrix that represents the posterior probability of each phonetic class for each time frame'. An example of a phonetic posteriorgram taken from \textcite{hazen2009} can be seen in \autoref{fig:phonetic-postgram}. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.22]{img/phonetic-postgram.png}
\caption{An example posteriorgram representation for the spoken phrase `basketball and baseball'. The x-axis represents the time across the utterance and the y-axis represents the possible phonemes.}
\label{fig:phonetic-postgram}
\end{figure} 

The phonetic posteriorgrams are computed using a native English speaker-independent acoustic model and then the most similar source and target frames are matched by calculating something known as the Kullback-Leibler divergence (0 indicating similar or same behavior, 1 indicating completely different) between the source and target posteriorgrams. After matching the frames, they train GMMs with 128-mixture components to model the distribution of the MCEPs to convert the speech. The performance of this proposed system is then compared to a standard voice conversion system using dynamic time warping to align the frames and the system described in \textcite{aryal2014}.

Like the previous works of \textcite{aryal2014} and \textcite{aryal2015}, this work also approaches evaluation using a perceptual listening test to evaluate acoustic quality, speaker identity and accentedness. However, in this work, they evaluate over 50 test utterances using 30 participants, which better substantiates their results compared to the evaluation of 10-15 utterances by 10-15 participants in some of their older studies. 

In terms of acoustic quality, they found that their proposed posteriorgram method received a score of 3.0 on a Mean Opinion Score scale of 1 to 5 (with 1 being 'bad' and 5 being 'excellent'), as compared to a score of 2.6 using the method from \textcite{aryal2014} and 2.5 for standard voice conversion. 

\colorbox{magenta}{[continue]}

Aside from the work conducted by the research group at Texas A&M University, it appears to be that there are not many, if any other researchers currently working in this subarea of accent conversion. This may be because voice conversion still leaves a lot to be desired itself, suggesting that most researchers may want to focus on perfecting standard voice conversion before attempting to tackle something more fine-grained.  However, as research in voice conversion continues to expand, it also creates the potential to apply methodologies from voice conversion to accent conversion. Following the general methodologies of voice conversion, I hypothesize that it should be plausible to convert accents in a similar fashion and apply more recent innovations to propose state-of-the-art methods.   

