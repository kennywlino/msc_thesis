
@article{holland1999,
  title = {Preliminary {{Tests}} of {{Language Learning}} in a {{Speech}}-{{Interactive Graphics Microworld}}},
  volume = {16},
  abstract = {A speech-interactive graphics microworld is described in which learners speak problem-solving directions to an animated agent and in which new scenarios can be authored. A proof-of-concept application is illustrated for sustaining basic speaking skill in Modern Standard Arabic. Preliminary tests of the application are summarized involving learners from both university and military settings. Problems are discussed in predicting and measuring learning gains based on brief exposure to new technologies.},
  number = {3},
  journal = {CALICO Journal},
  author = {Holland, V Melissa and Kaplan, Jonathan D and Sabol, Mark A},
  year = {1999},
  pages = {22},
  file = {/Users/kennylino/Documents/Articles/HollandV/undefined/Holland_Preliminary Tests of Language Learning in a Speech-Interactive Graphics.pdf}
}

@article{munro1999,
  title = {Foreign {{Accent}}, {{Comprehensibility}}, and {{Intelligibility}} in the {{Speech}} of {{Second Language Learners}}},
  volume = {49},
  issn = {00238333},
  doi = {10.1111/0023-8333.49.s1.8},
  language = {en},
  journal = {Language Learning},
  author = {Munro, Murray J. and Derwing, Tracey M.},
  year = {1999},
  pages = {285-310},
  file = {/Users/kennylino/Documents/Articles/MunroM/1999/Munro_1999_Foreign Accent, Comprehensibility, and Intelligibility in the Speech of Second.pdf}
}

@article{thompson2004,
  title = {Decoding Speech Prosody: {{Do}} Music Lessons Help?},
  volume = {4},
  issn = {1931-1516, 1528-3542},
  shorttitle = {Decoding Speech Prosody},
  doi = {10.1037/1528-3542.4.1.46},
  language = {en},
  number = {1},
  journal = {Emotion},
  author = {Thompson, William Forde and Schellenberg, E. Glenn and Husain, Gabriela},
  year = {2004},
  pages = {46-64},
  file = {/Users/kennylino/Documents/Articles/ThompsonW/2004/Thompson_2004_Decoding speech prosody.pdf}
}

@inproceedings{eskenazi1996,
  title = {Detection of Foreign Speakers' Pronunciation Errors for Second Language Training-Preliminary Results},
  volume = {3},
  isbn = {978-0-7803-3555-4},
  doi = {10.1109/ICSLP.1996.607892},
  publisher = {{IEEE}},
  author = {Eskenazi, M.},
  year = {1996},
  pages = {1465-1468},
  file = {/Users/kennylino/Documents/Articles/EskenaziM/1996/Eskenazi_1996_Detection of foreign speakers' pronunciation errors for second language.pdf}
}

@article{eskenazi2009,
  title = {An Overview of Spoken Language Technology for Education},
  volume = {51},
  issn = {01676393},
  doi = {10.1016/j.specom.2009.04.005},
  abstract = {This paper reviews research in spoken language technology for education and more specifically for language learning. It traces the history of the domain and then groups main issues in the interaction with the student. It addresses the modalities of interaction and their implementation issues and algorithms. Then it discusses one user population \textendash{} children \textendash{} and an application for them. Finally it has a discussion of overall systems. It can be used as an introduction to the field and a source of reference materials.},
  language = {en},
  number = {10},
  journal = {Speech Communication},
  author = {Eskenazi, Maxine},
  month = oct,
  year = {2009},
  keywords = {read},
  pages = {832-844},
  file = {/Users/kennylino/Documents/Articles/EskenaziM/2009/Eskenazi_2009_An overview of spoken language technology for education.pdf}
}

@article{hwang2016,
  title = {Evaluating Listening and Speaking Skills in a Mobile Game-Based Learning Environment with Situational Contexts},
  volume = {29},
  issn = {0958-8221, 1744-3210},
  doi = {10.1080/09588221.2015.1016438},
  language = {en},
  number = {4},
  journal = {Computer Assisted Language Learning},
  author = {Hwang, Wu-Yuin and Shih, Timothy K. and Ma, Zhao-Heng and Shadiev, Rustam and Chen, Shu-Yu},
  month = may,
  year = {2016},
  pages = {639-657},
  file = {/Users/kennylino/Documents/Articles/HwangW/2016/Hwang_2016_Evaluating listening and speaking skills in a mobile game-based learning.pdf}
}

@article{felps2009,
  title = {Foreign Accent Conversion in Computer Assisted Pronunciation Training},
  volume = {51},
  issn = {01676393},
  doi = {10.1016/j.specom.2008.11.004},
  abstract = {Learners of a second language practice their pronunciation by listening to and imitating utterances from native speakers. Recent research has shown that choosing a well-matched native speaker to imitate can have a positive impact on pronunciation training. Here we propose a voicetransformation technique that can be used to generate the (arguably) ideal voice to imitate: the own voice of the learner with a native accent. Our work extends previous research, which suggests that providing learners with prosodically corrected versions of their utterances can be a suitable form of feedback in computer assisted pronunciation training. Our technique provides a conversion of both prosodic and segmental characteristics by means of a pitch-synchronous decomposition of speech into glottal excitation and spectral envelope. We apply the technique to a corpus containing parallel recordings of foreign-accented and native-accented utterances, and validate the resulting accent conversions through a series of perceptual experiments. Our results indicate that the technique can reduce foreign accentedness without significantly altering the voice quality properties of the foreign speaker. Finally, we propose a pedagogical strategy for integrating accent conversion as a form of behavioral shaping in computer assisted pronunciation training.},
  language = {en},
  number = {10},
  journal = {Speech Communication},
  author = {Felps, Daniel and Bortfeld, Heather and {Gutierrez-Osuna}, Ricardo},
  month = oct,
  year = {2009},
  pages = {920-932},
  file = {/Users/kennylino/Documents/Articles/FelpsD/2009/Felps_2009_Foreign accent conversion in computer assisted pronunciation training.pdf}
}

@incollection{cardoso2017,
  title = {Can an Interactive Digital Game Help {{French}} Learners Improve Their Pronunciation?},
  isbn = {978-2-490-05704-7},
  abstract = {This study examines the effects of the pedagogical use of an interactive mobile digital game, Pr\^et \`a N\'egocier (P\`aN), on improving learners' pronunciation of French as a Second Language (FSL), using three holistic measures: comprehensibility, fluency, and overall pronunciation. Two groups of FSL learners engaged in different types of game-playing over one month: while the experimental group played P\`aN, the control group engaged in paper-based gamified information gap activities. Following a pre-test/post-test research design, our findings revealed no statistically significant differences between the two groups.},
  booktitle = {{{CALL}} in a Climate of Change: Adapting to Turbulent Global Conditions \textendash{} Short Papers from {{EUROCALL}} 2017},
  publisher = {{Research-publishing.net}},
  author = {Cardoso, Walcir and Rueb, Avery and Grimshaw, Jennica},
  editor = {Borthwick, Kate and Bradley, Linda and Thou\"esny, Sylvie},
  month = dec,
  year = {2017},
  pages = {67-72},
  file = {/Users/kennylino/Documents/Articles/CardosoW/2017/Cardoso_2017_Can an interactive digital game help French learners improve their pronunciation.pdf},
  doi = {10.14705/rpnet.2017.eurocall2017.691}
}

@article{hardison2004,
  title = {Generalization of Computer Assisted Prosody Training: {{Quantitative}} and Qualitative Findings},
  abstract = {Two experiments investigated the effectiveness of computer-assisted prosody training, its generalization to novel sentences and segmental accuracy, and the relationship between prosodic and lexical information in long-term memory. Experiment 1, using a pretest-posttest design, provided native English-speaking learners of French with 3 weeks of training focused on prosody using a real-time computerized pitch display. Multiple exemplars produced by native speakers (NSs) of French and stored on hard disk provided training feedback. Learners' recorded pre- and posttest productions were presented to NSs for evaluation in two conditions: filtered (unintelligible segmental information) and unfiltered. Ratings using 7-point scales for the prosody and segmental accuracy of unfiltered samples revealed significant improvement in prosody with generalization to segmental production and novel sentences. Comparison of prosody ratings for filtered and unfiltered samples revealed some segmental influence on the pretest ratings of prosody. In Experiment 2, involving a memory recall task using filtered stimuli of reduced intelligibility, learners identified the exact lexical content of an average of 80\% of the training sentences based on prosodic cues consistent with exemplar-based learning models. Questionnaire responses indicated a greater awareness of the various aspects of speech and increased confidence in producing another language.},
  journal = {Language Learning},
  author = {Hardison, Debra M.},
  year = {2004},
  pages = {19},
  file = {/Users/kennylino/Documents/Articles/HardisonD/undefined/Hardison_GENERALIZATION OF COMPUTER-ASSISTED PROSODY TRAINING.pdf}
}

@article{hyde-simon2008,
  title = {{\emph{Phonology and }}{{{\emph{Second Language Acquisition}}}} - {{Edited}} by {{Jette G}}. {{Hansen Edwards}} and {{Mary L}}. {{Zampini}}},
  volume = {18},
  issn = {08026106, 14734192},
  doi = {10.1111/j.1473-4192.2008.00205.x},
  language = {en},
  number = {3},
  journal = {International Journal of Applied Linguistics},
  author = {{Hyde-Simon}, Caroline},
  month = nov,
  year = {2008},
  pages = {306-309},
  file = {/Users/kennylino/Documents/Articles/Hyde-SimonC/2008/Hyde-Simon_2008_iPhonology and Second Language Acquisition-i - Edited by Jette G.pdf}
}

@article{shroff2016,
  title = {{{GAMIFIED PEDAGOGY}}: {{EXAMINING HOW A PHONETICS APP COUPLED WITH EFFECTIVE PEDAGOGY CAN SUPPORT LEARNING}}},
  abstract = {Research has demonstrated that educational game-based apps may provide an approach to instruction in education that allows for greater learning outcomes. The focal context of this paper centres around the discussion of how gamified pedagogy supports learning. The first part of this paper will delve into the components of gaming, including the application of gamification to education and the methods by which digital game-based components such as scores and rewards are used to engage and motivate learners. The second part will focus on existing research on gaming pedagogy and the gaming elements of a phonetics app developed by the Resource Centre for Ubiquitous Learning and Integrated Pedagogy (ULIP) at Hong Kong Baptist University. The gamified pedagogical element of the app is designed to offer levels of challenge that motivate the players by making learning more exciting and rewarding. The game-based elements of the app promote active student involvement in learning, as the games are specifically designed to provide challenges and goals for players. Moreover, the need to capture and maintain the players' attention through visual experiences and audio designs is also an important element in the design of the app. When learners are engaged in a game-based app of this nature, they are not only reinforcing their cognitive skills, but they are also constantly drawing connections between images, text and sounds, thereby allowing students to learn and practise basic skills in order to master complex tasks.},
  author = {Shroff, Ronnie H and Keyes, Christopher J and Wee, Lian-Hee},
  year = {2016},
  pages = {18},
  file = {/Users/kennylino/Documents/Articles/ShroffR/2016/Shroff_2016_GAMIFIED PEDAGOGY.pdf}
}

@article{tejedor-garcia2016,
  title = {Improving {{L2 Production}} with a {{Gamified Computer}}-{{Assisted Pronunciation Training Tool}}, {{TipTopTalk}}!},
  abstract = {We present a foreign language (L2) pronunciation training serious game, TipTopTalk!, based on the minimal-pairs technique. We carried out a three-week test experiment where participants had to overcome several challenges including exposure, discrimination and production, while using Text-To-Speech (TTS) and Automatic Speech Recognition (ASR) systems in a mobile application. The quality of users' production is measured in order to assess their improvement. The application implements gamification resources with the aim of promoting continued practice. Preliminary results show that users with poorer initial performance levels make relatively more progress than the rest. However, it is desirable to include specific and individualized feedback in future versions so as to avoid the performance drop detected after the protracted use of the tool.},
  author = {{Tejedor-Garc\'ia}, Cristian},
  year = {2016},
  keywords = {read},
  pages = {9},
  file = {/Users/kennylino/Documents/Articles/Tejedor-GarcC/undefined/Tejedor-Garc_Improving L2 Production with a Gamiﬁed Computer-Assisted Pronunciation Training.pdf}
}

@inproceedings{tejedor-garcia2017,
  title = {Evaluating the {{Efficiency}} of {{Synthetic Voice}} for {{Providing Corrective Feedback}} in a {{Pronunciation Training Tool Based}} on {{Minimal Pairs}}},
  doi = {10.21437/SLaTE.2017-5},
  abstract = {Feedback is an important concern in Computer-Assisted Pronunciation Training (CAPT), inasmuch as it bears on a system's capability to correct users' input and promote improved L2 pronunciation performance in the target language. In this paper, we test the use of synthetic voice as a corrective feedback resource. A group of students used a CAPT tool for carrying out a battery of minimal-pair discrimination-production tasks; to those who failed in production routines, the system offered the possibility of undergoing extra training by using synthetic voice as a model in a round of exposure exercises. Participants who made use of this resource significantly outperformed those who directly repeated the previously failed exercise. Results suggest that the Text-To-Speech systems offered by current operating systems (Android in our case) must be considered a relevant feedback resource in pronunciation training, especially when combined with efficient teaching methods.},
  language = {en},
  publisher = {{ISCA}},
  author = {{Tejedor-Garc\'ia}, Cristian and Escudero, David and {Gonz\'alez-Ferreras}, C\'esar and {C\'amara-Arenas}, Enrique and {Carde\~noso-Payo}, Valent\'in},
  month = aug,
  year = {2017},
  pages = {25-29},
  file = {/Users/kennylino/Documents/Articles/Tejedor-GarcíaC/2017/Tejedor-García_2017_Evaluating the Efficiency of Synthetic Voice for Providing Corrective Feedback.pdf}
}

@article{eskenazi1998,
  title = {The {{Fluency Pronunciation Trainer}}},
  abstract = {In this article we describe the basis of the Fluency project for foreign language pronunciation training using automatic speech recognition. We describe the theoretical base, the interactive duration correction module, and our work toward adaptation to the way in which the user learns best. We show results in preliminary tests of the latter, and discuss future directions of the project.},
  author = {Eskenazi, Maxine and Hansma, Scott},
  year = {1998},
  keywords = {read},
  pages = {6},
  file = {/Users/kennylino/Documents/Articles/EskenaziM/undefined/Eskenazi_The Fluency Pronunciation Trainer.pdf}
}

@incollection{lengeris2012,
  address = {Dordrecht},
  title = {Prosody and {{Second Language Teaching}}: {{Lessons}} from {{L2 Speech Perception}} and {{Production Research}}},
  volume = {15},
  isbn = {978-94-007-3882-9 978-94-007-3883-6},
  shorttitle = {Prosody and {{Second Language Teaching}}},
  booktitle = {Pragmatics and {{Prosody}} in {{English Language Teaching}}},
  publisher = {{Springer Netherlands}},
  author = {Lengeris, Angelos},
  editor = {{Romero-Trillo}, Jes\'us},
  year = {2012},
  pages = {25-40},
  file = {/Users/kennylino/Documents/Articles/LengerisA/2012/Lengeris_2012_Prosody and Second Language Teaching.pdf},
  doi = {10.1007/978-94-007-3883-6_3}
}

@article{neri2002,
  title = {The {{Pedagogy}}-{{Technology Interface}} in {{Computer Assisted Pronunciation Training}}},
  volume = {15},
  issn = {0958-8221},
  doi = {10.1076/call.15.5.441.13473},
  abstract = {In this paper, we examine the relationship between pedagogy and technology in Computer Assisted Pronunciation Training (CAPT) courseware. First, we will analyse available literature on second language pronunciation teaching and learning in order to derive some general guidelines for effective training. Second, we will present an appraisal of various CAPT systems with a view to establishing whether they meet pedagogical requirements. In this respect, we will show that many commercial systems tend to prefer technological novelties to the detriment of pedagogical criteria that could benefit the learner more. While examining the limitations of today's technology, we will consider possible ways to deal with these shortcomings. Finally, we will combine the information thus gathered to suggest some recommendations for future CAPT.},
  number = {5},
  journal = {Computer Assisted Language Learning},
  author = {Neri, Ambra and Cucchiarini, Catia and Strik, Helmer and Boves, Lou},
  month = dec,
  year = {2002},
  keywords = {read},
  pages = {441-467},
  file = {/Users/kennylino/Documents/Articles/NeriA/2002/Neri_2002_The Pedagogy-Technology Interface in Computer Assisted Pronunciation Training.pdf}
}

@article{pellegrino2015,
  title = {Self-Imitation in Prosody Training: {{A}} Study on {{Japanese}} Learners of {{Italian}}},
  abstract = {The proficiency in a second language is fully attained only if students have learnt to modulate the rhythmic and prosodic parameters equivalent to those of the native speakers. This study is aimed to test the pedagogical effectiveness of the selfimitation technique for the purpose of developing a native-like prosodic competence. Seven intermediate Japanese learners of Italian (NNSs) and 2 native Italian speakers (NSs) were involved in a read speech activity. NSs and NNSs were asked to read and record two Italian sentences conveying three different pragmatic functions (granting, order, request). NNSs performed the task twice, before and after the self-imitation prosodic training. The items used for the training were obtained by transferring the suprasegmental features of the native speakers, used as donors, to the Japanese learners, considered as the receivers. During the training phase, Japanese learners mimic their utterances previously modified to match the prosody of the reference native speaker, and then recorded the new performance. Seventeen native Italian listeners rated pre- and post-training productions for pragmatic function and accentedness. The results indicate that selfimitation promoted an improvement in learners' performances in terms of communicative effectiveness. Conversely, average rate of accentedness does not change significantly before and after training.},
  author = {Pellegrino, Elisa and Vigliano, Debora},
  year = {2015},
  pages = {5},
  file = {/Users/kennylino/Documents/Articles/PellegrinoE/2015/Pellegrino_2015_Self-imitation in prosody training.pdf}
}

@article{darcy2012,
  title = {Bringing Pronunciation Instruction Back into the Classroom: An {{ESL}} Teachers' Pronunciation "Toolbox"},
  author = {Darcy, Isabelle and Ewert, Doreen and Lidster, Ryan},
  year = {2012},
  keywords = {read},
  pages = {18},
  file = {/Users/kennylino/Documents/Articles/DarcyI/2012/Darcy_2012_BRINGING PRONUNCIATION INSTRUCTION BACK INTO THE CLASSROOM.pdf}
}

@article{gangireddy2015,
  title = {Prosodically-Enhanced {{Recurrent Neural Network Language Models}}},
  abstract = {Recurrent neural network language models have been shown to consistently reduce the word error rates (WERs) of large vocabulary speech recognition tasks. In this work we propose to enhance the RNNLMs with prosodic features computed using the context of the current word. Since it is plausible to compute the prosody features at the word and syllable level we have trained the models on prosody features computed at both these levels. To investigate the effectiveness of proposed models we report perplexity and WER for two speech recognition tasks, Switchboard and TED. We observed substantial improvements in perplexity and small improvements in WER.},
  author = {Gangireddy, Siva Reddy and Renals, Steve and Nankaku, Yoshihiko and Lee, Akinobu},
  year = {2015},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/WUKRJSGR/Gangireddy et al. - Prosodically-enhanced Recurrent Neural Network Lan.pdf}
}

@article{gauthier2009,
  title = {Learning {{Prosodic Focus}} from {{Continuous Speech Input}}:{{A Neural Network Exploration}}},
  volume = {5},
  issn = {1547-5441, 1547-3341},
  shorttitle = {Learning {{Prosodic Focus}} from {{Continuous Speech Input}}},
  doi = {10.1080/15475440802698524},
  language = {en},
  number = {2},
  journal = {Language Learning and Development},
  author = {Gauthier, Bruno and Shi, Rushen and Xu, Yi},
  month = apr,
  year = {2009},
  pages = {94-114},
  file = {/Users/kennylino/Zotero/storage/NZ57BXXN/Gauthier et al. - 2009 - Learning Prosodic Focus from Continuous Speech Inp.pdf}
}

@article{bernardy2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03952},
  primaryClass = {cs},
  title = {Modelling Prosodic Structure Using {{Artificial Neural Networks}}},
  abstract = {The ability to accurately perceive whether a speaker is asking a question or is making a statement is crucial for any successful interaction. However, learning and classifying tonal patterns has been a challenging task for automatic speech recognition and for models of tonal representation, as tonal contours are characterized by significant variation. This paper provides a classification model of Cypriot Greek questions and statements. We evaluate two state-of-the-art network architectures: a Long Short-Term Memory (LSTM) network and a convolutional network (ConvNet). The ConvNet outperforms the LSTM in the classification task and exhibited an excellent performance with 95\% classification accuracy.},
  journal = {arXiv:1706.03952 [cs]},
  author = {Bernardy, Jean-Philippe and Themistocleous, Charalambos},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Computation and Language,read},
  file = {/Users/kennylino/Documents/Articles/BernardyJ/2017/Bernardy_2017_Modelling prosodic structure using Artificial Neural Networks.pdf;/Users/kennylino/Zotero/storage/3TDZUD4J/1706.html}
}

@phdthesis{vainio2001,
  address = {Helsinki},
  title = {Artificial Neural Network Based Prosody Models for {{Finnish}} Text-to-Speech Synthesis},
  language = {English},
  school = {University of Helsinki},
  author = {Vainio, Martti},
  year = {2001},
  file = {/Users/kennylino/Zotero/storage/VT9NT5CP/Vainio - 2001 - Artificial neural network based prosody models for.pdf},
  note = {OCLC: 58373280}
}

@article{han2014,
  title = {Speech {{Emotion Recognition Using Deep Neural Network}} and {{Extreme Learning Machine}}},
  abstract = {Speech emotion recognition is a challenging problem partly because it is unclear what features are effective for the task. In this paper we propose to utilize deep neural networks (DNNs) to extract high level features from raw data and show that they are effective for speech emotion recognition. We first produce an emotion state probability distribution for each speech segment using DNNs. We then construct utterance-level features from segment-level probability distributions. These utterancelevel features are then fed into an extreme learning machine (ELM), a special simple and efficient single-hidden-layer neural network, to identify utterance-level emotions. The experimental results demonstrate that the proposed approach effectively learns emotional information from low-level features and leads to 20\% relative accuracy improvement compared to the stateof-the-art approaches.},
  author = {Han, Kun and Yu, Dong and Tashev, Ivan},
  year = {2014},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/8SRWUCP5/Han et al. - Speech Emotion Recognition Using Deep Neural Netwo.pdf}
}

@inproceedings{fu2015,
  title = {Integrating Prosodic Information into Recurrent Neural Network Language Model for Speech Recognition},
  isbn = {978-988-14768-0-7},
  doi = {10.1109/APSIPA.2015.7415462},
  abstract = {Prosody is a kind of cues that are critical to human speech perception and comprehension, so it is plausible to integrate prosodic information into machine speech recognition. However, as a result of the supra-segmental nature, it is hard to integrate prosodic information with conventional acoustic features. Recently, RNNLMs have shown to be the state-of-theart language model in many tasks. We thus attempt to integrate prosodic information into RNNLMs for improving speech recognition performance based on rescoring strategy. Firstly, three word-level prosodic features are extracted from speech and then passed to RNNLMs separately. Therefore RNNLMs predict the next word based on prosodic features and word history. Experiments conducted on LibriSpeech Corpus show that the word error rate decreases from 8.07\% to 7.96\%. Secondly, prosodic information is combined on feature-level and modellevel for further improvements and word error rate decreases 4.71\% relatively.},
  language = {en},
  publisher = {{IEEE}},
  author = {Fu, Tong and Han, Yang and Li, Xiangang and Liu, Yi and Wu, Xihong},
  month = dec,
  year = {2015},
  pages = {1194-1197},
  file = {/Users/kennylino/Zotero/storage/C4KRBWHJ/Fu et al. - 2015 - Integrating prosodic information into recurrent ne.pdf}
}

@article{mao2014,
  title = {Learning {{Salient Features}} for {{Speech Emotion Recognition Using Convolutional Neural Networks}}},
  volume = {16},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2014.2360798},
  abstract = {As an essential way of human emotional behavior understanding, speech emotion recognition (SER) has attracted a great deal of attention in human-centered signal processing. Accuracy in SER heavily depends on finding good affect-related, discriminative features. In this paper, we propose to learn affect-salient features for SER using convolutional neural networks (CNN). The training of CNN involves two stages. In the first stage, unlabeled samples are used to learn local invariant features (LIF) using a variant of sparse auto-encoder (SAE) with reconstruction penalization. In the second step, LIF is used as the input to a feature extractor, salient discriminative feature analysis (SDFA), to learn affect-salient, discriminative features using a novel objective function that encourages feature saliency, orthogonality, and discrimination for SER. Our experimental results on benchmark datasets show that our approach leads to stable and robust recognition performance in complex scenes (e.g., with speaker and language variation, and environment distortion) and outperforms several well-established SER features.},
  language = {en},
  number = {8},
  journal = {IEEE Transactions on Multimedia},
  author = {Mao, Qirong and Dong, Ming and Huang, Zhengwei and Zhan, Yongzhao},
  month = dec,
  year = {2014},
  pages = {2203-2213},
  file = {/Users/kennylino/Zotero/storage/DRJXYBPX/Mao et al. - 2014 - Learning Salient Features for Speech Emotion Recog.pdf}
}

@inproceedings{stuhlsatz2011,
  title = {Deep Neural Networks for Acoustic Emotion Recognition: {{Raising}} the Benchmarks},
  isbn = {978-1-4577-0538-0},
  shorttitle = {Deep Neural Networks for Acoustic Emotion Recognition},
  doi = {10.1109/ICASSP.2011.5947651},
  abstract = {Deep Neural Networks (DNNs) denote multilayer artificial neural networks with more than one hidden layer and millions of free parameters. We propose a Generalized Discriminant Analysis (GerDA) based on DNNs to learn discriminative features of low dimension optimized with respect to a fast classification from a large set of acoustic features for emotion recognition. On nine frequently used emotional speech corpora, we compare the performance of GerDA features and their subsequent linear classification with previously reported benchmarks obtained using the same set of acoustic features classified by Support Vector Machines (SVMs). Our results impressively show that low-dimensional GerDA features capture hidden information from the acoustic features leading to a significantly raised unweighted average recall and considerably raised weighted average recall.},
  language = {en},
  publisher = {{IEEE}},
  author = {Stuhlsatz, Andre and Meyer, Christine and Eyben, Florian and Zielke, Thomas and Meier, Gunter and Schuller, Bjorn},
  month = may,
  year = {2011},
  pages = {5688-5691},
  file = {/Users/kennylino/Zotero/storage/M6MMQ2YC/Stuhlsatz et al. - 2011 - Deep neural networks for acoustic emotion recognit.pdf}
}

@inproceedings{luengo2005,
  title = {Automatic {{Emotion Recognition}} Using {{Prosodic Parameters}}},
  abstract = {This paper presents the experiments made to automatically identify emotion in an emotional speech database for Basque. Three different classifiers have been built: one using spectral features and GMM, other with prosodic features and SVM and the last one with prosodic features and GMM. 86 prosodic features were calculated and then an algorithm to select the most relevant ones was applied. The first classifier gives the best result with a 98.4 \% accuracy when using 512 mixtures, but the classifier built with the best 6 prosodic features achieves an accuracy of 92.3 \% in spite of its simplicity, showing that prosodic information is very useful to identify emotions. 1.},
  booktitle = {In {{Proc}}. of {{INTERSPEECH}}},
  author = {Luengo, Iker and Navas, Eva and Hern\'aez, Inmaculada and S\'anchez, Jon},
  year = {2005},
  pages = {493--496},
  file = {/Users/kennylino/Documents/Articles/LuengoI/2005/Luengo_2005_Automatic Emotion Recognition using Prosodic Parameters.pdf;/Users/kennylino/Zotero/storage/R3G6QY9Y/summary.html}
}

@article{skerry-ryan2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09047},
  primaryClass = {cs, eess},
  title = {Towards {{End}}-to-{{End Prosody Transfer}} for {{Expressive Speech Synthesis}} with {{Tacotron}}},
  abstract = {We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.},
  journal = {arXiv:1803.09047 [cs, eess]},
  author = {{Skerry-Ryan}, R. J. and Battenberg, Eric and Xiao, Ying and Wang, Yuxuan and Stanton, Daisy and Shor, Joel and Weiss, Ron J. and Clark, Rob and Saurous, Rif A.},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/kennylino/Documents/Articles/Skerry-RyanR/2018/Skerry-Ryan_2018_Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with.pdf;/Users/kennylino/Zotero/storage/TVK5RWX4/1803.html}
}

@article{wang2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09017},
  primaryClass = {cs, eess},
  title = {Style {{Tokens}}: {{Unsupervised Style Modeling}}, {{Control}} and {{Transfer}} in {{End}}-to-{{End Speech Synthesis}}},
  shorttitle = {Style {{Tokens}}},
  abstract = {In this work, we propose "global style tokens" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable "labels" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.},
  journal = {arXiv:1803.09017 [cs, eess]},
  author = {Wang, Yuxuan and Stanton, Daisy and Zhang, Yu and {Skerry-Ryan}, R. J. and Battenberg, Eric and Shor, Joel and Xiao, Ying and Ren, Fei and Jia, Ye and Saurous, Rif A.},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/kennylino/Documents/Articles/WangY/2018/Wang_2018_Style Tokens.pdf;/Users/kennylino/Zotero/storage/4ZQQ8KCN/1803.html}
}

@article{wang2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00520},
  primaryClass = {cs},
  title = {Uncovering {{Latent Style Factors}} for {{Expressive Speech Synthesis}}},
  abstract = {Prosodic modeling is a core problem in speech synthesis. The key challenge is producing desirable prosody from textual input containing only phonetic information. In this preliminary study, we introduce the concept of "style tokens" in Tacotron, a recently proposed end-to-end neural speech synthesis model. Using style tokens, we aim to extract independent prosodic styles from training data. We show that without annotation data or an explicit supervision signal, our approach can automatically learn a variety of prosodic variations in a purely data-driven way. Importantly, each style token corresponds to a fixed style factor regardless of the given text sequence. As a result, we can control the prosodic style of synthetic speech in a somewhat predictable and globally consistent way.},
  journal = {arXiv:1711.00520 [cs]},
  author = {Wang, Yuxuan and {Skerry-Ryan}, R. J. and Xiao, Ying and Stanton, Daisy and Shor, Joel and Battenberg, Eric and Clark, Rob and Saurous, Rif A.},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound},
  file = {/Users/kennylino/Documents/Articles/WangY/2017/Wang_2017_Uncovering Latent Style Factors for Expressive Speech Synthesis.pdf;/Users/kennylino/Zotero/storage/ZIL9IE6Y/1711.html}
}

@article{wang2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.10135},
  primaryClass = {cs},
  title = {Tacotron: {{Towards End}}-to-{{End Speech Synthesis}}},
  shorttitle = {Tacotron},
  abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given $<$text, audio$>$ pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
  journal = {arXiv:1703.10135 [cs]},
  author = {Wang, Yuxuan and {Skerry-Ryan}, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Sound},
  file = {/Users/kennylino/Documents/Articles/WangY/2017/Wang_2017_Tacotron.pdf;/Users/kennylino/Zotero/storage/UT9FA57I/1703.html}
}

@article{arik2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.06006},
  primaryClass = {cs, eess},
  title = {Neural {{Voice Cloning}} with a {{Few Samples}}},
  abstract = {Voice cloning is a highly desired feature for personalized speech interfaces. Neural network based speech synthesis has been shown to generate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio samples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios and to be used with a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios. While speaker adaptation can achieve better naturalness and similarity, the cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment.},
  journal = {arXiv:1802.06006 [cs, eess]},
  author = {Arik, Sercan O. and Chen, Jitong and Peng, Kainan and Ping, Wei and Zhou, Yanqi},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/kennylino/Documents/Articles/ArikS/2018/Arik_2018_Neural Voice Cloning with a Few Samples2.pdf;/Users/kennylino/Zotero/storage/XMMJPJG3/1802.html}
}

@article{nguyen2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.01809},
  title = {High Quality Voice Conversion Using Prosodic and High-Resolution Spectral Features},
  volume = {75},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-015-3039-x},
  abstract = {Voice conversion methods have advanced rapidly over the last decade. Studies have shown that speaker characteristics are captured by spectral feature as well as various prosodic features. Most existing conversion methods focus on the spectral feature as it directly represents the timbre characteristics, while some conversion methods have focused only on the prosodic feature represented by the fundamental frequency. In this paper, a comprehensive framework using deep neural networks to convert both timbre and prosodic features is proposed. The timbre feature is represented by a high-resolution spectral feature. The prosodic features include F0, intensity and duration. It is well known that DNN is useful as a tool to model high-dimensional features. In this work, we show that DNN initialized by our proposed autoencoder pretraining yields good quality DNN conversion models. This pretraining is tailor-made for voice conversion and leverages on autoencoder to capture the generic spectral shape of source speech. Additionally, our framework uses segmental DNN models to capture the evolution of the prosodic features over time. To reconstruct the converted speech, the spectral feature produced by the DNN model is combined with the three prosodic features produced by the DNN segmental models. Our experimental results show that the application of both prosodic and high-resolution spectral features leads to quality converted speech as measured by objective evaluation and subjective listening tests.},
  number = {9},
  journal = {Multimedia Tools and Applications},
  author = {Nguyen, Hy Quy and Lee, Siu Wa and Tian, Xiaohai and Dong, Minghui and Chng, Eng Siong},
  month = may,
  year = {2016},
  keywords = {Computer Science - Sound},
  pages = {5265-5285},
  file = {/Users/kennylino/Documents/Articles/NguyenH/2016/Nguyen_2016_High quality voice conversion using prosodic and high-resolution spectral.pdf;/Users/kennylino/Zotero/storage/E4MU3IYC/1512.html}
}

@article{chorowski2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.08363},
  primaryClass = {cs, eess, stat},
  title = {On {{Using Backpropagation}} for {{Speech Texture Generation}} and {{Voice Conversion}}},
  abstract = {Inspired by recent work on neural network image generation which rely on backpropagation towards the network inputs, we present a proof-of-concept system for speech texture synthesis and voice conversion based on two mechanisms: approximate inversion of the representation learned by a speech recognition neural network, and on matching statistics of neuron activations between different source and target utterances. Similar to image texture synthesis and neural style transfer, the system works by optimizing a cost function with respect to the input waveform samples. To this end we use a differentiable mel-filterbank feature extraction pipeline and train a convolutional CTC speech recognition network. Our system is able to extract speaker characteristics from very limited amounts of target speaker data, as little as a few seconds, and can be used to generate realistic speech babble or reconstruct an utterance in a different voice.},
  journal = {arXiv:1712.08363 [cs, eess, stat]},
  author = {Chorowski, Jan and Weiss, Ron J. and Saurous, Rif A. and Bengio, Samy},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/kennylino/Documents/Articles/ChorowskiJ/2017/Chorowski_2017_On Using Backpropagation for Speech Texture Generation and Voice Conversion.pdf;/Users/kennylino/Zotero/storage/HYAFTY3Z/1712.html}
}

@article{mohammadi2017,
  title = {An Overview of Voice Conversion Systems},
  volume = {88},
  issn = {01676393},
  doi = {10.1016/j.specom.2017.01.008},
  abstract = {Voice transformation (VT) aims to change one or more aspects of a speech signal while preserving linguistic information. A subset of VT, Voice conversion (VC) specifically aims to change a source speaker's speech in such a way that the generated output is perceived as a sentence uttered by a target speaker. Despite many years of research, VC systems still exhibit deficiencies in accurately mimicking a target speaker spectrally and prosodically, and simultaneously maintaining high speech quality. In this work we provide an overview of real-world applications, extensively study existing systems proposed in the literature, and discuss remaining challenges.},
  language = {en},
  journal = {Speech Communication},
  author = {Mohammadi, Seyed Hamidreza and Kain, Alexander},
  month = apr,
  year = {2017},
  keywords = {read},
  pages = {65-82},
  file = {/Users/kennylino/Zotero/storage/TXB6AX59/Mohammadi and Kain - 2017 - An overview of voice conversion systems.pdf}
}

@inproceedings{luo2016,
  title = {Emotional Voice Conversion Using Deep Neural Networks with {{MCC}} and {{F0}} Features},
  doi = {10.1109/ICIS.2016.7550889},
  abstract = {An artificial neural network is one of the most important models for training features in a voice conversion task. Typically, Neural Networks (NNs) are not effective in processing low-dimensional F0 features, thus this causes that the performance of those methods based on neural networks for training Mel Cepstral Coefficients (MCC) are not outstanding. However, F0 can robustly represent various prosody signals (e.g., emotional prosody). In this study, we propose an effective method based on the NNs to train the normalized-segment-F0 features (NSF0) for emotional prosody conversion. Meanwhile, the proposed method adopts deep belief networks (DBNs) to train spectrum features for voice conversion. By using these approaches, the proposed method can change the spectrum and the prosody for the emotional voice at the same time. Moreover, the experimental results show that the proposed method outperforms other state-of-the-art methods for voice emotional conversion.},
  booktitle = {2016 {{IEEE}}/{{ACIS}} 15th {{International Conference}} on {{Computer}} and {{Information Science}} ({{ICIS}})},
  author = {Luo, Z. and Takiguchi, T. and Ariki, Y.},
  month = jun,
  year = {2016},
  keywords = {artificial neural network,Artificial neural networks,belief networks,cepstral analysis,DBN,deep belief network,deep neural network,emotional voice conversion,F0 feature,feature extraction,Feature extraction,MCC,mel cepstral coefficient,Mel frequency cepstral coefficient,neural nets,NN,prosody signal,spectrum feature training,Speech,speech processing,Training,Transforms},
  pages = {1-5},
  file = {/Users/kennylino/Zotero/storage/BNILSKCY/7550889.html}
}

@inproceedings{aryal2014,
  title = {Can Voice Conversion Be Used to Reduce Non-Native Accents?},
  isbn = {978-1-4799-2893-4},
  doi = {10.1109/ICASSP.2014.6855134},
  abstract = {Voice-conversion (VC) techniques aim to transform utterances from a source speaker to sound as if they had been produced by a target speaker. This includes not only organic properties (i.e., voice quality) but also linguistic cues (i.e., regional accents) of the target speaker. For this reason, VC is generally ill-suited for accent-conversion (AC) purposes, where the goal is to capture the voice quality of the target speaker but the regional accent of the source speaker. In this paper, we propose a modification of the conventional training process for VC that allows it to perform as an AC transform. The approach consists of pairing source and target vectors based not on their ordering within a parallel corpus, as is commonly done in VC, but based on their linguistic similarity. We validate the AC approach on a corpus containing native-accented and Spanish-accented utterances, and compare it against conventional VC through a series of perceptual listening tests. We also analyze the extent to which phonological differences between the two languages (Spanish and American English) help predict the relative performance of the two methods.},
  language = {en},
  publisher = {{IEEE}},
  author = {Aryal, Sandesh and {Gutierrez-Osuna}, Ricardo},
  month = may,
  year = {2014},
  pages = {7879-7883},
  file = {/Users/kennylino/Zotero/storage/LZS5YPC9/Aryal and Gutierrez-Osuna - 2014 - Can voice conversion be used to reduce non-native .pdf}
}

@article{felps2010,
  title = {Developing Objective Measures of Foreign-Accent Conversion},
  abstract = {Abstract\textemdash{}Various methods have recently appeared to transform foreign-accented speech into its native-accented counterpart. Eval-uation of these accent conversion methods requires extensive lis-tening tests across a number of perceptual dimensions. This article presents three objective measures that may be used to assess the acoustic quality, degree of foreign accent, and speaker identity of accent-converted utterances. Accent conversion generates novel ut-terances: those of a foreign speaker with a native accent. There-fore, the acoustic quality in accent conversion cannot be evaluated with conventional measures of spectral distortion, which assume that a clean recording of the speech signal is available for compar-ison. Here we evaluate a single-ended measure of speech quality, ITU-T recommendation P.563 for narrow-band telephony. We also propose a measure of foreign accent that exploits a weakness of automatic speech recognizers: their sensitivity to foreign accents. Namely, we use phoneme-level match scores given by the HTK rec-ognizer trained on a large number of English American speakers to obtain a measure of native accent. Finally, we propose a measure of speaker identity that projects acoustic vectors (e.g., Mel cep-stral, F0) onto the linear discriminant that maximizes separability for a given pair of source and target speakers. The three measures are evaluated on a corpus of accent-converted utterances that had been previously rated through perceptual tests. Our results show that the three measures have a high degree of correlation with their corresponding subjective ratings, suggesting that they may be used to accelerate the development of foreign-accent conversion tools. Applications of these measures in the context of computer assisted pronunciation training and voice conversion are also discussed. Index Terms\textemdash{}Accent conversion, foreign accent recognition, speaker recognition, voice conversion. I.},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  author = {Felps, Daniel and {Gutierrez-Osuna}, Ricardo},
  year = {2010},
  pages = {1030--1040},
  file = {/Users/kennylino/Documents/Articles/FelpsD/2010/Felps_2010_Developing objective measures of foreign-accent conversion.pdf;/Users/kennylino/Zotero/storage/LXQSAWW2/summary.html}
}

@article{aryal2010,
  title = {Foreign {{Accent Conversion Through Voice Morphing}}},
  abstract = {We present a voice morphing strategy that can be used to generate a continuum of accent transformations between a foreign speaker and a native speaker. The approach performs a cepstral decomposition of speech into spectral slope and spectral detail. Accent conversions are then generated by combining the spectral slope of the foreign speaker with a morph of the spectral detail of the native speaker. Spectral morphing is achieved by representing the spectral detail through pulse density modulation and averaging pulses in a pair-wise fashion. The technique is validated on parallel recordings from two ARCTIC speakers using both objective and subjective measures of acoustic quality, speaker identity and foreign accent.},
  language = {en},
  author = {Aryal, Sandesh and Felps, Daniel and {Gutierrez-Osuna}, Ricardo},
  year = {2010},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/JCYBS2C4/Aryal et al. - Foreign Accent Conversion Through Voice Morphing.pdf}
}

@inproceedings{aryal2014a,
  title = {Accent Conversion through Cross-Speaker Articulatory Synthesis},
  isbn = {978-1-4799-2893-4},
  doi = {10.1109/ICASSP.2014.6855097},
  abstract = {Accent conversion (AC) seeks to transform second-language (L2) utterances to appear as if produced with a native (L1) accent. In the acoustic domain, AC is difficult due to the complex interaction between linguistic content and voice quality. Alternatively, AC can be performed in the articulatory domain by building a mapping from L2 articulators to L2 acoustics, and then driving the model with L1 articulators. However, collecting articulatory data for each L2 learner is impractical. Here we propose an approach that avoids this expensive step. Our method builds a cross-speaker forward mapping (CSFM) to generate L2 acoustic observations directly from L1 articulatory trajectories. We evaluated the CSFM against a baseline articulatory synthesizer trained with L2 articulators. Subjective listening tests show that both methods perform comparably in terms of accent reduction and ability to preserve the voice quality of the L2 speaker, with only a small impact in acoustic quality.},
  language = {en},
  publisher = {{IEEE}},
  author = {Aryal, Sandesh and {Gutierrez-Osuna}, Ricardo},
  month = may,
  year = {2014},
  pages = {7694-7698},
  file = {/Users/kennylino/Zotero/storage/LF4886GR/background.md;/Users/kennylino/Zotero/storage/TDJ33NSU/Aryal and Gutierrez-Osuna - 2014 - Accent conversion through cross-speaker articulato.pdf}
}

@inproceedings{toda2016,
  title = {The {{Voice Conversion Challenge}} 2016},
  doi = {10.21437/Interspeech.2016-1066},
  abstract = {This paper describes the Voice Conversion Challenge 2016 devised by the authors to better understand different voice conversion (VC) techniques by comparing their performance on a common dataset. The task of the challenge was speaker conversion, i.e., to transform the voice identity of a source speaker into that of a target speaker while preserving the linguistic content. Using a common dataset consisting of 162 utterances for training and 54 utterances for evaluation from each of 5 source and 5 target speakers, 17 groups working in VC around the world developed their own VC systems for every combination of the source and target speakers, i.e., 25 systems in total, and generated voice samples converted by the developed systems. These samples were evaluated in terms of target speaker similarity and naturalness by 200 listeners in a controlled environment. This paper summarizes the design of the challenge, its result, and a future plan to share views about unsolved problems and challenges faced by the current VC techniques.},
  language = {en},
  author = {Toda, Tomoki and Chen, Ling-Hui and Saito, Daisuke and Villavicencio, Fernando and Wester, Mirjam and Wu, Zhizheng and Yamagishi, Junichi},
  month = sep,
  year = {2016},
  pages = {1632-1636},
  file = {/Users/kennylino/Zotero/storage/PFYB59NR/Toda et al. - 2016 - The Voice Conversion Challenge 2016.pdf}
}

@article{luo2017,
  title = {Emotional Voice Conversion Using Neural Networks with Arbitrary Scales {{F0}} Based on Wavelet Transform},
  volume = {2017},
  issn = {1687-4722},
  doi = {10.1186/s13636-017-0116-2},
  abstract = {An artificial neural network is an important model for training features of voice conversion (VC) tasks. Typically, neural networks (NNs) are very effective in processing nonlinear features, such as Mel Cepstral Coefficients (MCC), which represent the spectrum features. However, a simple representation of fundamental frequency (F0) is not enough for NNs to deal with emotional voice VC. This is because the time sequence of F0 for an emotional voice changes drastically. Therefore, in our previous method, we used the continuous wavelet transform (CWT) to decompose F0 into 30 discrete scales, each separated by one third of an octave, which can be trained by NNs for prosody modeling in emotional VC. In this study, we propose the arbitrary scales CWT (AS-CWT) method to systematically capture F0 features of different temporal scales, which can represent different prosodic levels ranging from micro-prosody to sentence levels. Meanwhile, the proposed method uses deep belief networks (DBNs) to pre-train the NNs that then convert spectral features. By utilizing these approaches, the proposed method can change the spectrum and the F0 for an emotional voice simultaneously as well as outperform other state-of-the-art methods in terms of emotional VC.},
  language = {en},
  number = {1},
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  author = {Luo, Zhaojie and Chen, Jinhui and Takiguchi, Tetsuya and Ariki, Yasuo},
  month = dec,
  year = {2017},
  pages = {18},
  file = {/Users/kennylino/Documents/Articles/LuoZ/2017/Luo_2017_Emotional voice conversion using neural networks with arbitrary scales F0 based.pdf;/Users/kennylino/Zotero/storage/RR4JHYB6/s13636-017-0116-2.html}
}

@article{lorenzo-trueba2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.04262},
  primaryClass = {cs, eess, stat},
  title = {The {{Voice Conversion Challenge}} 2018: {{Promoting Development}} of {{Parallel}} and {{Nonparallel Methods}}},
  shorttitle = {The {{Voice Conversion Challenge}} 2018},
  abstract = {We present the Voice Conversion Challenge 2018, designed as a follow up to the 2016 edition with the aim of providing a common framework for evaluating and comparing different state-of-the-art voice conversion (VC) systems. The objective of the challenge was to perform speaker conversion (i.e. transform the vocal identity) of a source speaker to a target speaker while maintaining linguistic information. As an update to the previous challenge, we considered both parallel and non-parallel data to form the Hub and Spoke tasks, respectively. A total of 23 teams from around the world submitted their systems, 11 of them additionally participated in the optional Spoke task. A large-scale crowdsourced perceptual evaluation was then carried out to rate the submitted converted speech in terms of naturalness and similarity to the target speaker identity. In this paper, we present a brief summary of the state-of-the-art techniques for VC, followed by a detailed explanation of the challenge tasks and the results that were obtained.},
  journal = {arXiv:1804.04262 [cs, eess, stat]},
  author = {{Lorenzo-Trueba}, Jaime and Yamagishi, Junichi and Toda, Tomoki and Saito, Daisuke and Villavicencio, Fernando and Kinnunen, Tomi and Ling, Zhenhua},
  month = apr,
  year = {2018},
  keywords = {Computer Science - Computation and Language,read,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/kennylino/Documents/Articles/Lorenzo-TruebaJ/2018/Lorenzo-Trueba_2018_The Voice Conversion Challenge 2018.pdf;/Users/kennylino/Zotero/storage/V4PHRZST/1804.html}
}

@article{azarov2013,
  title = {Real-{{Time Voice Conversion Using Artificial Neural Networks}} with {{Rectified Linear Units}}},
  abstract = {This paper presents an approach to parametric voice conversion that can be used in real-time entertainment applications. The approach is based on spectral mapping using an artificial neural network (ANN) with rectified linear units (ReLU). To overcome the oversmoothing problem a special network configuration is proposed that utilizes temporal states of the speaker. The speech is represented using the harmonic plus noise model. The parameters of the model are estimated using instantaneous harmonic parameters. Using objective and subjective measures the proposed voice conversion technique is compared to the main alternative approaches.},
  language = {en},
  author = {Azarov, Elias and Vashkevich, Maxim and Likhachov, Denis and Petrovsky, Alexander},
  year = {2013},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/J8ZF22C2/Azarov et al. - Real-Time Voice Conversion Using Artificial Neural.pdf}
}

@inproceedings{sun2015,
  title = {Voice Conversion Using Deep {{Bidirectional Long Short}}-{{Term Memory}} Based {{Recurrent Neural Networks}}},
  doi = {10.1109/ICASSP.2015.7178896},
  abstract = {This paper investigates the use of Deep Bidirectional Long Short-Term Memory based Recurrent Neural Networks (DBLSTM-RNNs) for voice conversion. Temporal correlations across speech frames are not directly modeled in frame-based methods using conventional Deep Neural Networks (DNNs), which results in a limited quality of the converted speech. To improve the naturalness and continuity of the speech output in voice conversion, we propose a sequence-based conversion method using DBLSTM-RNNs to model not only the frame-wised relationship between the source and the target voice, but also the long-range context-dependencies in the acoustic trajectory. Experiments show that DBLSTM-RNNs outperform DNNs where Mean Opinion Scores are 3.2 and 2.3 respectively. Also, DBLSTM-RNNs without dynamic features have better performance than DNNs with dynamic features.},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sun, L. and Kang, S. and Li, K. and Meng, H.},
  month = apr,
  year = {2015},
  keywords = {Speech,speech processing,Training,acoustic trajectory,Acoustics,bidirectional long short-term memory,Context,DBLSTM-RNNs,deep bidirectional long short-term memory based recurrent neural networks,DNNs,dynamic features,frame-based methods,Logic gates,long-range context-dependency,mean opinion scores,recurrent neural nets,recurrent neural networks,Recurrent neural networks,sequence-based conversion method,speech frames,temporal correlations,voice conversion},
  pages = {4869-4873},
  file = {/Users/kennylino/Documents/Articles/SunL/2015/Sun_2015_Voice conversion using deep Bidirectional Long Short-Term Memory based.pdf;/Users/kennylino/Zotero/storage/IBVUDJUQ/7178896.html}
}

@inproceedings{desai2009,
  title = {Voice Conversion Using {{Artificial Neural Networks}}},
  doi = {10.1109/ICASSP.2009.4960478},
  abstract = {In this paper, we propose to use artificial neural networks (ANN) for voice conversion. We have exploited the mapping abilities of ANN to perform mapping of spectral features of a source speaker to that of a target speaker. A comparative study of voice conversion using ANN and the state-of-the-art Gaussian mixture model (GMM) is conducted. The results of voice conversion evaluated using subjective and objective measures confirm that ANNs perform better transformation than GMMs and the quality of the transformed speech is intelligible and has the characteristics of the target speaker.},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Desai, S. and Raghavendra, E. V. and Yegnanarayana, B. and Black, A. W. and Prahallad, K.},
  month = apr,
  year = {2009},
  keywords = {Artificial neural networks,neural nets,speech processing,voice conversion,ANN,artificial neural networks,Artificial Neural Networks,Books,Data mining,Databases,Filters,Frequency estimation,Gaussian mixture model,Gaussian Mixture Model,Gaussian processes,Loudspeakers,source speaker,spectral analysis,spectral feature mapping,speech intelligibility,Speech synthesis,target speaker,Training data,Vectors,Voice conversion},
  pages = {3893-3896},
  file = {/Users/kennylino/Documents/Articles/DesaiS/2009/Desai_2009_Voice conversion using Artificial Neural Networks.pdf;/Users/kennylino/Zotero/storage/MWIINPND/4960478.html}
}

@article{chen2014,
  title = {Voice {{Conversion Using Deep Neural Networks}} with {{Layer}}-Wise {{Generative Training}}},
  volume = {22},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2014.2353991},
  abstract = {This paper presents a new spectral envelope conversion method using deep neural networks (DNNs). The conventional joint density Gaussian mixture model (JDGMM) based spectral conversion methods perform stably and effectively. However, the speech generated by these methods suffer severe quality degradation due to the following two factors: 1) inadequacy of JDGMM in modeling the distribution of spectral features as well as the non-linear mapping relationship between the source and target speakers, 2) spectral detail loss caused by the use of high-level spectral features such as mel-cepstra. Previously, we have proposed to use the mixture of restricted Boltzmann machines (MoRBM) and the mixture of Gaussian bidirectional associative memories (MoGBAM) to cope with these problems. In this paper, we propose to use a DNN to construct a global non-linear mapping relationship between the spectral envelopes of two speakers. The proposed DNN is generatively trained by cascading two RBMs, which model the distributions of spectral envelopes of source and target speakers respectively, using a Bernoulli BAM (BBAM). Therefore, the proposed training method takes the advantage of the strong modeling ability of RBMs in modeling the distribution of spectral envelopes and the superiority of BAMs in deriving the conditional distributions for conversion. Careful comparisons and analysis among the proposed method and some conventional methods are presented in this paper. The subjective results show that the proposed method can significantly improve the performance in terms of both similarity and naturalness compared to conventional methods.},
  number = {12},
  journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  author = {Chen, Ling-Hui and Ling, Zhen-Hua and Liu, Li-Juan and Dai, Li-Rong},
  month = dec,
  year = {2014},
  keywords = {deep neural network,voice conversion,Gaussian mixture model,bidirectional associative memory,restricted Boltzmann machine,spectral envelope conversion},
  pages = {1859--1872},
  file = {/Users/kennylino/Documents/Articles/ChenL/2014/Chen_2014_Voice Conversion Using Deep Neural Networks with Layer-wise Generative Training.pdf}
}

@inproceedings{mohammadi2014,
  title = {Voice Conversion Using Deep Neural Networks with Speaker-Independent Pre-Training},
  doi = {10.1109/SLT.2014.7078543},
  abstract = {In this study, we trained a deep autoencoder to build compact representations of short-term spectra of multiple speakers. Using this compact representation as mapping features, we then trained an artificial neural network to predict target voice features from source voice features. Finally, we constructed a deep neural network from the trained deep autoencoder and artificial neural network weights, which were then fine-tuned using back-propagation. We compared the proposed method to existing methods using Gaussian mixture models and frame-selection. We evaluated the methods objectively, and also conducted perceptual experiments to measure both the conversion accuracy and speech quality of selected systems. The results showed that, for 70 training sentences, frame-selection performed best, regarding both accuracy and quality. When using only two training sentences, the pre-trained deep neural network performed best, regarding both accuracy and quality.},
  booktitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Mohammadi, S. H. and Kain, A.},
  month = dec,
  year = {2014},
  keywords = {Artificial neural networks,deep neural network,neural nets,Speech,Training,voice conversion,Gaussian processes,Accuracy,artificial neural network training,autoencoder,back-propagation,backpropagation,Conferences,conversion accuracy,deep autoencoder,frame-selection,Gaussian mixture models,mapping features,mixture models,pre-training,signal representation,source voice features,speaker recognition,speaker short-term spectra compact representation,speaker-independent pretraining,Speech processing,speech quality,target voice feature prediction},
  pages = {19-23},
  file = {/Users/kennylino/Documents/Articles/MohammadiS/2014/Mohammadi_2014_Voice conversion using deep neural networks with speaker-independent.pdf;/Users/kennylino/Zotero/storage/VT7UYVHD/7078543.html}
}

@incollection{bongaerts1995,
  title = {Can {{Late Starters Attain}} a {{Native Accent}} in a {{Foreign Language}}? {{A Test}} of the {{Critical Period Hypothesis}}},
  author = {Bongaerts, Theo and Planken, Brigitte and Schils, Erik},
  year = {1995}
}

@article{wu2010,
  title = {Text-{{Independent F0 Transformation}} with {{Non}}-{{Parallel Data}} for {{Voice Conversion}}},
  abstract = {In voice conversion, frame-level mean and variance normalization is typically used for fundamental frequency (F0) transformation, which is text-independent and requires no parallel training data. Some advanced methods transform pitch contours instead, but require either parallel training data or syllabic annotations. We propose a method which retains the simplicity and text-independence of the frame-level conversion while yielding high-quality conversion. We achieve these goals by (1) introducing a text-independent tri-frame alignment method, (2) including delta features of F0 into Gaussian mixture model (GMM) conversion and (3) reducing the well-known GMM oversmoothing effect by F0 histogram equalization. Our objective and subjective experiments on the CMU Arctic corpus indicate improvements over both the mean/variance normalization and the baseline GMM conversion.},
  language = {en},
  author = {Wu, Zhi-Zheng and Kinnunen, Tomi and Chng, Eng Siong and Li, Haizhou},
  year = {2010},
  pages = {4},
  file = {/Users/kennylino/Zotero/storage/AKVLGYDG/Wu et al. - Text-Independent F0 Transformation with Non-Parall.pdf}
}

@incollection{chun2008,
  title = {Technologies for Prosody in Context: {{Past}} and Future of {{L2}} Research and Practice},
  author = {Chun, Dorothy M. and Hardison, Debra M. and Pennington, Martha C.},
  year = {2008},
  pages = {323-346}
}

@article{hardison2005,
  title = {Contextualized {{Computer}}-Based {{L2 Prosody Training}}: {{Evaluating}} the {{Effects}} of {{Discourse Context}} and {{Video Input}}},
  volume = {22},
  abstract = {Two types of contextualized input in prosody training were investigated for 28 advanced L2 speakers of English (L1 Chinese). Their oral presentations provided training materials. Native-speakers (NSs) of English provided global prosody ratings, and participants completed questionnaires on perceived training effectiveness. Two groups received training input using Anvil, a web-based annotation tool integrating the video of a speech event with visual displays of the pitch contour, and practiced with Real-Time Pitch (RTP) in Computerized Speech Lab including feedback from a NS. Two groups used only RTP to view their pitch contours and practiced with the same feedback. Within these pairs, one group received discourse-level input and the other individual sentences. Each group served as its own control in a time-series design. All had comparable levels of performance prior to training. Results indicated that although all groups improved as a result of training, discourse-level input produced better transfer to novel natural discourse. The presence of video was more helpful with discourselevel input than with individual sentences. Speech samples collected 1 week after training revealed sustained improvement. Questionnaire results support the use of computer-based tools and authentic speech samples. Findings strongly suggest that meaningful contextualized input is valuable in prosody training when the measurement is at the level of extended connected speech typical of natural discourse.},
  language = {en},
  number = {2},
  journal = {CALICO Journal},
  author = {Hardison, Debra M.},
  year = {2005},
  pages = {16},
  file = {/Users/kennylino/Zotero/storage/DEL2DFGZ/HARDISON - Contextualized Computer-based L2 Prosody Training.pdf}
}

@inproceedings{wu2016,
  title = {On the Use of {{I}}-Vectors and Average Voice Model for Voice Conversion without Parallel Data},
  doi = {10.1109/APSIPA.2016.7820901},
  abstract = {Recently, deep and/or recurrent neural networks (DNNs/RNNs) have been employed for voice conversion, and have significantly improved the performance of converted speech. However, DNNs/RNNs generally require a large amount of parallel training data (e.g., hundreds of utterances) from source and target speakers. It is expensive to collect such a large amount of data, and impossible in some applications, such as cross-lingual conversion. To solve this problem, we propose to use average voice model and i-vectors for long short-term memory (LSTM) based voice conversion, which does not require parallel data from source and target speakers. The average voice model is trained using other speakers' data, and the i-vectors, a compact vector representing the identities of source and target speakers, are extracted independently. Subjective evaluation has confirmed the effectiveness of the proposed approach.},
  booktitle = {2016 {{Asia}}-{{Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA}})},
  author = {Wu, J. and Wu, Z. and Xie, L.},
  month = dec,
  year = {2016},
  keywords = {Feature extraction,Speech,speech processing,Training,DNNs,recurrent neural nets,recurrent neural networks,voice conversion,Data mining,Speech processing,Adaptation models,average voice model,compact vector,Data models,deep neural networks,i-vector,i-vectors,long short-term memory,long short-term memory based voice conversion,LSTM based voice conversion,nonparallel training,RNNs,speech conversion,vectors},
  pages = {1-6},
  file = {/Users/kennylino/Zotero/storage/VI7YKJV6/Wu et al. - 2016 - On the use of I-vectors and average voice model fo.pdf;/Users/kennylino/Zotero/storage/L5BPLJI7/7820901.html}
}

@article{kobayashi2014,
  title = {Statistical {{Singing Voice Conversion}} with {{Direct Waveform Modification}} Based on the {{Spectrum Differential}}},
  abstract = {This paper presents a novel statistical singing voice conversion (SVC) technique with direct waveform modification based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms. SVC makes it possible to convert singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer. However, speech quality of the converted singing voice is significantly degraded compared to that of a natural singing voice due to various factors, such as analysis and modeling errors in the vocoderbased framework. To alleviate this degradation, we propose a statistical conversion process that directly modifies the signal in the waveform domain by estimating the difference in the spectra of the source and target singers' singing voices. The differential spectral feature is directly estimated using a differential Gaussian mixture model (GMM) that is analytically derived from the traditional GMM used as a conversion model in the conventional SVC. The experimental results demonstrate that the proposed method makes it possible to significantly improve speech quality in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional SVC.},
  language = {en},
  author = {Kobayashi, Kazuhiro and Toda, Tomoki and Neubig, Graham and Sakti, Sakriani and Nakamura, Satoshi},
  year = {2014},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/AM8NCYMC/Kobayashi et al. - Statistical Singing Voice Conversion with Direct W.pdf}
}

@article{toda2007,
  title = {Voice {{Conversion Based}} on {{Maximum}}-{{Likelihood Estimation}} of {{Spectral Parameter Trajectory}}},
  volume = {15},
  issn = {1558-7916},
  doi = {10.1109/TASL.2007.907344},
  abstract = {In this paper, we describe a novel spectral conversion method for voice conversion (VC). A Gaussian mixture model (GMM) of the joint probability density of source and target features is employed for performing spectral conversion between speakers. The conventional method converts spectral parameters frame by frame based on the minimum mean square error. Although it is reasonably effective, the deterioration of speech quality is caused by some problems: 1) appropriate spectral movements are not always caused by the frame-based conversion process, and 2) the converted spectra are excessively smoothed by statistical modeling. In order to address those problems, we propose a conversion method based on the maximum-likelihood estimation of a spectral parameter trajectory. Not only static but also dynamic feature statistics are used for realizing the appropriate converted spectrum sequence. Moreover, the oversmoothing effect is alleviated by considering a global variance feature of the converted spectra. Experimental results indicate that the performance of VC can be dramatically improved by the proposed method in view of both speech quality and conversion accuracy for speaker individuality.},
  number = {8},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  author = {Toda, T. and Black, A. W. and Tokuda, K.},
  month = nov,
  year = {2007},
  keywords = {speech processing,voice conversion,Gaussian mixture model,Gaussian processes,Loudspeakers,spectral analysis,Training data,Speech processing,speech quality,Dynamic feature,Gaussian mixture model (GMM),global variance,joint probability density,least mean squares methods,maximum likelihood estimation,Maximum likelihood estimation,maximum-likelihood estimation,maximum-likelihood estimation (MLE),Mean square error methods,minimum mean square error,Natural languages,Parameter estimation,probability,spectral conversion method,spectral parameter trajectory,Speech enhancement,speech synthesis,statistical modeling,Statistics,Virtual colonoscopy,voice conversion (VC)},
  pages = {2222-2235},
  file = {/Users/kennylino/Zotero/storage/FWL43PJA/Toda et al. - 2007 - Voice Conversion Based on Maximum-Likelihood Estim.pdf;/Users/kennylino/Zotero/storage/H8FGCIG7/4317579.html}
}

@inproceedings{kinnunen2017,
  title = {Non-Parallel Voice Conversion Using i-Vector {{PLDA}}: Towards Unifying Speaker Verification and Transformation},
  isbn = {978-1-5090-4117-6},
  shorttitle = {Non-Parallel Voice Conversion Using i-Vector {{PLDA}}},
  doi = {10.1109/ICASSP.2017.7953215},
  abstract = {Text-independent speaker verification (recognizing speakers regardless of content) and non-parallel voice conversion (transforming voice identities without requiring content-matched training utterances) are related problems. We adopt i-vector method to voice conversion. An i-vector is a fixed-dimensional representation of a speech utterance that enables treating voice conversion in utterance domain, as opposed to frame domain. The high dimensionality (800) and small number of training utterances (24) necessitates using prior information of speakers. We adopt probabilistic linear discriminant analysis (PLDA) for voice conversion. The proposed approach requires neither parallel utterances, transcriptions nor time alignment procedures at any stage.},
  language = {en},
  publisher = {{IEEE}},
  author = {Kinnunen, Tomi and Juvela, Lauri and Alku, Paavo and Yamagishi, Junichi},
  month = mar,
  year = {2017},
  pages = {5535-5539},
  file = {/Users/kennylino/Zotero/storage/PJLZMP7A/Kinnunen et al. - 2017 - Non-parallel voice conversion using i-vector PLDA.pdf}
}

@article{aryal2015,
  title = {Articulatory-{{Based Conversion}} of {{Foreign Accents}} with {{Deep Neural Networks}}},
  abstract = {We present an articulatory-based method for real-time accent conversion using deep neural networks (DNN). The approach consists of two steps. First, we train a DNN articulatory synthesizer for the non-native speaker that estimates acoustics from contextualized articulatory gestures. Then we drive the DNN with articulatory gestures from a reference native speaker \textendash{}mapped to the nonnative articulatory space via a Procrustes transform. We evaluate the accent-conversion performance of the DNN through a series of listening tests of intelligibility, voice identity and nonnative accentedness. Compared to a baseline method based on Gaussian mixture models, the DNN accent conversions were found to be 31\% more intelligible, and were perceived more native-like in 68\% of the cases. The DNN also succeeded in preserving the voice identity of the nonnative speaker.},
  language = {en},
  author = {Aryal, Sandesh and {Gutierrez-Osuna}, Ricardo},
  year = {2015},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/36X4R8UA/Aryal and Gutierrez-Osuna - Articulatory-Based Conversion of Foreign Accents w.pdf}
}

@article{fang2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.00425},
  primaryClass = {cs, eess, stat},
  title = {High-Quality Nonparallel Voice Conversion Based on Cycle-Consistent Adversarial Network},
  abstract = {Although voice conversion (VC) algorithms have achieved remarkable success along with the development of machine learning, superior performance is still difficult to achieve when using nonparallel data. In this paper, we propose using a cycle-consistent adversarial network (CycleGAN) for nonparallel data-based VC training. A CycleGAN is a generative adversarial network (GAN) originally developed for unpaired image-to-image translation. A subjective evaluation of inter-gender conversion demonstrated that the proposed method significantly outperformed a method based on the Merlin open source neural network speech synthesis system (a parallel VC system adapted for our setup) and a GAN-based parallel VC system. This is the first research to show that the performance of a nonparallel VC method can exceed that of state-of-the-art parallel VC methods.},
  journal = {arXiv:1804.00425 [cs, eess, stat]},
  author = {Fang, Fuming and Yamagishi, Junichi and Echizen, Isao and {Lorenzo-Trueba}, Jaime},
  month = apr,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/kennylino/Documents/Articles/FangF/2018/Fang_2018_High-quality nonparallel voice conversion based on cycle-consistent adversarial.pdf;/Users/kennylino/Zotero/storage/Z26IC6FW/1804.html}
}

@article{demarco2013,
  title = {Native {{Accent Classification}} via {{I}}-{{Vectors}} and {{Speaker Compensation Fusion}}},
  abstract = {We present a comprehensive analysis of the use of I-vector based classifiers for the classification of unlabelled acoustic data as native British accents. We demonstrate the different behaviours of various popular dimensionality reduction techniques that have been previously used in problems such as speaker and language classification. Our results show that a fusion of I-vector based systems gives state-of-the-art performance for unlabelled classification of British accent speech data, reaching $\sim$81\% accuracy.},
  language = {en},
  author = {DeMarco, Andrea and Cox, Stephen J},
  year = {2013},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/MRTN93E5/DeMarco and Cox - Native Accent Classification via I-Vectors and Spe.pdf}
}

@article{darcy2004,
  title = {The {{Accents}} of the {{British Isles}} ({{ABI}}), Corpus},
  language = {en},
  author = {D'Arcy, Shona M and Russell, Martin J and Browning, Sue R and Tomlinson, Mike J},
  year = {2004},
  pages = {6},
  file = {/Users/kennylino/Zotero/storage/GWQJ3ZU7/D’Arcy et al. - 2004 - The Accents of the British Isles (ABI), corpus.pdf}
}

@article{dehak2011,
  title = {Front-{{End Factor Analysis}} for {{Speaker Verification}}},
  volume = {19},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2010.2064307},
  abstract = {This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12\% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4\% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.},
  language = {en},
  number = {4},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  author = {Dehak, Najim and Kenny, Patrick J and Dehak, R\'eda and Dumouchel, Pierre and Ouellet, Pierre},
  month = may,
  year = {2011},
  pages = {788-798},
  file = {/Users/kennylino/Zotero/storage/GESBYG4P/Dehak et al. - 2011 - Front-End Factor Analysis for Speaker Verification.pdf}
}

@book{jurafsky2009,
  address = {Upper Saddle River, N.J.},
  edition = {2nd ed.},
  series = {Prentice Hall series in artificial intelligence},
  title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  isbn = {978-0-13-187321-6},
  lccn = {P98 .J87 2008, P98 .J87 2009},
  shorttitle = {Speech and Language Processing},
  abstract = {Introduction -- Regular Expressions and Automata -- Words \& Transducers -- N-grams -- Part-of-Speech Tagging -- Hidden Markov and Maximum Entropy Models -- Phonetics -- Speech Synthesis -- Automatic Speech Recognition -- Speech Recognition: Advanced Topics -- Computational Phonology -- Formal Grammars of English -- Syntactic Parsing -- Statistical Parsing -- Features and Unification -- Language and Complexity -- The Representation of Meaning -- Computational Semantics -- Lexical Semantics -- Computational Lexical Semantics -- Computational Discourse -- Information Extraction -- Question Answering and Summarization -- Dialogue and Conversational Agents -- Machine Translation., An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology - at all levels and with all modern technologies - this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing. -- Book Description from Website., Includes bibliographical references (pages 909-958) and indexes.},
  language = {eng},
  publisher = {{Pearson Prentice Hall}},
  author = {Jurafsky, Dan},
  collaborator = {Martin, James H.},
  year = {2009},
  keywords = {Automatic speech recognition.,Computational linguistics.,Linguistics.,Natural Language Processing.,Speech Recognition Software.}
}

@inproceedings{sundermann2006,
  title = {Text-{{Independent Voice Conversion Based}} on {{Unit Selection}}},
  volume = {1},
  isbn = {978-1-4244-0469-8},
  doi = {10.1109/ICASSP.2006.1659962},
  abstract = {So far, most of the voice conversion training procedures are text-dependent, i.e., they are based on parallel training utterances of source and target speaker. Since several applications (e.g. speech-to-speech translation or dubbing) require textindependent training, over the last two years, training techniques that use non-parallel data were proposed. In this paper, we present a new approach that applies unit selection to find corresponding time frames in source and target speech. By means of a subjective experiment it is shown that this technique achieves the same performance as the conventional text-dependent training.},
  language = {en},
  publisher = {{IEEE}},
  author = {Sundermann, D. and Hoge, H. and Bonafonte, A. and Ney, H. and Black, A. and Narayanan, S.},
  year = {2006},
  pages = {I-81-I-84},
  file = {/Users/kennylino/Zotero/storage/8VK2NEUH/Sundermann et al. - 2006 - Text-Independent Voice Conversion Based on Unit Se.pdf}
}

@article{zhao2018a,
  title = {Accent Conversion Using Phonetic Posteriorgrams},
  abstract = {Accent conversion (AC) aims to transform non-native speech to sound as if the speaker had a native accent. This can be achieved by mapping source spectra from a native speaker into the acoustic space of the non-native speaker. In prior work, we proposed an AC approach that matches frames between the two speakers based on their acoustic similarity after compensating for differences in vocal tract length. In this paper, we propose an approach that matches frames between the two speakers based on their phonetic (rather than acoustic) similarity. Namely, we map frames from the two speakers into a phonetic posteriorgram using speakerindependent acoustic models trained on native speech. We evaluate the proposed algorithm on a corpus containing multiple native and non-native speakers. Compared to the previous AC algorithm, the proposed algorithm improves the ratings of acoustic quality (20\% increase in mean opinion score) and native accent (69\% preference) while retaining the voice identity of the non-native speaker.},
  language = {en},
  author = {Zhao, Guanlong and Sonsaat, Sinem and Levis, John and {Chukharev-Hudilainen}, Evgeny and {Gutierrez-Osuna}, Ricardo},
  year = {2018},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/N6W86WMM/Zhao et al. - ACCENT CONVERSION USING PHONETIC POSTERIORGRAMS.pdf}
}

@article{piske2001,
  title = {Factors Affecting Degree of Foreign Accent in an {{L2}}: A Review},
  volume = {29},
  issn = {00954470},
  shorttitle = {Factors Affecting Degree of Foreign Accent in an {{L2}}},
  doi = {10.1006/jpho.2001.0134},
  language = {en},
  number = {2},
  journal = {Journal of Phonetics},
  author = {Piske, Thorsten and MacKay, Ian R.A. and Flege, James E.},
  month = apr,
  year = {2001},
  pages = {191-215},
  file = {/Users/kennylino/Zotero/storage/SB4WLYEJ/Piske et al. - 2001 - Factors affecting degree of foreign accent in an L.pdf}
}

@article{morise2016,
  title = {{{WORLD}}: {{A Vocoder}}-{{Based High}}-{{Quality Speech Synthesis System}} for {{Real}}-{{Time Applications}}},
  volume = {E99.D},
  issn = {0916-8532, 1745-1361},
  shorttitle = {{{WORLD}}},
  doi = {10.1587/transinf.2015EDP7457},
  abstract = {Japan's largest platform for academic e-journals: J-STAGE is a full text database for reviewed academic papers published by Japanese societies},
  language = {en},
  number = {7},
  journal = {IEICE Transactions on Information and Systems},
  author = {Morise, Masanori and Yokomori, Fumiya and Ozawa, Kenji},
  month = jul,
  year = {2016},
  pages = {1877-1884},
  file = {/Users/kennylino/Zotero/storage/MGMQDM2C/Morise et al. - 2016 - WORLD A Vocoder-Based High-Quality Speech Synthes.pdf;/Users/kennylino/Zotero/storage/ZYRRQE9P/_article.html}
}

@article{veaux2017,
  title = {{{CSTR VCTK Corpus}}: {{English Multi}}-Speaker {{Corpus}} for {{CSTR Voice Cloning Toolkit}}},
  shorttitle = {{{CSTR VCTK Corpus}}},
  doi = {http://dx.doi.org/10.7488/ds/1994},
  abstract = {This CSTR VCTK Corpus (Centre for Speech Technology Voice Cloning Toolkit) includes speech data uttered by 109 native speakers of English with various accents. 96kHz versions of the recordings are available at http://dx.doi.org/10.7488/ds/2101. Each speaker reads out about 400 sentences, most of which were selected from a newspaper plus the Rainbow Passage and an elicitation paragraph intended to identify the speaker's accent. The newspaper texts were taken from The Herald (Glasgow), with permission from Herald \& Times Group. Each speaker reads a different set of the newspaper sentences, where each set was selected using a greedy algorithm designed to maximise the contextual and phonetic coverage. The Rainbow Passage and elicitation paragraph are the same for all speakers. The Rainbow Passage can be found in the International Dialects of English Archive: (http://web.ku.edu/\textasciitilde{}idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the speech accent archive can be found at e All speech data were recorded using an identical recording setup: an omni-directional head-mounted microphone (DPA 4035), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. All recordings were converted into 16 bits, downsampled to 48 kHz based on STPK, and manually end-pointed. This corpus was recorded for the purpose of building HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis using average voice models trained on multiple speakers and speaker adaptation technologies. The file was previously available on the CSTR website, and was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf .},
  language = {eng},
  journal = {The Rainbow Passage which the speakers read out can be found in the International Dialects of English Archive: (http://web.ku.edu/\textasciitilde{}idea/readings/rainbow.htm).},
  author = {Veaux, Christophe and Yamagishi, Junichi and MacDonald, Kirsten},
  month = apr,
  year = {2017},
  file = {/Users/kennylino/Zotero/storage/3VUA3L8W/2651.html}
}

@article{h.weinberger2011,
  title = {The {{Speech Accent Archive}}: {{Towards}} a Typology of {{English}} Accents},
  volume = {73},
  shorttitle = {The {{Speech Accent Archive}}},
  abstract = {The theoretical and practical value of studying human accented speech is of interest to linguists, language teachers, speech recognition researchers, and computational linguists. It is also part of the research program behind the Speech Accent Archive. The Archive is a growing annotated corpus of English speech varieties. All samples include a complete digital audio version and a narrow phonetic transcription. Each speaker is located geographically and crucial demographic parameters are supplied. This paper will discuss the architecture of the Speech Accent Archive and a computational mechanism we developed to assist in analyzing the archive, the Speech Transcription Analysis Tool (STAT). We show how such a tool generates a "typological corpus" of English accents that can benefit researchers in the fields of theoretical and applied linguistics.},
  journal = {Language and Computers},
  author = {H. Weinberger, Steven and Kunath, Stephen},
  month = dec,
  year = {2011}
}

@article{meier1998,
  title = {{{IDEA}}: {{International}} Dialects of {{English}} Archive},
  volume = {17},
  shorttitle = {{{IDEA}}},
  journal = {Accessed May},
  author = {Meier, Paul and Muller, S. M.},
  year = {1998},
  pages = {2005}
}

@article{flege1995,
  title = {Factors Affecting Strength of Perceived Foreign Accent in a Second Language},
  volume = {97},
  issn = {0001-4966},
  doi = {10.1121/1.413041},
  language = {en},
  number = {5},
  journal = {The Journal of the Acoustical Society of America},
  author = {Flege, James Emil and Munro, Murray J. and MacKay, Ian R. A.},
  month = may,
  year = {1995},
  pages = {3125-3134},
  file = {/Users/kennylino/Zotero/storage/LVIWZSMU/Flege et al. - 1995 - Factors affecting strength of perceived foreign ac.pdf}
}

@book{scovel1988a,
  title = {A Time to Speak: {{A}} Psycholinguistic Inquiry into the Critical Period for Human Speech},
  shorttitle = {A Time to Speak},
  publisher = {{Newbury House Publishers}},
  author = {Scovel, Thomas},
  year = {1988}
}

@article{patkowski1990,
  title = {Age and Accent in a Second Language: {{A}} Reply to {{James Emil Flege}}},
  volume = {11},
  shorttitle = {Age and Accent in a Second Language},
  number = {1},
  journal = {Applied linguistics},
  author = {Patkowski, Mark S.},
  year = {1990},
  pages = {73--89},
  file = {/Users/kennylino/Zotero/storage/XHXN4NE7/Patkowski - 1990 - Age and accent in a second language A reply to Ja.pdf;/Users/kennylino/Zotero/storage/NI5LCM7Y/255990.html}
}

@article{long1990,
  title = {Maturational Constraints on Language Development},
  volume = {12},
  number = {3},
  journal = {Studies in second language acquisition},
  author = {Long, Michael H.},
  year = {1990},
  pages = {251--285},
  file = {/Users/kennylino/Zotero/storage/MBBZBKQT/3C0E1658F007F775F7E0DE03D62925DD.html}
}

@article{lenneberg1967a,
  title = {The Biological Foundations of Language},
  volume = {2},
  number = {12},
  journal = {Hospital Practice},
  author = {Lenneberg, Eric H.},
  year = {1967},
  pages = {59--67},
  file = {/Users/kennylino/Zotero/storage/QAGJFL6K/Lenneberg - 1967 - The biological foundations of language.pdf;/Users/kennylino/Zotero/storage/4HHZNLKQ/21548331.1967.html}
}

@article{antonsen,
  title = {Open {{Set Speaker Identification}}},
  language = {en},
  author = {Antonsen, J\o{}rgen Johan},
  pages = {54},
  file = {/Users/kennylino/Downloads/appendix.pdf;/Users/kennylino/Zotero/storage/JUJNFQQB/Antonsen - Open Set Speaker Identification.pdf}
}

@article{kominek2004,
  title = {{{THE CMU ARCTIC SPEECH DATABASES}}},
  abstract = {The CMU Arctic databases designed for the purpose of speech synthesis research. These single speaker speech databases have been carefully recorded under studio conditions and consist of approximately 1200 phonetically balanced English utterances. In addition to wavefiles, the databases provide complete support for the Festival Speech Synthesis System, including pre-built voices that may be used as is. The entire package is distributed as free software, without restriction on commercial or non-commercial use.},
  language = {en},
  journal = {th ISCA Speech Synthesis Workshop},
  author = {Kominek, John and Black, Alan W},
  year = {2004},
  pages = {2},
  file = {/Users/kennylino/Zotero/storage/JXZTISB5/Kominek and Black - THE CMU ARCTIC SPEECH DATABASES.pdf}
}

@inproceedings{yan2004,
  title = {Analysis by Synthesis of Acoustic Correlates of {{British}}, {{Australian}} and {{American}} Accents},
  volume = {01},
  isbn = {978-0-7803-8484-2},
  doi = {10.1109/ICASSP.2004.1326066},
  abstract = {This paper presents analysis through synthesis of the acoustic correlates of British, Australian and American accents by transforming the correlates individually across the accents. The acoustic correlates of accents are grouped into three main categories: (a) the spectral features at formants, (b) the pitch intonation pattern and (c) duration. The modeling and transformation methods for each group of voice features are outlined. The spectral features at formants are modeled using two-dimensional (2D) phoneme-dependent HMM. Subband frequency warping is used for spectrum transformation where the subbands are centred on estimates of the formant trajectories. The F0 contour is used for modeling the pitch and intonation patterns of speech. A method based on the time domain pitch synchronous overlap and add algorithm (TD-PSOLA) is used for transformation of pitch intonation and duration pattern. Perceptual tests based on mean opinion score (MOS) are conducted to rank the main features of accents. Formants are regarded as the most important features of accents, followed by intonation pattern and duration.},
  booktitle = {2004 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}({{ICASSP}})},
  author = {Yan, Qin and Vaseghi, S. and Rentzos, D. and Ho, Ching-Hsiang},
  year = {2004},
  keywords = {feature extraction,speech processing,spectral analysis,speech synthesis,acoustic correlation,frequency estimation,hidden Markov models,speech recognition},
  pages = {I-637-40 vol.1},
  file = {/Users/kennylino/Zotero/storage/I7H8JG7R/Yan et al. - Analysis by synthesis of acoustic correlates of Br.pdf;/Users/kennylino/Zotero/storage/4SJGR6LT/01326066-abs.html}
}

@article{huckvale2007,
  title = {Spoken {{Language Conversion}} with {{Accent Morphing}}},
  abstract = {Spoken language conversion is the challenge of using synthesis systems to generate utterances in the voice of a speaker but in a language unknown to the speaker. Previous approaches have been based on voice conversion and voice adaptation technologies applied to the output of a foreign language TTS system. This inevitably reduces the quality and intelligibility of the output, since the source speaker will not be a good source of phonetic material in the new language. This article contrasts previous work with a new approach that uses two synthesis systems: one in the source speaker's voice, one in the voice of a native speaker of the target language. Audio morphing technology is then exploited to correct the foreign accent of the source speaker, while at the same time trying to maintain his or her identity. In this paper we construct a spoken language conversion system using accent morphing and evaluate its performance in terms of intelligibility. Encouraging results tell us more about the challenges of spoken language conversion.},
  language = {en},
  author = {Huckvale, Mark and Yanagisawa, Kayoko},
  year = {2007},
  pages = {7},
  file = {/Users/kennylino/Zotero/storage/IXU744WJ/Huckvale and Yanagisawa - 2007 - Spoken Language Conversion with Accent Morphing.pdf}
}

@article{verma2015,
  title = {I-{{Vectors}} in Speech Processing Applications: A Survey},
  volume = {18},
  issn = {1572-8110},
  shorttitle = {I-{{Vectors}} in Speech Processing Applications},
  doi = {10.1007/s10772-015-9295-3},
  abstract = {In the domain of speech recognition many methods have been proposed over time like Gaussian mixture models (GMM), GMM with universal background model (GMM-UBM framework), joint factor analysis, etc. i-Vector subspace modeling is one of the recent methods that has become the state of the art technique in this domain. This method largely provides the benefit of modeling both the intra-domain and inter-domain variabilities into the same low dimensional space. In this survey, we present a comprehensive collection of research work related to i-vectors since its inception. Some recent trends of using i-vectors in combination with other approaches are also discussed. The application of i-vectors in various fields of speech recognition, viz speaker, language, accent recognition, etc. is also presented. This paper should serve as a good starting point for anyone interested in working with i-vectors for speech processing in general. We then conclude the paper with a brief discussion on the future of i-vectors.},
  language = {en},
  number = {4},
  journal = {International Journal of Speech Technology},
  author = {Verma, Pulkit and Das, Pradip K.},
  month = dec,
  year = {2015},
  keywords = {Feature extraction,Speech processing,Factor analysis,i-Vectors,JFA,PLDA},
  pages = {529-546},
  file = {/Users/kennylino/Documents/Articles/VermaP/2015/Verma_2015_i-Vectors in speech processing applications.pdf}
}

@book{vanderplas2016,
  title = {Python Data Science Handbook: Essential Tools for Working with Data},
  shorttitle = {Python Data Science Handbook},
  publisher = {{O'Reilly Media, Inc.}},
  author = {VanderPlas, Jake},
  year = {2016},
  file = {/Users/kennylino/Zotero/storage/K77H9LNH/books.html}
}

@misc{mcgonagle2016,
  title = {Gaussian {{Mixture Model}}},
  abstract = {Gaussian mixture models are a probabilistic model for representing normally distributed subpopulations within an overall population. Mixture models in general don\&\#39;t require knowing which subpopulation a data point belongs to, allowing the model to learn the subpopulations automatically. Since subpopulation assignment is not known, this constitutes a form of unsupervised learning. For example, in modeling human height data, height is typically modeled as a normal distribution for each gender with a mean of approximately ...},
  language = {en-us},
  howpublished = {https://brilliant.org/wiki/gaussian-mixture-model/},
  journal = {Brilliant.org},
  author = {McGonagle, John and Pilling, Geoff and Tembo, Vincent and Chumbley, Alex and Ross, Eli and Khim, Jimin},
  year = {2016},
  file = {/Users/kennylino/Zotero/storage/I87G9FAT/gaussian-mixture-model.html}
}

@article{zhao2018,
  title = {L2-{{ARCTIC}}: {{A Non}}-{{Native English Speech Corpus}}},
  abstract = {In this paper, we introduce L2-ARCTIC, a speech corpus of non-native English that is intended for research in voice conversion, accent conversion, and mispronunciation detection. This initial release includes recordings from ten non-native speakers of English whose first languages (L1s) are Hindi, Korean, Mandarin, Spanish, and Arabic, each L1 containing recordings from one male and one female speaker. Each speaker recorded approximately one hour of read speech from the Carnegie Mellon University ARCTIC prompts, from which we generated orthographic and forced-aligned phonetic transcriptions. In addition, we manually annotated 150 utterances per speaker to identify three types of mispronunciation errors: substitutions, deletions, and additions, making it a valuable resource not only for research in voice conversion and accent conversion but also in computer-assisted pronunciation training. The corpus is publicly accessible at https://psi.engr.tamu.edu/l2-arctic-corpus/.},
  language = {en},
  author = {Zhao, Guanlong and Sonsaat, Sinem and Silpachai, Alif and Lucic, Ivana and {Chukharev-Hudilainen}, Evgeny and Levis, John and {Gutierrez-Osuna}, Ricardo},
  year = {2018},
  pages = {5},
  file = {/Users/kennylino/Zotero/storage/WN5EU93Q/Zhao et al. - L2-ARCTIC A Non-Native English Speech Corpus.pdf}
}

@inproceedings{hazen2009,
  title = {Query-by-Example Spoken Term Detection Using Phonetic Posteriorgram Templates},
  abstract = {Abstract\textemdash{}This paper examines a query-by-example approach to spoken term detection in audio files. The approach is designed for low-resource situations in which limited or no in-domain training material is available and accurate word-based speech recognition capability is unavailable. Instead of using word or phone strings as search terms, the user presents the system with audio snippets of desired search terms to act as the queries. Query and test materials are represented using phonetic posteriorgrams obtained from a phonetic recognition system. Query matches in the test data are located using a modified dynamic time warping search between query templates and test utterances. Experiments using this approach are presented using data from the Fisher corpus. I.},
  booktitle = {In {{Proc}}. {{ASRU}}},
  author = {Hazen, Timothy J. and Shen, Wade and White, Christopher},
  year = {2009},
  file = {/Users/kennylino/Documents/Articles/HazenT/2009/Hazen_2009_Query-by-example spoken term detection using phonetic posteriorgram templates.pdf;/Users/kennylino/Zotero/storage/7DI9NQAS/summary.html}
}

@article{felps2012,
  title = {Foreign {{Accent Conversion Through Concatenative Synthesis}} in the {{Articulatory Domain}}},
  volume = {20},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2012.2201474},
  abstract = {We propose a concatenative synthesis approach to the problem of foreign accent conversion. The approach consists of replacing the most accented portions of nonnative speech with alternative segments from a corpus of the speaker's own speech based on their similarity to those from a reference native speaker. We propose and compare two approaches for selecting units, one based on acoustic similarity [e.g., mel frequency cepstral coefficients (MFCCs)] and a second one based on articulatory similarity, as measured through electromagnetic articulography (EMA). Our hypothesis is that articulatory features provide a better metric for linguistic similarity across speakers than acoustic features. To test this hypothesis, we recorded an articulatory-acoustic corpus from a native and a nonnative speaker, and evaluated the two speech representations (acoustic versus articulatory) through a series of perceptual experiments. Formal listening tests indicate that the approach can achieve a 20\% reduction in perceived accent, but also reveal a strong coupling between accent and speaker identity. To address this issue, we disguised original and resynthesized utterances by altering their average pitch and normalizing vocal tract length. An additional listening experiment supports the hypothesis that articulatory features are less speaker dependent than acoustic features.},
  language = {en},
  number = {8},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  author = {Felps, Daniel and Geng, Christian and {Gutierrez-Osuna}, Ricardo},
  month = oct,
  year = {2012},
  pages = {2301-2312},
  file = {/Users/kennylino/Zotero/storage/M5Z8DUBU/Felps et al. - 2012 - Foreign Accent Conversion Through Concatenative Sy.pdf}
}


